<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Reverse Proxy for RStudio Server and JupyterLab-Hub with SSL in NGINX]]></title>
    <url>%2Fposts%2F201902%2F2019-02-27-nginx-reverse-proxy-for-rstudio-server-and-jupyterhub-with-ssl.html</url>
    <content type="text"><![CDATA[Simply record the configuration files. Install NGINX, RStudio server, JupyterHub, JupyterLab and Jupyterlab-hub extension. Setup NGINX 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# generate SSLmkdir /etc/nginx/sslsudo openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt# configtee /etc/nginx/conf.d/rstudio_jupyter.conf &lt;&lt; EOFserver &#123; listen 80; return 301 https://$host$request_uri;&#125;map $http_upgrade $connection_upgrade &#123; default upgrade; '' close;&#125;server &#123; listen 443 ssl default_server; listen [::]:443 ssl default_server; server_name _; ssl_certificate /etc/nginx/ssl/nginx.crt; ssl_certificate_key /etc/nginx/ssl/nginx.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; add_header Strict-Transport-Security "max-age=15768000"; client_max_body_size 512M; fastcgi_buffers 64 4K; location /rstudio/ &#123; rewrite ^/rstudio/(.*)$ /$1 break; proxy_pass http://localhost:8787; proxy_redirect http://localhost:8787/ $scheme://$host/rstudio/; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_read_timeout 86400; &#125; location ~* /jupyter.* &#123; proxy_pass http://localhost:8000; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_read_timeout 86400; &#125;&#125;EOF Setup RStudio Server 12345sed -i '/www-address/d' /etc/rstudio/rserver.conftee -a /etc/rstudio/rserver.conf &lt;&lt; EOFwww-address=127.0.0.1EOFsystemctl restart rstudio-server Setup JupyterHub 12345678910111213141516171819202122232425262728293031323334353637383940mkdir /jupyter# setup configtee /jupyter/jupyterhub_config.py &lt;&lt; EOFc.JupyterHub.authenticator_class = 'jupyterhub.auth.PAMAuthenticator'c.JupyterHub.base_url = '/jupyter'c.JupyterHub.config_file = '/jupyter/jupyterhub_config.py'c.JupyterHub.cookie_secret_file = '/jupyter/jupyterhub_cookie_secret'c.JupyterHub.data_files_path = '/usr/local/share/jupyterhub'c.JupyterHub.db_url = 'sqlite://jupyterhub.sqlite'c.JupyterHub.hub_connect_ip = '127.0.0.1'c.JupyterHub.hub_ip = '127.0.0.1'c.JupyterHub.ip = '127.0.0.1'c.JupyterHub.cmd = ['jupyter-labhub']c.JupyterHub.admin_users = set(['user'])EOF# setup servicetee /jupyter/jupyterhub.service &lt;&lt; EOF[Unit]Description=JupyterhubAfter=syslog.target network.target[Service]User=rootEnvironment="PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"ExecStart=/jupyter/start_jupyterhub.sh[Install]WantedBy=multi-user.targetEOF# link to service folderln -s /jupyter/jupyterhub.service /etc/systemd/system/jupyterhub.service# start shell scrtipttee /jupyter/start_jupyterhub.sh &lt;&lt; EOF#!/usr/bin/env bash/usr/local/bin/jupyterhub -f /jupyter/jupyterhub_config.pyEOFchmod +x /jupyter/start_jupyterhub.shsystemctl start jupyterhub]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>R</tag>
        <tag>linux</tag>
        <tag>Julia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Start an Jupyter Lab With Julia built with MKL on Docker]]></title>
    <url>%2Fposts%2F201902%2F2019-02-03-Julia-MKL-build-in-docker.html</url>
    <content type="text"><![CDATA[安裝Docker-ce，並啟動。然後pull image，就可以啟動了。 123456789101112# install dockeryum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce -ysystemctl enable dockersystemctl start docker# pull images and startdocker pull jamal0230/centos-mkl-julia-jupyter:latest# run Jupyter Lab with Julia built with MKL. Then connect to https://localhost:8888docker run -d -p 8888:8888 -e PASSWORD=password -e USE_HTTPS=yes --name julia_jupyter jamal0230/centos-mkl-julia-jupyter:latest]]></content>
      <categories>
        <category>julia</category>
      </categories>
      <tags>
        <tag>julia</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SLURM Cluster in Docker Swarm Cluster]]></title>
    <url>%2Fposts%2F201812%2F2018-12-21-SLURM-cluster-in-docker.html</url>
    <content type="text"><![CDATA[這篇主要是介紹怎麼在Docker Swarm Cluster起SLURM Cluster 先提供References: slurm-in-docker slurm-docker-cluster 首先先部署docker swarm cluster 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# modify hoststee -a /etc/hosts &lt;&lt; EOF192.168.1.131 jamalslurm01192.168.1.132 jamalslurm02192.168.1.133 jamalslurm03EOF# generate ssh keyssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa -q -N ""# install sshpass and copy ssh keyyum install sshpass -yPASS=`cat password`for host in jamalslurm01 jamalslurm02 jamalslurm03; do ssh-keyscan $host &gt;&gt; ~/.ssh/known_hosts sshpass -p $PASS ssh-copy-id root@$hostdone# copy hosts filefor host in jamalslurm02 jamalslurm03; do scp /etc/hosts $host:/etcdone# install pip and docker-composeyum install -y epel-releaseyum install -y python-pippip install pip setuptools --upgradepip install docker-compose# init docker swarm clusterdocker swarm init --advertise-addr 192.168.1.131# show key to add workerdocker swarm join-token worker# run following command in node (example)# docker swarm join --token SWMTKN-1-35f9hdj8scn5o58xnsxffvh12eeagw67r5mim87o1g3hzvs60d-dk72tjwi9elppjgq37agrpp4o 192.168.1.131:2377# show key to add managerdocker swarm join-token manager# run following command in node (example)# docker swarm join --token SWMTKN-1-35f9hdj8scn5o58xnsxffvh12eeagw67r5mim87o1g3hzvs60d-521e7shudywaqbpxz2ox9231n 192.168.1.131:2377# setup iptablesfor host in jamalslurm01 jamalslurm02 jamalslurm03; do ssh $host &lt;&lt; EOFiptables -Lsystemctl restart dockerEOFdone 安裝swarm visualizer: 12345docker service create --name=viz \ --publish=8080:8080/tcp \ --constraint=node.role==manager \ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \ dockersamples/visualizer 安裝之後連上該host的8080就可以看到下圖： 安裝nfs用來share swarm的volumes 12345678910111213141516171819202122232425262728293031323334353637383940# install nfsfor host in jamalslurm01 jamalslurm02 jamalslurm03; do ssh $host &lt;&lt; EOFyum -y install nfs-utilssystemctl start rpcbindsystemctl enable rpcbindEOFdone## jamalslurm01mkdir -p /data/nfs_voltouch /data/nfs_vol/test.txttee /etc/exports &lt;&lt; EOF# swarm nfs share volume# /data/nfs_vol: shared directory# 192.168.1.0/24: subnet having privilege to access# rw: permission to read and write. ro: read only# sync: synchronized, slow, secure.# async: asynchronized, fast, less secure# no_root_squash: open to root to use/data/nfs_vol 192.168.1.0/24(rw,sync,no_root_squash)EOF# start nfssystemctl enable nfssystemctl start nfs# install and config autofs on nodesfor host in jamalslurm02 jamalslurm03; do ssh $host &lt;&lt; EOFyum -y install autofstee -a /etc/auto.master &lt;&lt; EOF2/mnt /etc/auto.mntEOF2tee /etc/auto.mnt &lt;&lt; EOF2nfs_vol -rw,bg,soft,intr,rsize=8192,wsize=8192 jamalslurm01:/data/nfs_volEOF2systemctl enable autofssystemctl start autofsEOFdone 測試一下nfs沒問題之後，就可以進主菜SLURM了 12345678910111213141516171819202122232425262728293031323334353637383940# create needed foldersmkdir /data/nfs_vol/slurm_spoolmkdir /data/nfs_vol/slurm_confmkdir /data/nfs_vol/datamkdir /data/nfs_vol/mysql# clone filesgit clone https://github.com/ChingChuan-Chen/slurm-mpi-r.gitcd slurm-mpi-r# label hostfor host in jamalslurm01 jamalslurm02 jamalslurm03; do docker node update --label-add role=slurmd $hostdone# generate config./generate-slurm-config.sh slurmd worker normal# copy configrm -f /data/nfs_vol/slurm_conf/*.confcp slurm-confs/*.conf /data/nfs_vol/slurm_conf# pull needed images (master pull and share to other nodes)docker pull mysql:5.7docker pull jamal0230/slurm-mpi-r:latestdocker save jamal0230/mysql:5.7 &gt; /data/nfs_vol/mysql-5.7.tardocker save jamal0230/slurm-mpi-r:latest &gt; /data/nfs_vol/slurm-mpi-r-latest.tarfor host in jamalslurm02 jamalslurm03; do ssh $host &lt;&lt; EOFdocker load &lt; /mnt/nfs_vol/mysql-5.7.tardocker load &lt; /mnt/nfs_vol/slurm-mpi-r-latest.tarEOFdone# deploydocker stack deploy --with-registry-auth slurm --compose-file=docker-compose.yml# registerhost=$(docker service ps -f 'name=slurm' slurm_slurmctld | awk '&#123;print $4&#125;' | head -2 | tail -1)scp register_cluster.sh $host:~/ssh $host ~/register_cluster.sh 用visualizer查看佈署情況： 最後利用這個指令就可以進去slurmctld的container: 1234# get host of slurmctldhost=$(docker service ps -f &apos;name=slurm&apos; slurm_slurmctld | awk &apos;&#123;print $4&#125;&apos; | tail -1)ssh $hostdocker exec -it slurm_slurmctld.1.$(docker service ps -f &apos;name=slurm&apos; slurm_slurmctld -q --no-trunc | head -n1) /bin/bash 然後執行sinfo就可以看到下圖了 PS: 後來測試Multi-node job都失敗，原因目前找不到…]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How does R Process Existed Connections When Exit?]]></title>
    <url>%2Fposts%2F201812%2F2018-12-16-how-does-R-process-existed-connections-when-exit.html</url>
    <content type="text"><![CDATA[這篇主要是介紹在R裡面在離開Process時是怎麼處理Process的連線。 熟悉R的人都知道showConnections(all = TRUE)會show出這個Process目前的所有連線為何 基本上一定會有stdin，stdout跟stderr 而且在connections.c裡面會定義最大連線數 基本上，R的最大連線數都是128，除非自己編譯R，才可能取得更高的連線數量，這個數字其實還有很多意義 其中之一是，如果有用parallel或是snow都是R可以透過ssh多台電腦來spawn slaves 而slaves的最大數目就是這個數字減3(需要扣掉stdin，stdout跟stderr) 扯得有點遠了，我們今天的主題是介紹R裡面怎麼處理這些Connection的 要知道這個，我們首先從connections.c下手 在R_new_custom_connection這個函數上面可以看到兩個註解的block，裡面寫到： / — C-level entry to create a custom connection object – // The returned value is the R-side instance. To avoid additional call to getConnection() the internal Rconnection pointer will be placed in ptr[0] if ptr is not NULL. It is the responsibility of the caller to customize callbacks in the structure, they are initialized to dummy_ (where available) and null_ (all others) callbacks. Also note that the resulting object has a finalizer, so any clean up (including after errors) is done by garbage collection - the caller may not free anything in the structure explicitly (that includes the con-&gt;private pointer!). / 我們重點是裡面提到每一個connection的object都會有一個finalizer，所以所有包含錯誤的clean up相關動作就會被正確處理掉 那我們看一下R_new_custom_connection，其實裡面就會有R_RegisterCFinalizerEx的動作 我們先往下挖，而它會註冊到conFinalizer這個函數，然後會連結到con_destroy 接著到con_close1，然後call con裡面對應的close method完成關閉 但是我們怎麼知道R離開時會call finalizer呢? 我們就要深入去看memory.c中的R_RegisterCFinalizerEx了 我們循著脈絡來往下找R_RegisterCFinalizerEx -&gt; R_MakeWeakRefC -&gt; MakeCFinalizer，所以最後會註冊到CFinalizer 我們再從取的CFinalizer的function GetCFinalizer往上找，就會看到R_RunWeakRefFinalizer -&gt; RunFinalizers 看一下RunFinalizers裡面的註解 / A top level context is established for the finalizer to insure that any errors that might occur do not spill into the call that triggered the collection. / 它提到context是為了finalizer而被建立的，用來確認說在任何錯誤發生時，能夠trigger資源回收 最後，我們的R Source Code Trace之旅就結束了 我們看到說R針對它自己離開時，會在不論什麼情況下，回收掉所有的connections，避免connection殘留]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Start an RStudio Server With R built on MKL]]></title>
    <url>%2Fposts%2F201812%2F2018-12-04-Rstudio-server-in-docker.html</url>
    <content type="text"><![CDATA[安裝Docker-ce，並啟動。然後pull image，就可以啟動了。 123456789101112# install dockeryum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce -ysystemctl enable dockersystemctl start docker# pull images and startdocker pull jamal0230/centos-rstudio-mkl-r:latest# run RStudio server with R built on MKL. The account is user with password password.docker run -d -p 8787:8787 -e USER=user -e PASSWORD=password --name rstudio jamal0230/centos-rstudio-mkl-r:latest]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Write a Generalized Plotting Function Via R]]></title>
    <url>%2Fposts%2F201809%2F2018-09-18-write-A-general-plot-function-with-R.html</url>
    <content type="text"><![CDATA[這篇主要是介紹怎麼利用R的ggplot2去寫一個generalized的畫圖函數 先載入資料，轉成data.table，以及把factor的欄位都轉成character 12345678library(ggplot2)library(data.table)library(pipeR)data("diamonds")factorCols &lt;- names(diamonds)[sapply(diamonds, is.factor)]diamondsDT &lt;- data.table(diamonds) %&gt;&gt;% `[`(j = eval(factorCols) := lapply(.SD, as.character), .SDcols = factorCols) 目標是把下面這張圖做成一個generalized plotting function來應付各種需求 畫圖程式如下： 1234567statDT &lt;- diamondsDT[ , .(cnt = .N, mean = mean(price), sd = sd(price)), by = .(cut)] %&gt;&gt;% `[`(j = label := sprintf("cnt: %i\nmean: %.1f\nsd: %.2f", cnt, mean, sd))g &lt;- ggplot(diamondsDT, aes(cut, price)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.25) + ylim(c(300, 21000)) + geom_text(aes(cut, 20000, label = label), statDT)ggsave("boxplot_example.png", g, width = 7, height = 7) 第一步，我們先能夠產出ggplot2的aes，讓他能吃characters 1234567891011121314# 利用aes_string來達成，為什麼要用bquote，下面那段程式說明aesExpr &lt;- bquote(aes_string(x = "cut", y = "price"))ggplot(diamondsDT, eval(aesExpr)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.25) + ylim(c(300, 21000)) + geom_text(aes(cut, 20000, label = label), statDT, inherit.aes = FALSE)# 加點花樣 (不同的color給不同的顏色)，使用bquote好處就是可以很容易新增其他變數aesExpr &lt;- bquote(aes_string(x = "cut", y = "price"))aesExpr$colour &lt;- "color"ggplot(diamondsDT, eval(aesExpr)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.25) + ylim(c(300, 21000)) + geom_text(aes(cut, 20000, label = label), statDT, inherit.aes = FALSE) 第二步，我們將statDT用函數算出來，這邊大量使用data.table的技巧 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556getStatDTFunc &lt;- function(DT, calVar, byVars, labelName = "label", sprintFmts = c("%i", "%.1f", "%.2f"), funcList = list(cnt = length, mean = mean, sd = sd)) &#123; stopifnot(is.data.frame(DT), is.character(calVar), length(calVar) == 1, calVar %in% names(DT), is.character(labelName), length(labelName) == 1, is.character(byVars), all(byVars %in% names(DT)), all(grepl("^%[0-9.]*[if]", sprintFmts)), length(sprintFmts) == length(funcList), all(nchar(names(funcList)) &gt; 0)) if (!is.data.table(DT)) DT &lt;- data.table(DT) outDT &lt;- DT[ , lapply(funcList, function(f) f(get(calVar))) %&gt;&gt;% setNames(names(funcList)), by = byVars] fmt &lt;- sprintf("%s: %s", names(funcList), sprintFmts) %&gt;&gt;% paste0(collapse = "\n") outDT[ , eval(labelName) := do.call(sprintf, c(list(fmt = fmt), mget(names(funcList))))] return(outDT)&#125;# defaultgetStatDTFunc(diamondsDT, "price", "cut")## 結果如下# cut cnt mean sd label# 1: Ideal 21551 3457.542 3808.401 cnt: 21551\nmean: 3457.5\nsd: 3808.40# 2: Premium 13791 4584.258 4349.205 cnt: 13791\nmean: 4584.3\nsd: 4349.20# 3: Good 4906 3928.864 3681.590 cnt: 4906\nmean: 3928.9\nsd: 3681.59# 4: Very Good 12082 3981.760 3935.862 cnt: 12082\nmean: 3981.8\nsd: 3935.86# 5: Fair 1610 4358.758 3560.387 cnt: 1610\nmean: 4358.8\nsd: 3560.39# variant 1 (data.table has a bug on median.)getStatDTFunc(diamondsDT, "price", "cut", "label", c("%i", "%.1f"), list(cnt = length, median = function(x) quantile(x, 0.5)))## 結果如下# cut cnt median label# 1: Ideal 21551 1810.0 cnt: 21551\nmedian: 1810.0# 2: Premium 13791 3185.0 cnt: 13791\nmedian: 3185.0# 3: Good 4906 3050.5 cnt: 4906\nmedian: 3050.5# 4: Very Good 12082 2648.0 cnt: 12082\nmedian: 2648.0# 5: Fair 1610 3282.0 cnt: 1610\nmedian: 3282.0# variant 2getStatDTFunc(diamondsDT, "price", "clarity", "label", c("%i", "%.1f", "%.1f", "%.1f"), list(cnt = length, q25 = function(x) quantile(x, 0.25), q50 = function(x) quantile(x, 0.5), q75 = function(x) quantile(x, 0.75)))## 結果如下# clarity cnt q25 q50 q75 label# 1: SI2 9194 2264.00 4072 5777.25 cnt: 9194\nq25: 2264.0\nq50: 4072.0\nq75: 5777.2# 2: SI1 13065 1089.00 2822 5250.00 cnt: 13065\nq25: 1089.0\nq50: 2822.0\nq75: 5250.0# 3: VS1 8171 876.00 2005 6023.00 cnt: 8171\nq25: 876.0\nq50: 2005.0\nq75: 6023.0# 4: VS2 12258 900.00 2054 6023.75 cnt: 12258\nq25: 900.0\nq50: 2054.0\nq75: 6023.8# 5: VVS2 5066 794.25 1311 3638.25 cnt: 5066\nq25: 794.2\nq50: 1311.0\nq75: 3638.2# 6: VVS1 3655 816.00 1093 2379.00 cnt: 3655\nq25: 816.0\nq50: 1093.0\nq75: 2379.0# 7: I1 741 2080.00 3344 5161.00 cnt: 741\nq25: 2080.0\nq50: 3344.0\nq75: 5161.0# 8: IF 1790 895.00 1080 2388.50 cnt: 1790\nq25: 895.0\nq50: 1080.0\nq75: 2388.5 所以這樣一來，statDT的產生也能用函數產生了，最後就是寫畫圖函數了 12345678910111213141516171819202122232425262728293031323334boxPlotFunc &lt;- function(data, x, y, colour = NULL, title = "", xlab = x, ylab = y, needStats = FALSE, statDT = NULL, labelCol = NULL, textSize = 11, titleTextSize = rel(1.2), legendTitle = "", legendTitleSize = rel(1), legendLabelSize = rel(0.8), axisTitleSize = rel(0.8), axisTextSize = rel(0.8)) &#123; aesExpr &lt;- bquote(aes_string(x = "cut", y = "price")) labExpr &lt;- bquote(labs(x = xlab, y = ylab, title = title)) if (!is.null(colour)) &#123; aesExpr$colour &lt;- colour labExpr$colour &lt;- ifelse(nchar(legendTitle) == 0, colour, legendTitle) &#125; g &lt;- ggplot(data, eval(aesExpr)) + eval(labExpr) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.25) + theme(text = element_text(size = textSize), plot.title = element_text(size = titleTextSize), axis.title.x = element_text(size = axisTitleSize), axis.title.y = element_text(size = axisTitleSize), axis.text = element_text(size = axisTextSize), legend.title = element_text(size = legendTitleSize), legend.text = element_text(size = legendLabelSize)) if (needStats) &#123; calStatLoc &lt;- extendrange(diamondsDT$price, f = 0.05)[2] ylim &lt;- c(extendrange(diamondsDT$price, f = 0.025)[1], extendrange(diamondsDT$price, f = 0.075)[2]) if (!is.null(statDT) &amp;&amp; !is.null(labelCol)) g &lt;- g + ylim(ylim) + geom_text(aes_string(x, y = as.character(calStatLoc), label = labelCol), statDT, inherit.aes = FALSE) else warning("needStats is TRUE, but statDT or labelCol is null, so not add statistics.") &#125; return(g)&#125; 這裡有幾點要說明 rel是ggplot2的相對大小的函數，ggplot2預設的text大小是11，則rel(0.8) = 11 * 0.8 = 8.8的size ggplot2參數預設值可以參考這裡 最後簡單的測試一下： 簡單功能測試： 1boxPlotFunc(diamondsDT, "cut", "price") 測試一下其他參數： 12345678statDT &lt;- getStatDTFunc(diamondsDT, "price", "cut", "label", c("%i", "%f"), list(cnt = length, median = function(x) quantile(x, 0.5)))g2 &lt;- boxPlotFunc(diamondsDT, "cut", "price", "color", "Boxplox for Price vs Cut + Color of Diamonds", "Cut of Diamonds", "Price of Diamonds", legendTitle = "Color of Diamonds", needStats = TRUE, statDT = statDT, labelCol = "label")ggsave("boxplot_final.png", g2, width = 9, height = 7) 圖：]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>ggplot2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Compile Julia on CentOS 7 with Intel MKL and Intel C++]]></title>
    <url>%2Fposts%2F201809%2F2018-09-11-compile-julia-in-centos-7.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄在CentOS 7編譯Julia 1234567891011121314151617181920212223242526# clone source codegit clone https://github.com/JuliaLang/julia.gitgit checkout v1.0.0# create make infotee Make.user &lt;&lt; EOFUSE_INTEL_MKL = 1MKLROOT = /opt/intel/mklEOF# install needed componentsyum install gcc gcc-c++ libatomic gcc-gfortran wget m4 bzip2 -ywget https://cmake.org/files/v3.11/cmake-3.11.4.tar.gztar xzvf cmake-3.11.4.tar.gzcd cmake-3.11.4./bootstrap --prefix=/usrgmakemake install# 把so複製過來MKLROOT=/opt/intel/mklmkdir -p usr/bincp "$MKLROOT/lib/intel64/libiomp5md.dll" usr/bin/cp $MKLROOT/lib/intel64/lib* usr/bincp /opt/intel/compilers_and_libraries/linux/lib/intel64/libiomp5* usr/bin# 開始編譯make -j 15]]></content>
      <categories>
        <category>julia</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>julia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build the GPU Version of LightGBM in CentOS 7]]></title>
    <url>%2Fposts%2F201808%2F2018-08-16-build-gpu-version-of-lightgbm-in-centos-7.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄如何在CentOS要怎麼去編譯GPU Version的LightGBM 其實官方安裝文件已經寫得非常清楚 但是因為Centos 7提供的boost實在太舊了，你需要自己編譯。cmake則是需要安裝cmake3，CentOS的cmake是2.8 CUDA安裝就不介紹了，從Nvidia那裡載下來rpm，然後安裝，再輸入指令yum install -y cuda即可，最後就可以在/usr/local看到你的cuda 12345678910111213141516171819202122232425262728293031323334yum install cmake3 opencl-headers -yln -s /usr/bin/cmake3 /usr/bin/cmakewget https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gztar -zxvf boost_1_66_0.tar.gzcd boost_1_66_0./bootstrap.sh --prefix=/usr --libdir=/usr/lib64./b2 install# 下載lightGBM，並重新命名git clone --recursive https://github.com/Microsoft/LightGBMmv LightGBM LightGBM-20180816cd LightGBM-20180816# 開始編譯mkdir buildcd buildexport OpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.soexport OpenCL_INCLUDE_DIR=/usr/local/cuda-9.2/include/CLcmake -DUSE_GPU=1 ..make -j4# R安裝cd ../R-packageRscript build_package.R# 改成用precompiled so跟GPUsed -i -e "s/use_precompile &lt;.*/use_precompile &lt;- TRUE/g" src/install.libs.Rsed -i -e "s/use_gpu &lt;.*/use_gpu &lt;- TRUE/g" src/install.libs.RR CMD INSTALL . --build --no-multiarch# python安裝cd ../python-packagepip3.5 install setuptools numpy scikit-learn scipypython3.5 setup.py install --precompile -O2]]></content>
      <categories>
        <category>LightGBM</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>R</tag>
        <tag>GPU</tag>
        <tag>cuda</tag>
        <tag>LightGBM</tag>
        <tag>Liunx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build the GPU Version of LightGBM in Windows]]></title>
    <url>%2Fposts%2F201808%2F2018-08-15-build-gpu-version-of-lightgbm-in-windows.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄如何在Windows要怎麼去編譯GPU Version的LightGBM 其實官方安裝文件已經寫得非常清楚 我這裡就只是把整個流程可以被自動化而已 事前準備： 安裝R, Rtools, Python (Recommend Miniconda or Anaconda) 安裝Git, CMake 安裝Boost，我自己是下載msvc-14.0-64的版本，打開來安裝即可 安裝Visual Studio 2017，照著做就好 我把Git跟CMake都加入了PATH裡面，如果不會加PATH，或是不想破壞Windows的PATH，也可以考慮用下面指令把git跟cmake命令短暫地可以被access到 12REM 這裡是舉例我安裝的地方，您的安裝位置可能不一樣set PATH=%PATH%;C:\Program Files\CMake\bin;C:\Program Files\Git\bin 然後就可以開始安裝了 1234567891011121314151617181920212223242526272829REM 這裡是先設定BOOST的環境變數Set BOOST_ROOT=C:\local\boost_1_64_0\Set BOOST_LIBRARYDIR=C:\local\boost_1_64_0\lib64-msvc-14.0REM 下載lightGBM，並重新命名git clone --recursive https://github.com/Microsoft/LightGBMmv LightGBM LightGBM-20180815cd LightGBM-20180815REM 開始編譯mkdir buildcd buildcmake -DUSE_GPU=1 -DCMAKE_GENERATOR_PLATFORM=x64 ..cmake --build . --target ALL_BUILD --config Release -- /maxcpucountREM R安裝cd ..\R-packageset PATH=%PATH%;C:\Program Files\Microsoft\R Open\R-3.4.4\bin;C:\Rtools\bin;C:\Rtools\mingw_64\binRscript build_package.RREM 改成用precompiled dll跟GPUsed -i -e "s/use_precompile &lt;.*/use_precompile &lt;- TRUE/g" src/install.libs.Rsed -i -e "s/use_gpu &lt;.*/use_gpu &lt;- TRUE/g" src/install.libs.RR CMD INSTALL . --build --no-multiarchREM python安裝cd ..\python-packageC:\Miniconda3\Scripts\activate.batconda activate myenvpython setup.py install --precompile -O2]]></content>
      <categories>
        <category>LightGBM</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>R</tag>
        <tag>GPU</tag>
        <tag>cuda</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of SLURM]]></title>
    <url>%2Fposts%2F201807%2F2018-07-27-installation-of-slurm.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄如何在CentOS 7中安裝SLURM 1. Installation of munge123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# 1-1. install mariadbyum install mariadb-server mariadb-devel -y# 1-2. create userstee tmp.sh &lt;&lt; EOFexport MUNGEUSER=991groupadd -g $MUNGEUSER mungeuseradd -m -d /var/lib/munge -u $MUNGEUSER -g munge -s /sbin/nologin mungeexport SLURMUSER=990groupadd -g $SLURMUSER slurmuseradd -m -d /var/lib/slurm -u $SLURMUSER -g slurm -s /bin/bash slurmEOFbash tmp.sh# checkgrep '990' /etc/passwdgrep '991' /etc/passwd# slurm::990:990::/var/lib/slurm:/bin/bash# munge::991:991::/var/lib/munge:/sbin/nologinfor i in &#123;122..123&#125;; do scp ~/tmp.sh root@192.168.1.$i:~/ ssh root@192.168.1.$i bash ~/tmp.sh ssh root@192.168.1.$i grep '990' /etc/passwd ssh root@192.168.1.$i grep '991' /etc/passwddone# tmp.sh 100% 219 173.5KB/s 00:00# slurm::990:990::/var/lib/slurm:/bin/bash# munge::991:991::/var/lib/munge:/sbin/nologin# tmp.sh 100% 219 214.9KB/s 00:00# slurm::990:990::/var/lib/slurm:/bin/bash# munge::991:991::/var/lib/munge:/sbin/nologin# 1-3. Installation of mungeyum install epel-release -yyum install munge munge-libs munge-devel -yyum install rng-tools -y/usr/sbin/create-munge-key -rdd if=/dev/urandom bs=1 count=1024 &gt; /etc/munge/munge.keyfor i in &#123;122..123&#125;; do ssh root@192.168.1.$i yum install epel-release -y ssh root@192.168.1.$i yum install munge munge-libs munge-devel -y scp /etc/munge/munge.key root@192.168.1.$i:/etc/mungedone# 1-4. Start servicestee ~/tmp.sh &lt;&lt; EOFchown munge: /etc/munge/munge.keychmod 400 /etc/munge/munge.keychown -R munge: /etc/munge/ /var/log/munge/chmod 0700 /etc/munge/ /var/log/munge/systemctl enable mungesystemctl start mungeEOFbash tmp.shfor i in &#123;122..123&#125;; do scp ~/tmp.sh root@192.168.1.$i:~/ ssh root@192.168.1.$i bash ~/tmp.shdone# 1-5. testfor i in &#123;121..123&#125;; do munge -n | ssh 192.168.1.$i unmungedone# STATUS: Success (0)# ENCODE_HOST: jamalvm11 (192.168.1.121)# ENCODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# DECODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# TTL: 300# CIPHER: aes128 (4)# MAC: sha1 (3)# ZIP: none (0)# UID: root (0)# GID: root (0)# LENGTH: 0## STATUS: Success (0)# ENCODE_HOST: jamalvm11 (192.168.1.121)# ENCODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# DECODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# TTL: 300# CIPHER: aes128 (4)# MAC: sha1 (3)# ZIP: none (0)# UID: root (0)# GID: root (0)# LENGTH: 0## STATUS: Success (0)# ENCODE_HOST: jamalvm11 (192.168.1.121)# ENCODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# DECODE_TIME: 2018-07-26 23:16:50 +0800 (1532618210)# TTL: 300# CIPHER: aes128 (4)# MAC: sha1 (3)# ZIP: none (0)# UID: root (0)# GID: root (0)# LENGTH: 0 2. Build SLURM123456789101112131415161718192021222324252627282930# 1-1. install MPICH2 (optional)yum install gcc gcc-c++ gcc-gfortran kernel-devel -ywget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gztar zxvf mpich-3.2.1.tar.gzcd mpich-3.2.1./configuremake -j4make installfor i in &#123;122..123&#125;; do scp ~/mpich-3.2.1 root@192.168.1.$i:~/ ssh root@192.168.1.$i &lt;&lt; EOFcd mpich-3.2.1make installEOFdone# 1-2. install depsyum install openssl openssl-devel pam-devel numactl numactl-devel hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel man2html libibmad libibumad cpanm* -yyum install wget gcc gcc-c++ hdf5 hdf5-devel -yyum install libcurl-devel json-c-devel lz4-devel libibmad-devel libssh2-devel glibc-devel glib2-devel gtk2-devel -y# 1-3. build slurmwget https://download.schedmd.com/slurm/slurm-17.11.8.tar.bz2yum install rpmdevtools -yrpmbuild -ta slurm-17.11.8.tar.bz2mkdir ~/slurm_rpmsmv rpmbuild/RPMS/x86_64/slurm*.rpm ~/slurm_rpmsfor i in &#123;122..123&#125;; do scp -r ~/slurm_rpms root@192.168.1.$i:~/done 3. Installation of SLURMGenerate configuration in here. Below is my config.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# slurm.conf file generated by configurator easy.html.# Put this file on all nodes of your cluster.# See the slurm.conf man page for more information.#ControlMachine=jamalvm11#ControlAddr=#MailProg=/bin/mailMpiDefault=none#MpiParams=ports=#-#ProctrackType=proctrack/cgroupReturnToService=1SlurmctldPidFile=/var/run/slurm/slurmctld.pidSlurmctldPort=8017SlurmdPidFile=/var/run/slurm/slurmd.pidSlurmdPort=8018SlurmdSpoolDir=/var/spool/slurmSlurmUser=slurm#SlurmdUser=rootStateSaveLocation=/var/spool/slurmSwitchType=switch/noneTaskPlugin=task/affinity### TIMERS#KillWait=30#MinJobAge=300#SlurmctldTimeout=120#SlurmdTimeout=300### SCHEDULINGFastSchedule=1SchedulerType=sched/backfillSelectType=select/cons_resSelectTypeParameters=CR_Core### LOGGING AND ACCOUNTINGAccountingStorageType=accounting_storage/noneClusterName=cluster#JobAcctGatherFrequency=30JobAcctGatherType=jobacct_gather/none#SlurmctldDebug=3SlurmctldLogFile=/var/log/slurmctld.log#SlurmdDebug=3SlurmdLogFile=/var/log/slurmd.log### COMPUTE NODESNodeName=jamalvm[11-13] CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 State=UNKNOWNPartitionName=production Nodes=jamalvm[11-13] Default=YES MaxTime=INFINITE State=UP 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 1-1. install slurmfor i in &#123;121..123&#125;; do ssh root@192.168.1.$i yum install mailx -y ssh root@192.168.1.$i yum install ~/slurm_rpms/*.rpm -ydone# 1-2. setting up environmenttee ~/tmp.sh &lt;&lt; EOFmkdir /var/run/slurmchown slurm: /var/run/slurmchmod 755 /var/run/slurmmkdir /var/spool/slurmchown slurm: /var/spool/slurmchmod 755 /var/spool/slurmslurmd -CEOFbash ~/tmp.sh# NodeName=jamalvm11 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15869# UpTime=0-02:06:12for i in &#123;122..123&#125;; do scp ~/tmp.sh root@192.168.1.$i:~/ ssh root@192.168.1.$i bash ~/tmp.shdone# tmp.sh 100% 164 188.6KB/s 00:00# NodeName=jamalvm12 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15869# UpTime=0-02:06:24# tmp.sh 100% 164 172.7KB/s 00:00# NodeName=jamalvm13 CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=15869# UpTime=0-02:06:25sed -i -e 's/PIDFile=.*/PIDFile=\/var\/run\/slurm\/slurmctld.pid/g' /usr/lib/systemd/system/slurmctld.servicesed -i -e 's/PIDFile=.*/PIDFile=\/var\/run\/slurm\/slurmd.pid/g' /usr/lib/systemd/system/slurmd.servicefor i in &#123;122..123&#125;; do ssh root@192.168.1.$i &lt;&lt; EOFsed -i -e 's/PIDFile=.*/PIDFile=\/var\/run\/slurm\/slurmd.pid/g' /usr/lib/systemd/system/slurmd.serviceEOFdone# 1-3. start servicessystemctl enable slurmctldsystemctl start slurmctldcp /etc/slurm/cgroup.conf.example /etc/slurm/cgroup.conffor i in &#123;122..123&#125;; do scp /etc/slurm/cgroup.conf root@192.168.1.$i:/etc/slurm scp /etc/slurm/slurm.conf root@192.168.1.$i:/etc/slurmdonefor i in &#123;121..123&#125;; do ssh root@192.168.1.$i &lt;&lt; EOFsystemctl disable firewalldsystemctl stop firewalldsystemctl enable slurmdsystemctl start slurmdEOFdone# 1-4. checksinfo# PARTITION AVAIL TIMELIMIT NODES STATE NODELIST# production* up infinite 3 idle jamalvm[11-13] 4. Simple tests1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# test 1tee submit.sh &lt;&lt; EOF#!/bin/bash##SBATCH --job-name=test#SBATCH --output=res.txt#SBATCH --ntasks=1srun hostnameEOFsbatch submit.sh# test 2tee hello_mpi.c &lt;&lt; EOF/* "Hello World" MPI Test Program*/#include &lt;assert.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;mpi.h&gt;int main(int argc, char **argv)&#123; char buf[256]; int my_rank, num_procs; /* Initialize the infrastructure necessary for communication */ MPI_Init(&amp;argc, &amp;argv); /* Identify this process */ MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank); /* Find out how many total processes are active */ MPI_Comm_size(MPI_COMM_WORLD, &amp;num_procs); /* Until this point, all programs have been doing exactly the same. Here, we check the rank to distinguish the roles of the programs */ if (my_rank == 0) &#123; int other_rank; printf("We have %i processes.\n", num_procs); /* Send messages to all other processes */ for (other_rank = 1; other_rank &lt; num_procs; other_rank++) &#123; sprintf(buf, "Hello %i!", other_rank); MPI_Send(buf, sizeof(buf), MPI_CHAR, other_rank, 0, MPI_COMM_WORLD); &#125; /* Receive messages from all other process */ for (other_rank = 1; other_rank &lt; num_procs; other_rank++) &#123; MPI_Recv(buf, sizeof(buf), MPI_CHAR, other_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); printf("%s\n", buf); &#125; &#125; else &#123; /* Receive message from process #0 */ MPI_Recv(buf, sizeof(buf), MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); assert(memcmp(buf, "Hello ", 6) == 0), /* Send message to process #0 */ sprintf(buf, "Process %i reporting for duty.", my_rank); MPI_Send(buf, sizeof(buf), MPI_CHAR, 0, 0, MPI_COMM_WORLD); &#125; /* Tear down the communication infrastructure */ MPI_Finalize(); return 0;&#125;EOFmpicc hello_mpi.c -o hello.mpitee submit_mpi.sh &lt;&lt; EOF#!/bin/bash##SBATCH --job-name=test#SBATCH --output=res_mpi.txt#SBATCH --ntasks=12srun ~/F/hello.mpiEOFsbatch submit_mpi.sh]]></content>
      <categories>
        <category>SLURM</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SLURM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of VMs]]></title>
    <url>%2Fposts%2F201807%2F2018-07-26-installation-of-vms.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄在Windows安裝VM的一些紀錄 紀錄安裝ssh, sshpass以及VMware-tools 123456789101112131415161718192021222324252627282930313233343536373839404142ssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa -q -N ""echo cat ~/.ssh/id_rsa.pubyum install -y sshpassfor i in &#123;121..123&#125;; do ssh-keyscan 192.168.1.$i &gt;&gt; ~/.ssh/known_hosts sshpass -p XXXXXXXX ssh-copy-id root@192.168.1.$idonetee -a /etc/hosts &lt;&lt; EOF192.168.1.121 jamalvm11192.168.1.122 jamalvm12192.168.1.123 jamalvm13EOFfor i in &#123;122..123&#125;; do scp /etc/hosts root@192.168.1.$i:/etcdone## Installation of VMware-toolsyum install -y gitgit clone https://github.com/rasa/vmware-tools-patches.gitcd vmware-tools-patches./patched-open-vm-tools.shif [ -d "/etc/vmware-tools" ]; then perl /root/vmware-tools-patches/vmware-tools-patches/vmware-tools-distrib/bin/vmware-uninstall-tools.plfiperl /root/vmware-tools-patches/vmware-tools-patches/vmware-tools-distrib/vmware-install.pl# link host computer to guestfor i in &#123;121..123&#125;; do ssh root@192.168.1.$i ln -s /mnt/hgfs/F ~/Fdone# installation of Intel Parallel Studio XE 2017yum install gcc gcc-c++ gcc-gfortran -y./install.sh# step 1: Enter and key accept to accept End User License Agreement# step 3: to activate by using a license file =&gt; 1 to Activate offline =&gt; give the location of license# step 4: 2 to not help# step 5: 2 to go to Customize installation =&gt; cancel IA-32 installation# step 6: y to install now]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Compile Julia on Windows with Intel MKL]]></title>
    <url>%2Fposts%2F201807%2F2018-07-14-compile-julia-in-windows.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄在Windows編譯Julia 首先是安裝cygwin64，下載好exe之後，下指令安裝需要的東西: 1setup-x86_64.exe -s http://ftp.yzu.edu.tw/cygwin/ -q -P cmake,gcc-g++,git,make,patch,curl,m4,python,p7zip,mingw64-x86_64-gcc-g++,mingw64-x86_64-gcc-fortran 打開cygwin terminal: 123456789101112131415161718192021222324# clone source codegit clone https://github.com/JuliaLang/julia.gitmv julia julia-mastercd julia-master# create make infotee Make.user &lt;&lt; EOFXC_HOST = x86_64-w64-mingw32USE_INTEL_MKL = 1MKLROOT = /cygdrive/c/Program\ Files\ \(x86\)/IntelSWTools/compilers_and_libraries_2017/windows/mklEOF# 把dll複製過來MKLROOT=/cygdrive/c/Program\ Files\ \(x86\)/IntelSWTools/compilers_and_libraries_2017/windows/mklmkdir -p usr/bincp "$MKLROOT/../redist/intel64_win/compiler/libiomp5md.dll" usr/bin/cp "$MKLROOT/../redist/intel64_win/mkl/mkl*.dll" usr/bin/cp usr/bin/mkl_rt.dll usr/bin/libmkl_rt.dll# 開始編譯make -j 28# 編譯extramake win-extras# create binary distributionmake binary-dist 然後就會出現julia-53db863142-win64.exe類似這種檔案在你的Cygwin資料夾下面了，點擊兩下後就可以安裝了 下面比較測試一下OpenBLAS跟MKL (@Ryzen ThreadRipper 1950X@3.7GHz, 128 GB RAM) 程式碼如下 12345using LinearAlgebraA = Array(ones(m,n))B = Bidiagonal(ones(n,p), :U)@elapsed inv(B) * A@elapsed B * A 結果:123456789101112131415161718192021222324251. m = n = p = 5001 - openblasThe elapsed time of inv(B) * A: 0.18717380649999998The elapsed time of B * A: 0.18258008961 - mklThe elapsed time of inv(B) * A: 0.19427489685000002The elapsed time of B * A: 0.162375210852. m = n = p = 15002 - openblasThe elapsed time of inv(B) * A: 1.6726551640500003The elapsed time of B * A: 1.230330704352 - mklThe elapsed time of inv(B) * A: 1.6964857918000007The elapsed time of B * A: 1.13289885540000013. m = n = p = 25003 - openblasThe elapsed time of inv(B) * A: 4.77680297335The elapsed time of B * A: 3.53229717915000043 - mklThe elapsed time of inv(B) * A: 4.916846529599999The elapsed time of B * A: 3.2682480265000002 基本上互有勝負… 雖然我覺得根本就是Intel MKL對AMD處理器負優化…]]></content>
      <categories>
        <category>julia</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>julia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Create A Mirror of NPM Registry]]></title>
    <url>%2Fposts%2F201807%2F2018-07-13-npm-mirror.html</url>
    <content type="text"><![CDATA[這篇主要是創一個mirror of NPM Registry 首先是安裝CouchDB 1234567891011yum install epel-release -ytee /etc/yum.repos.d/npm_repo.repo &lt;&lt; EOF[apache-couchdb-rpm]name=bintray--apache-couchdb-rpmbaseurl=http://apache.bintray.com/couchdb-rpm/el7/$basearch/gpgcheck=0repo_gpgcheck=0enabled=1EOFyum repolist allyum install couchdb -y 接下來修改設定，然後啟動 123456sed -i '/\[httpd\]/a allow_jsonp = true' /opt/couchdb/etc/local.inised -i '/\[httpd\]/a secure_rewrites = false' /opt/couchdb/etc/local.inised -i '/\[httpd\]/a port = 8092' /opt/couchdb/etc/local.inised -i '/\[httpd\]/a bind_address = 0.0.0.0' /opt/couchdb/etc/local.inisystemctl start couchdbsystemctl enable couchdb 開防火牆 1234567891011firewall-cmd --add-port=8092/tcpfirewall-cmd --add-port=8094/tcpfirewall-cmd --reload# 測試localhostcurl http://localhost:8092/# &#123;"couchdb":"Welcome","version":"2.1.2","features":["scheduler"],"vendor":&#123;"name":"The Apache Software Foundation"&#125;&#125;# 測試網路位置curl http://192.168.1.116:8092/# &#123;"couchdb":"Welcome","version":"2.1.2","features":["scheduler"],"vendor":&#123;"name":"The Apache Software Foundation"&#125;&#125; 開始mirror: 1234567# create usercurl -X PUT http://localhost:8092/_config/admins/jamal -d '"jamal"'# create databasecurl -X PUT http://jamal:jamal@localhost:8092/registry# replicatecurl -X POST http://localhost:8092/_replicate -d '&#123;"source":"https://skimdb.npmjs.com/registry", "target":&#123;"headers":&#123;"Authorization":"Basic amFtYWw6amFtYWw="&#125;,"url":"http://192.168.1.116:8092/registry"&#125;, "continuous":true, "create_target": false, "user_ctx": &#123;"name": "jamal","roles":["_admin","_reader","_writer"]&#125;&#125;' -H "Content-Type: application/json"# amFtYWw6amFtYWw=為jamal:jamal的decode]]></content>
      <categories>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Extend The Disk Space of CentOS 7 VM]]></title>
    <url>%2Fposts%2F201807%2F2018-07-13-extend-disk-space-centos-7-vm.html</url>
    <content type="text"><![CDATA[這篇主要是紀錄怎麼在CentOS 7 VM上延展Disk 基本上，我是參考這篇做出來的 首先，當然你要先把VM的設定打開，將硬碟空間加大 之後，回到VM操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 對disk做操作fdisk /dev/sda# 用 p &gt; d &gt; 2 &gt; n &gt; p &gt; 2 &gt; t &gt; 2 &gt; 8e &gt; p &gt; w 這個順序來操作# p是print，d是刪除，裡面的2都是選擇第二個分區的意思# n是新增，t應該是align，8e是某個位置，w是寫入## d之前# Device Boot Start End Blocks Id System# /dev/sda1 * 2048 2099199 1048576 83 Linux# /dev/sda2 2099200 83886079 40893440 8e Linux LVM## t之後 (我把硬碟從40G變成600G)# Device Boot Start End Blocks Id System# /dev/sda1 * 2048 2099199 1048576 83 Linux# /dev/sda2 2099200 1258291199 628096000 8e Linux LVM# 然後重開reboot# resize 你修改的硬碟# 列出目前空間df -h# Filesystem Size Used Avail Use% Mounted on# /dev/mapper/centos_jamalvm01-root 35G 1.6G 34G 5% /# devtmpfs 3.8G 0 3.8G 0% /dev# tmpfs 3.9G 0 3.9G 0% /dev/shm# tmpfs 3.9G 12M 3.8G 1% /run# tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup# /dev/sda1 1014M 222M 793M 22% /boot# tmpfs 781M 0 781M 0% /run/user/0pvresize /dev/sda2# Physical volume "/dev/sda2" changed# 1 physical volume(s) resized / 0 physical volume(s) not resizedlvextend -l +100%FREE /dev/mapper/centos_jamalvm01-root# Size of logical volume centos_jamalvm01/root changed from &lt;35.00 GiB (8959 extents) to &lt;595.00 GiB (152319 extents).# Logical volume centos_jamalvm01/root successfully resized.xfs_growfs /# meta-data=/dev/mapper/centos_jamalvm01-root isize=512 agcount=4, agsize=2293504 blks# = sectsz=512 attr=2, projid32bit=1# = crc=1 finobt=0 spinodes=0# data = bsize=4096 blocks=9174016, imaxpct=25# = sunit=0 swidth=0 blks# naming =version 2 bsize=4096 ascii-ci=0 ftype=1# log =internal bsize=4096 blocks=4479, version=2# = sectsz=512 sunit=0 blks, lazy-count=1# realtime =none extsz=4096 blocks=0, rtextents=0# data blocks changed from 9174016 to 155974656# 列出目前空間df -h# Filesystem Size Used Avail Use% Mounted on# /dev/mapper/centos_jamalvm01-root 595G 1.6G 594G 1% /# devtmpfs 3.8G 0 3.8G 0% /dev# tmpfs 3.9G 0 3.9G 0% /dev/shm# tmpfs 3.9G 12M 3.8G 1% /run# tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup# /dev/sda1 1014M 222M 793M 22% /boot# tmpfs 781M 0 781M 0% /run/user/0 這樣就搞定了~~~]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installations of Go 1.10 in CentOS 7.4]]></title>
    <url>%2Fposts%2F201804%2F2018-04-25-installation-of-go-1.10-in-centos-7.4.html</url>
    <content type="text"><![CDATA[簡單記錄一下在CentOS 7.4安裝Go 1.10 12345678wget https://dl.google.com/go/go1.10.1.linux-amd64.tar.gztar -zxvf go1.10.1.linux-amd64.tar.gzmv go /usr/localtee -a /etc/profile &lt;&lt; "EOF"export GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binEOFsource /etc/profile]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PySpark with MongoDB]]></title>
    <url>%2Fposts%2F201804%2F2018-04-25-pyspark-with-mongodb.html</url>
    <content type="text"><![CDATA[這篇主要是要用PySpark去操作MongoDB 我的環境是windows 10 x64，下面會用到的檔案都有傳到github repo 需要的元件有： winutils repo mongo-spark repo IntelliJ Idea (IDE, 可用其他代替) 安裝篇簡單安裝一下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869tee /etc/yum.repos.d/mongodb-org-3.4.repo &lt;&lt; EOF[mongodb-org-3.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/redhat/\$releasever/mongodb-org/3.4/x86_64/gpgcheck=0enabled=1EOF#installyum install -y mongodb-org#ignore updatesed -i '$a exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools' /etc/yum.conf#disable selinuxsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinuxsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsetenforce 0#kernel settingsif [[ -f /sys/kernel/mm/transparent_hugepage/enabled ]];then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif [[ -f /sys/kernel/mm/transparent_hugepage/defrag ]];then echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfitee -a /etc/rc.local &lt;&lt;EOFif [[ -f /sys/kernel/mm/transparent_hugepage/enabled ]];then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif [[ -f /sys/kernel/mm/transparent_hugepage/defrag ]];then echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfiEOF#configuresed -i "s/bindIp:.*/bindIp: $(hostname -I)/" /etc/mongod.conf# start servicesystemctl enable mongodsystemctl start mongod# create usertee createUser.js &lt;&lt; EOFuse admin;db.createUser(&#123; user: "admin", pwd: "password", roles: [&#123;role: "root", db: "admin"&#125;]&#125;);EOFmongo 192.168.1.112/admin createUser.js# restart mongodsystemctl restart mongod# enable authorizationtee -a /etc/mongod.conf &lt;&lt; EOFsecurity: authorization: enabledEOF# open mongo shellmongo -u "admin" -p "password" -host 192.168.1.112 --authenticationDatabase "admin"# open port to connect mongodbsudo firewall-cmd --zone=public --add-port=27017/tcp --permanentsudo firewall-cmd --reload Spark環境下載winutils後，把hadoop-2.7.1丟到C:\hadoop 然後加一個環境變數HADOOP_HOME指向C:\hadoop\hadoop-2.7.1 然後用conda install pyspark把pyspark安裝起來，然後用pyspark指令打開來測試看看 執行PySparktest_script.py如下： 123import pysparkspark = pyspark.sql.SparkSession.builder.appName('test').getOrCreate()print(spark.range(10).collect()) 用下面指令來執行： 1spark-submit --master local[2] test_script.py 執行，就可以看到輸出為 1[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4), Row(id=5), Row(id=6), Row(id=7), Row(id=8), Row(id=9)] 這樣就代表pyspark環境完成了 IDEA跟編譯mongo spark connector接著打開Idea，安裝sbt, scala這兩個plugins 接著create一個scala的sbt專案於mongo-spark的資料夾 稍待一下，直接停掉idea的build job 打開Terminal打bash sbt check就會開始編譯 (要有mingw) 然後到sbt shell輸入asembly，就可以編譯出mongo spark connector的fat jar了 盪target/scala-2.11裡面就會有mongo-spark-connector-assembly-2.2.2.jar了 把mongo-spark-connector-assembly-2.2.2.jar丟到你的project下裡面 使用PySpark access mongo先來一個簡單的例子，mongo.py: 1234567891011121314from pyspark.sql import SparkSessionuri = "mongodb://admin:password@192.168.1.112"spark = SparkSession \ .builder \ .appName("myApp") \ .config("spark.mongodb.input.uri", uri + "/admin.test") \ .config("spark.mongodb.output.uri", uri + "/admin.test") \ .getOrCreate()people = spark.createDataFrame([("Bilbo Baggins", 50), ("Gandalf", 1000), ("Thorin", 195), ("Balin", 178), ("Kili", 77), ("Dwalin", 169), ("Oin", 167), ("Gloin", 158), ("Fili", 82), ("Bombur", None)], ["name", "age"])people.write.format("com.mongodb.spark.sql.DefaultSource").mode("append").save() 執行命令如下： 1spark-submit --master local[8] --jars mongo-spark-connector-assembly-2.2.2.jar --conf spark.driver.extraClassPath=mongo-spark-connector-assembly-2.2.2.jar --conf spark.executor.extraClassPath=mongo-spark-connector-assembly-2.2.2.jar mongo.py 用mongo shell執行就可以看到資料已經被塞進去了 1234567use admin;db.test.findOne();// &#123;// &quot;_id&quot; : ObjectId(&quot;5ae1e04885beb03a349ffbcf&quot;),// &quot;name&quot; : &quot;Dwalin&quot;,// &quot;age&quot; : NumberLong(169)// &#125; 再來是本次目標，我們想要用pyspark從一個csv塞入一個架構如下面這樣的資料： (舉一個可能類似婚友社的資料長相) 12345678910111213141516171819202122232425262728[&#123; "id": 1, "name": "Kevin", "sex": "man", "age": 27, "history": [ &#123; "pairId": 2, "statusCheck": "Interested", "memo": "Rose is following" &#125;, &#123; "pairId": 5, "statusCheck": "Not interested" &#125; ]&#125;, &#123; "id": 2, "name": "Cindy", "sex": "woman", "age": 22, "optional": [ &#123; "pairId": 1, "memo": "unchecked, need to be followed!" &#125; ]&#125;] profiles.csv檔案： 1234id,name,sex,age,pairId,statusCheck,memo1,Kevin,man,27,2,&quot;Interested&quot;,&quot;Rose is following&quot;1,Kevin,man,27,5,&quot;Not Interested&quot;,2,Cindy,woman,22,1,,&quot;unchecked, need to be followed!&quot; profile_data_builder.py檔案： 12345678910111213141516171819202122232425262728293031import pandas as pdfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import *from pyspark.sql.functions import struct, col, collect_listdf_pd = pd.read_csv("profile.csv")spark = SparkSession\ .builder\ .appName("dataTransform")\ .getOrCreate()schema = StructType( [StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("sex", StringType(), True), StructField("age", IntegerType(), True), StructField("pairId", StringType(), True), StructField("statusCheck", StringType(), True), StructField("memo", StringType(), True)])df = spark.createDataFrame(df_pd, schema)grpByCols = ["id", "name", "sex", "age"]historyCols = ["pairId", "statusCheck", "memo"]outDf = df.withColumn("history", struct(*[col(name) for name in historyCols]))\ .drop(*historyCols).groupBy(*grpByCols)\ .agg(collect_list(col("history")).alias("history"))uri = 'mongodb://admin:password@192.168.1.112'outDf.write.format("com.mongodb.spark.sql.DefaultSource")\ .option('uri', uri).option('database', 'admin').option('collection', 'profile')\ .mode("append").save() 最後用mongo shell查看塞進去的資料： 123456789101112131415161718192021use admin;db.profile.findOne();// &#123;// &quot;_id&quot; : ObjectId(&quot;5ae1ff1885beb047444fc948&quot;),// &quot;id&quot; : 1,// &quot;name&quot; : &quot;Kevin&quot;,// &quot;sex&quot; : &quot;man&quot;,// &quot;age&quot; : 27,// &quot;history&quot; : [// &#123;// &quot;pairId&quot; : &quot;2&quot;,// &quot;statusCheck&quot; : &quot;Interested&quot;,// &quot;memo&quot; : &quot;Rose is following&quot;// &#125;,// &#123;// &quot;pairId&quot; : &quot;5&quot;,// &quot;statusCheck&quot; : &quot;Not Interested&quot;,// &quot;memo&quot; : &quot;NaN&quot;// &#125;// ]// &#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Spark</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Call Golang Part 2]]></title>
    <url>%2Fposts%2F201804%2F2018-04-24-python-call-go-part2.html</url>
    <content type="text"><![CDATA[我們還可以利用cffi幫我們直接build module來供Python來使用 你可以在我的github repo找到這些程式碼(點我) 你可以一樣利用os.system來call go幫忙編譯成動態程式庫，提供Python來用 123456789101112131415import osos.system("go build -buildmode=c-shared -o libtest.dll test.go")from cffi import FFIffi = FFI()ffi.cdef("""typedef long long GoInt;void printBye();GoInt sum(GoInt p0, GoInt p1);""")lib = ffi.dlopen("libtest.dll")lib.printBye()lib.sum(1, 2) 然後就可以用cffi載入dll來呼叫函數，不這個例子還看不出來cffi有什麼差別 下面這個例子，可以讓你透過cffi來使用Go的struct 123456789101112131415161718192021222324252627282930313233343536373839package mainimport "C"import ( "fmt" "math" "sort" "sync")var count intvar mtx sync.Mutex//export Addfunc Add(a, b int) int &#123; return a + b&#125;//export Cosinefunc Cosine(x float64) float64 &#123; return math.Cos(x)&#125;//export Sortfunc Sort(vals []int) &#123; sort.Ints(vals)&#125;//export Logfunc Log(msg string) int &#123; mtx.Lock() defer mtx.Unlock() fmt.Println(msg) count++ return count&#125;func main() &#123;&#125; 上面是go檔案，下面是Python檔 1234567891011121314151617181920212223242526272829303132333435363738394041from cffi import FFIimport osos.system("go build -buildmode=c-shared -o awesome.dll awesome.go")ffi = FFI()ffi.cdef("""typedef long long GoInt;typedef struct &#123; void* data; GoInt len; GoInt cap;&#125; GoSlice;typedef struct &#123; const char *data; GoInt len;&#125; GoString;GoInt Add(GoInt a, GoInt b);double Cosine(double v);void Sort(GoSlice values);GoInt Log(GoString str);""")lib = ffi.dlopen("awesome.dll")print("awesome.Add(12,99) = %d" % lib.Add(12,99))print("awesome.Cosine(1) = %f" % lib.Cosine(1))data = ffi.new("GoInt[]", [74,4,122,9,12])nums = ffi.new("GoSlice*", &#123;'data':data, 'len':5, 'cap':5&#125;)lib.Sort(nums[0])print("awesome.Sort(74,4,122,9,12) = %s" % \ [ffi.cast("GoInt*", nums.data)[i] for i in range(nums.len)])data = ffi.new("char[]", b"Hello Python!")msg = ffi.new("GoString*", &#123;'data':data, 'len':13&#125;)print("log id %d" % lib.Log(msg[0]))]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyCharm</tag>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Call Golang]]></title>
    <url>%2Fposts%2F201804%2F2018-04-16-python-call-go.html</url>
    <content type="text"><![CDATA[Golang的速度極快，而且Go可以直接編譯成shared library 直接就譨讓Python使用ctypes載入，就能使用了 事先需求： 安裝好Go (版本&gt;=1.5)，而且Go要在環境變數裡(執行go -v要有東西XD) 123456789101112131415161718package mainimport "C"import "fmt"// 匯出printBye method//Export printByefunc printBye() &#123; fmt.Println("From DLL: Bye!")&#125;// 匯出 sum method//Export sumfunc sum(a int, b int) int &#123; return a + b;&#125;func main() &#123;&#125; // 一定要有 然後就可以執行下面兩行Python來編譯成shared library檔案 12import osos.system("go build -buildmode=c-shared -o libtest.dll test.go") 就可以在你的目錄下看到libtest.dll了，就載入後，就能使用裡面的method了 12345import ctypestest = ctypes.cdll.LoadLibrary("libtest.dll")test.printBye()test.sum(1, 2)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyCharm</tag>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kivy]]></title>
    <url>%2Fposts%2F201804%2F2018-04-16-kivy.html</url>
    <content type="text"><![CDATA[Kivy是Python開源的一個庫 他可以直接在Android, iOS, Linux, Max OS X跟Windows上直接執行 這篇會介紹下面三點： 安裝kivy 執行一個簡單的範例 安裝Kivy Designer，並設定可於PyCharm開啟 複雜一點的case 安裝kivy1234pip install docutils pygments pypiwin32 kivy.deps.sdl2 kivy.deps.glewpip install kivy.deps.gstreamerpip install kivypip install kivy_examples 簡單的kivy範例12345678from kivy.app import Appfrom kivy.uix.button import Buttonclass TestApp(App): def build(self): return Button(text='Test')TestApp().run() 執行之後就可以看到這樣結果 安裝Kivy Designer，並設定可於PyCharm開啟1234conda install watchdog pygments docutils jedi gitpython six kivy-gardengarden install xpopupcd C:\Users\%USERNAME%\git clone http://github.com/kivy/kivy-designer/ 然後打開PyCharm，到settings =&gt; Tools =&gt; External Tools 然後點+，新增一個工具，設定如下： 紅框部分就是你的Username，其位置可以根據你把你的kivy designer放哪兒決定 Program就用$ModuleSdkPath$，Arguments則是-m designer 然後在Project裡面點右鍵點External Tools就可以看到Kivy Designer了，點下去就會打開了! 複雜一點的caseExample連結 效果就會長這樣：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyCharm</tag>
        <tag>Kivy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyCharm and Cython]]></title>
    <url>%2Fposts%2F201804%2F2018-04-15-PyCharm-and-Cython.html</url>
    <content type="text"><![CDATA[這篇文章主要介紹PyCharm跟Cython 先寫下我的檔案架構 12345678.+-- main.py+-- fib| +-- __init__.py| +-- cfib.c| +-- cfib.h| +-- fib.pyx| +-- setup.py 每一個檔案的用途： main.py是主程式 __init__.py是套件引用檔案 cfib.c是用c寫成的fib函數 cfib.h是cfib的header檔，供fib.pyx參照 fib.pyx是Cython檔 setup.py則是拿來編譯整個Cython的 cfib.c內容如下： 123456789#include "cfib.h"unsigned long fib(unsigned long n) &#123; unsigned long a=0, b=1, i, tmp; for (i=0; i&lt;n; ++i) &#123; tmp = a; a = a + b; b = tmp; &#125; return a;&#125; cfib.h內容如下： 123456#ifndef __CFIB_H__#define __CFIB_H__unsigned long fib(unsigned long n);#endif 基本上cfib.c跟cfib.h就是很標準的c程式寫法，就不贅述，主要是拿來比performance用的 下面則是fix.pyx檔案，fib_c是定義引用的c函數 fib_cython是未優化過的cython程式，基本上就是python樣子 fib_cython_optimized是加上型別的cythoin程式 1234567891011121314151617181920cdef extern from &quot;cfib.h&quot;: unsigned long _fib &quot;fib&quot;(unsigned long n)def fib_c(n): &apos;&apos;&apos; Returns the nth Fibonacci number.&apos;&apos;&apos; return _fib(n)def fib_cython(n): &apos;&apos;&apos;Returns the nth Fibonacci number.&apos;&apos;&apos; a, b = 0, 1 for i in range(n): a, b = a + b, a return adef fib_cython_optimized(unsigned long n): &apos;&apos;&apos;Returns the nth Fibonacci number.&apos;&apos;&apos; cdef unsigned long a=0, b=1, i for i in range(n): a, b = a + b, a return a __init__.py檔案內容： 1from .fib import * setup.py檔案內容如下： 1234from distutils.core import setup, Extensionfrom Cython.Build import cythonizesetup(ext_modules = cythonize(Extension(name="fib", sources=["cfib.c", "fib.pyx"]))) 有了這幾個檔案之後，我們先來設定PyCharm 打開PyCharm，到settings =&gt; Tools =&gt; External Tools 然後點+，新增一個工具，設定如下： Porgram設定$ModuleSdkPath$，Arguments設定$FilePath$ build_ext --inplace，Working directory設定$FileDir$ 這樣基本上就大功告成了，然後回到Project對setup.py點右鍵，選External Tools，選CythonBuild就可以看到開始跑了 如果設定對的話，就可以看到編譯成功的訊息 最後就可以來跑跑看啦，main.py如下： 1234567891011121314151617181920212223def fib_python(n): '''Returns the nth Fibonacci number.''' a, b = 0, 1 for i in range(n): a, b = a + b, a return aif __name__ == '__main__': print("##### check result #####") import fib print("fib(47) in python:", fib_python(47)) print("fib.fib_c(47):", fib.fib_c(47)) print("fib.fib_cython(47):", fib.fib_cython(47)) print("fib.fib_cython_optimized(47):", fib.fib_cython_optimized(47)) print("\n##### performace benchmark #####") import timeit python_setup = "from __main__ import fib_python" cython_setup = "import fib" print("Python code: ", timeit.timeit('fib_python(47)', setup=python_setup), "seconds") print("Cython code: ", timeit.timeit('fib.fib_cython(47)', setup=cython_setup), "seconds") print("Optimized Cython code: ", timeit.timeit('fib.fib_cython_optimized(47)', setup=cython_setup), "seconds") print("C code: ", timeit.timeit('fib.fib_c(47)', setup=cython_setup), "seconds") 輸出結果就會是下面這樣： 1234567891011121314C:\Users\*****\Miniconda3\envs\idp\python.exe D:/Dropbox/Data/programming/Python/Cython/1_fib/main.py##### check result #####fib(47) in python: 2971215073fib.fib_c(47): 2971215073fib.fib_cython(47): 2971215073fib.fib_cython_optimized(47): 2971215073##### performace benchmark #####Python code: 2.9352622053858113 secondsCython code: 1.7331176511158422 secondsOptimized Cython code: 0.14643933094340067 secondsC code: 0.11884286952119272 secondsProcess finished with exit code 0]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyCharm</tag>
        <tag>Cython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installations of Python]]></title>
    <url>%2Fposts%2F201804%2F2018-04-14-Python-installation.html</url>
    <content type="text"><![CDATA[簡單記錄一些安裝命令，以及安裝Tensorflow、PyTorch，跟編譯PyTorch等 Intel Distributed Python稍微記錄一下怎麼用Anaconda建置虛擬環境，安裝Intel發行的Python 安裝miniconda之後，用下面指令可以載入Python的環境 (建議安裝all-user，並安裝到C:\Miniconda3，可以避免一些問題) 1C:\Miniconda3\Scripts\activate.bat 引入Intel channel，安裝intel發行的Python 12conda config --add channels intelconda create -n idp intelpython3_full python=3 安裝之後，啟用idp這個環境 1activate idp Tensorflow, Keras安裝方法建議不要更新html5lib，避免conda出現錯誤，無法再使用 1pip install tensorflow-gpu -I --no-update-deps PyTorch安裝方法直接利用conda安裝12C:\Miniconda3\Scripts\activate.bat idpC:\Miniconda3\Scripts\conda install -c peterjc123 pytorch cuda90 自行編譯安裝好VS2017, CUDA 9.0之後，執行下面 1234git clone https://github.com/peterjc123/pytorch-scripts.gitcd pytorch-scriptsC:\Miniconda3\Scripts\activate.bat idpcuda90.bat 編譯PyTorch延伸套件12345git clone https://github.com/pytorch/extension-ffi.git"C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Auxiliary\Build\vcvarsall.bat" x86_amd64C:\Miniconda3\Scripts\activate.bat idpcd extension-ffi/scriptpython build.py]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>MKL</tag>
        <tag>Keras</tag>
        <tag>PyTorch</tag>
        <tag>Tensorflow</tag>
        <tag>Intel</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Compilation of Python 3.5.5 in CentOS 7.4]]></title>
    <url>%2Fposts%2F201804%2F2018-04-25-installation-of-python-3.5.5-in-centos-7.4.html</url>
    <content type="text"><![CDATA[簡單記錄一下在CentOS自行編譯Python 3.5.5的過程 12345678910111213yum install gcc gcc-c++yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel expat-develyum install -y wgetwget https://www.python.org/ftp/python/3.5.5/Python-3.5.5.tgztar xzf Python-3.5.5.tgzcd Python-3.5.5# 極大可能會抓不到g++，先exportexport CXX=g++./configure --prefix=/usr/local --enable-shared --enable-optimizations --enable-profilingmake -j4make altinstallln -s /usr/local/lib/libpython3.5m.so.1.0 /usr/lib64/libpython3.5m.soln -s /usr/local/lib/libpython3.5m.so.1.0 /usr/lib64/libpython3.5m.so.1.0 12345678910# 從Oracle官網下載Oracle Instant Client 12.2 basic, devel and sqlplusyum install oracle-instanclient12.2* -ytee -a /etc/profile &lt;&lt; "EOF"export ORACLE_HOME=/usr/lib/oracle/12.2/client64export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/libexport PATH=$PATH:$ORACLE_HOME/binEOFsource /etc/profilepip3.5 install cx_Oracle]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R call javascript]]></title>
    <url>%2Fposts%2F201803%2F2018-03-15-R-call-javascript.html</url>
    <content type="text"><![CDATA[今天看到javascript call R似乎很容易 就稍微看一下R call javascript的方法 然後就看到V8這個套件，玩了一下覺得很驚豔XD 感覺javascript很適合拿來做一些R不容易做的功能XDD 12345678910111213141516171819202122232425262728293031323334353637383940414243library(V8)ctx &lt;- v8()ctx$source("http://underscorejs.org/underscore-min.js")ctx$eval("var list = [[0, 1], [2, 3], [4, 5]];")ctx$eval("var flat = _.reduceRight(list, function(a, b) &#123; return a.concat(b); &#125;, []);")ctx$get("flat")# [1] 4 5 2 3 0 1ctx$assign("x", 1:5)ctx$eval("var y = _.each(x, function(x)&#123; return x*3;&#125;)")ctx$get("y")# [1] 1 2 3 4 5x2 &lt;- 1:5ctx$call("_.each", x2, JS("function(x)&#123; return x*3;&#125;"))# [1] 1 2 3 4 5ctx$call("_.filter", mtcars, JS("function(x)&#123;return x.mpg &lt; 15&#125;"))# mpg cyl disp hp drat wt qsec vs am gear carb# Duster 360 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4# Cadillac Fleetwood 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4# Lincoln Continental 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4# Chrysler Imperial 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4# Camaro Z28 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4# work with npmSys.setenv(PATH = paste0(Sys.getenv("PATH"), ";C:\\Program Files\\nodejs;C:\\Users\\Jamal\\AppData\\Roaming\\npm"))Sys.setenv(NODE_PATH = "C:\\Program Files\\nodejs\\node_modules;C:\\Users\\Jamal\\AppData\\Roaming\\npm\\node_modules")system("npm install -g browserify js-beautify")writeLines("global.beautify = require('js-beautify');", "in.js")system("browserify in.js -o bundle.js")ct &lt;- v8()ct$source("bundle.js")test &lt;- "(function(x,y)&#123;x = x || 1; y = y || 1; return y * x;&#125;)(4, 9)"pretty_test &lt;- ct$call("pr.js_beautify", test, list(indent_size = 2))cat(pretty_test)# (function(x, y) &#123;# x = x || 1;# y = y || 1;# return y * x;# &#125;)(4, 9)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Swish Beta Function In Keras]]></title>
    <url>%2Fposts%2F201711%2F2017-11-19-Swish-Beta-Function-In-Keras.html</url>
    <content type="text"><![CDATA[Google Brain had published a paper: “Swish : A Self-Gated Activation Function” (Arxiv link). In this blogger, I use Keras API of customizing layer to fulfill the Swish Beta function mentioned in paper. Here is the source code: GitHub. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import kerasfrom keras import backend as Kfrom keras.datasets import mnistfrom keras.layers import Dense, Dropout, Activationfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling1Dfrom keras.layers import BatchNormalizationfrom keras.layers import initializers, InputSpecfrom keras.models import Sequentialfrom keras.engine.topology import Layerclass SwishBeta(Layer): def __init__(self, trainable_beta = False, beta_initializer = 'ones', **kwargs): super(SwishBeta, self).__init__(**kwargs) self.supports_masking = True self.trainable = trainable_beta self.beta_initializer = initializers.get(beta_initializer) def build(self, input_shape): self.beta = self.add_weight(shape=[1], name='beta', initializer=self.beta_initializer) self.input_spec = InputSpec(ndim=len(input_shape)) self.built = True def call(self, inputs): return inputs * K.sigmoid(self.beta * inputs) def get_config(self): config = &#123;'trainable_beta': self.trainable_beta, 'beta_initializer': initializers.serialize(self.beta_initializer)&#125; base_config = super(SwishBeta, self).get_config() return dict(list(base_config.items()) + list(config.items()))num_classes = 10img_rows, img_cols = 28, 28(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255.x_test /= 255.y_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(64, kernel_size=(3, 3), padding = 'same', kernel_initializer = 'he_uniform', input_shape=input_shape))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(Conv2D(128, (3, 3), padding = 'same', kernel_initializer = 'he_uniform'))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_uniform'))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_uniform'))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Conv2D(512, (3, 3), padding = 'same', kernel_initializer = 'he_uniform'))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(Conv2D(512, (3, 3), padding = 'same', kernel_initializer = 'he_uniform'))model.add(BatchNormalization())model.add(SwishBeta(True))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(GlobalAveragePooling2D())model.add(SwishBeta(True))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])history = model.fit(x_train, y_train, batch_size = 128, epochs = 500, verbose = 1, callbacks = [keras.callbacks.EarlyStopping(patience=7)], validation_data=(x_test, y_test))score = model.evaluate(x_test, y_test, verbose=0)print('Test loss:', score[0])print('Test accuracy:', score[1]) # 99.66%]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
        <tag>Keras</tag>
        <tag>Swish</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RcppArmadillo Uses NVBLAS]]></title>
    <url>%2Fposts%2F201710%2F2017-10-06-RcppArmadillo-Use-NVBLAS.html</url>
    <content type="text"><![CDATA[本篇主要介紹如何在RcppArmadillo去link NVBLAS來做運算 Set an environmental variables for CUDA_HOME. For Linux: 123456789export CUDA_HOME=/path/to/cuda/hometee -a `Rscript -e "R.home()"`/etc/Renviron &lt;&lt; EOFexport LD_PRELOAD=$CUDA_HOME/lib64/libnvblas.soEOFtee -a `Rscript -e "R.home()"`/etc/Rprofile.site &lt;&lt; EOFSys.setenv(PKG_CXXFLAgs = sprintf("-I%s/include", Sys.getenv("CUDA_HOME")))Sys.setenv(PKG_LIBS = sprintf("-L%s/lib64 -lnvblas", Sys.getenv("CUDA_HOME")))EOF For Windows: 12345tee -a `Rscript -e "R.home()"`/etc/Rprofile.site &lt;&lt; EOFdyn.load(paste0(Sys.getenv("CUDA_HOME"), "/bin/nvblas64_90.dll"))Sys.setenv(PKG_CXXFLAGS = sprintf("-I\"%s/include\"", Sys.getenv("CUDA_HOME")))Sys.setenv(PKG_LIBS = sprintf("-L\"%s/lib64\" -lnvblas", Sys.getenv("CUDA_HOME")))EOF Run R: 1234567891011121314151617181920212223242526272829303132333435363738394041424344# NVBLAS_CONFIG_FILEwrite(c("NVBLAS_LOGFILE nvblas.log", "NVBLAS_CPU_BLAS_LIB Rblas.dll", "NVBLAS_GPU_LIST ALL"), "nvblas.conf") # library Rcpp and RcppArmadillolibrary(Rcpp)library(RcppArmadillo)# Rcpp codecode &lt;- '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using Rcpp::_;using Rcpp::List;// [[Rcpp::export]]Rcpp::List arma_fastLm_chol(const arma::mat&amp; X, const arma::vec&amp; y) &#123; arma::mat xtx = X.t() * X; arma::vec coef = arma::solve(xtx, X.t() * y); arma::vec res = y - X*coef; arma::uword df = X.n_rows - X.n_cols; double s2 = arma::dot(res, res) / (double) df; arma::colvec se = arma::sqrt(s2 * arma::diagvec(arma::inv_sympd(xtx))); return List::create(_["coefficients"] = coef, _["stderr"] = se, _["df.residual"] = df);&#125;'# compile Rcpp codeRcpp::sourceCpp(code = code)# data generationn &lt;- 2e5Lp &lt;- 300Lmm &lt;- cbind(1, matrix(rnorm(n * (p - 1L)), nc = p-1L))y &lt;- mm %*% rnorm(p, sd = 3) + rnorm(n, sd = 5)# test functionsystem.time(&#123; arma_fastLm_chol(mm, y)&#125;)## cpu# user system elapsed# 7.03 0.22 0.45## gpu# user system elapsed# 0.60 0.25 0.38 My computer: AMD Threadripper 1950X with 128GB Ram and GTX 1080 SLI]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>NVBLAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp link system library]]></title>
    <url>%2Fposts%2F201708%2F2017-08-12-Rcpp-link-system-library.html</url>
    <content type="text"><![CDATA[這篇主要是介紹Linux環境下，如何直接在Rcpp裡面去link系統已有的library 這篇主要是因為我在測試Blaze為什麼效能遠不如直接跑blazemark的結果 所以就想說直接link系統裡面的armadillo, blaze試試看 我這裡讓Rcpp直接使用我系統安裝好的Intel MKL以及Armaillo當第一個例子 12345678910111213141516171819202122232425# armadilloSys.setenv(PKG_CPPFLAGS = paste("-I/opt/intel/compilers_and_libraries_2017.1.132/linux/mkl/include -I/usr/include", "-L/opt/intel/compilers_and_libraries_2017.1.132/linux/mkl/lib/intel64 -L/usr/lib64", "-lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm -larmadillo", "-O3 -mavx -DNDEBUG -fpermissive"), PKG_LIBS = paste("-L/usr/lib64 -larmadillo"))sourceCpp(code = "#include &lt;Rcpp.h&gt;#include &lt;mkl_blas.h&gt;#include &lt;armadillo&gt;// [[Rcpp::export]]Rcpp::NumericMatrix arma_XTX2(Rcpp::NumericMatrix Xs) &#123; int nr = Xs.nrow(), nc = Xs.ncol(), i_col, i_row, k; arma::mat X(Xs.begin(), nr, nc, false); arma::mat XTX = X.t() * X; Rcpp::NumericMatrix out(nc, nc); for( i_col=0, k=0 ; i_col &lt; nc; ++i_col)&#123; for( i_row = 0; i_row &lt; nc ; ++i_row, ++k )&#123; out[k] = XTX(i_row,i_col) ; &#125; &#125; return out;&#125;", verbose = TRUE, rebuild = TRUE) 下面是Rcpp link Blaze and MKL的例子 12345678910111213141516171819202122232425262728293031library(Rcpp)Sys.setenv(PKG_CPPFLAGS = paste("-I/opt/intel/compilers_and_libraries_2017.1.132/linux/mkl/include -I/usr/local/include/blaze", "-L/opt/intel/compilers_and_libraries_2017.1.132/linux/mkl/lib/intel64", "-lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm", "-Wall -Wextra -Werror -Woverloaded-virtual -ansi -O3 -mavx -DNDEBUG -fpermissive -std=c++14"))sourceCpp(code = "#include &lt;mkl_cblas.h&gt;#include &lt;Blaze.h&gt;#include &lt;Rcpp.h&gt;// [[Rcpp::export]]Rcpp::NumericMatrix blaze3_XTX3(Rcpp::NumericMatrix Xs) &#123; int nr = Xs.nrow(), nc = Xs.ncol(), i_col, i_row, k; blaze::DynamicMatrix&lt;double, blaze::columnMajor&gt; X(nr, nc); for( i_col=0; i_col &lt; nc; ++i_col)&#123; for( i_row = 0; i_row &lt; nc ; ++i_row )&#123; X(i_row, i_col) = Xs(i_row,i_col) ; &#125; &#125; blaze::DynamicMatrix&lt;double, blaze::rowMajor&gt; XT = blaze::trans(X); blaze::DynamicMatrix&lt;double, blaze::columnMajor&gt; XTX( XT * X ); Rcpp::NumericMatrix out(nc, nc); for( i_col=0, k=0 ; i_col &lt; nc; ++i_col)&#123; for( i_row = 0; i_row &lt; nc ; ++i_row, ++k )&#123; out[k] = XTX(i_row,i_col) ; &#125; &#125; return out;&#125;", verbose = TRUE, rebuild = TRUE)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>Intel C++</tag>
        <tag>R</tag>
        <tag>Rcpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[inconsolata.sty not found in CentOS]]></title>
    <url>%2Fposts%2F201707%2F2017-07-20-inconsolata-in-centos.html</url>
    <content type="text"><![CDATA[因為R編譯manual需要用到inconsolata 但是CentOS 7下的texlive沒有inconsolata.sty這個檔案，所以要自己安裝 先用yum install levien-inconsolata-fonts安裝字型 再來是安裝tex檔案： 123456789101112wget http://mirrors.ctan.org/install/fonts/inconsolata.tds.zipmkdir inconsolataunzip inconsolata.tds.zip -d inconsolatasudo cp -r inconsolata/* /usr/share/texmfsudo mktexlsrtee -a /usr/share/texlive/texmf-dist/web2c/updmap.cfg &lt;&lt; EOFMap zi4.mapEOFsudo yum install perl-Digest-MD5sudo updmap-sys --force 這樣就可以快樂的編譯R manual了！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CentOS</tag>
        <tag>TexLive</tag>
        <tag>inconsolata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用R來建立web service]]></title>
    <url>%2Fposts%2F201705%2F2017-05-25-use-R-to-construct-web-service.html</url>
    <content type="text"><![CDATA[使用httpuv來建立簡單的R web service 簡單的GET服務，傳入x, y跟species (optional)來取得png raw vector： 123456789101112131415161718192021222324252627282930313233343536library(httpuv)library(urltools)library(pipeR)library(Cairo)library(png)library(lattice)app &lt;- list( call = function(req) &#123; if(req$REQUEST_METHOD != "GET") return(list(status = 400L, headers = list('Content-Type' = 'plain/text'), body = "Bad Request")) params &lt;- param_get(req$QUERY_STRING, c("x", "y", "species")) if (is.na(params$x) || is.na(params$y)) return(list(status = 400L, headers = list('Content-Type' = 'plain/text'), body = "Bad Request with wrong query string")) iris2 &lt;- iris if (!is.na(params$species)) &#123; if (nchar(params$species) &gt; 0 &amp;&amp; params$species %in% unique(iris$Species)) iris2 &lt;- subset(iris, Species == params$species) else return(list(status = 400L, headers = list('Content-Type' = 'plain/text'), body = "Bad Request with wrong species")) &#125; Cairo(640, 640, "/dev/null", bg = "white") print(xyplot(iris2[[params$x]], iris2[[params$y]], xlab = params$x, ylab = params$y)) output &lt;- Cairo.capture() %&gt;&gt;% writePNG dev.off() return(list(status = 200L, headers = list('Content-Type' = 'image/png'), body = output)) &#125;)runServer("0.0.0.0", 9454, app, 250) 驗證程式： 123456789101112131415161718library(httr)library(pipeR)library(jsonlite)GET("http://localhost:9454", query = list(x = "Sepal.Length", y = "Sepal.Width")) %&gt;&gt;% content("raw") %&gt;&gt;% writeBin("test.png")# test.png is a pngGET("http://localhost:9454", query = list(x = "Sepal.Length", y = "Sepal.Width")) %&gt;&gt;% content("raw") %&gt;&gt;% writeBin("test.png")# test.png is a pngGET("http://localhost:9454", query = list(x = "Sepal.Length", y = "Sepal.Width", species = "")) %&gt;&gt;% content("text")# "Bad Request with wrong species"POST("http://localhost:9454", query = list(table = "xxx", id = "123")) %&gt;&gt;% content("text")# Bad Request 簡單的POST服務，傳入包含x, y值的json檔案畫圖，然後回傳 123456789101112131415161718192021app &lt;- list( call = function(req) &#123; if (req$REQUEST_METHOD != "POST") return(list(status = 400L, headers = list('Content-Type' = 'plain/text'), body = "Bad Request")) plotDF &lt;- fromJSON(req$rook.input$read_lines()) if (!all(c("x", "y") %in% names(plotDF))) return(list(status = 400L, headers = list('Content-Type' = 'plain/text'), body = "Bad Request with wrong arguments in json")) Cairo(640, 640, "/dev/null", bg = "white") print(xyplot(y ~ x, plotDF)) output &lt;- Cairo.capture() %&gt;&gt;% writePNG dev.off() return(list(status = 200L, headers = list('Content-Type' = 'image/png'), body = output)) &#125;)runServer("0.0.0.0", 9454, app, 250) 驗證程式： 1234567library(httr)library(pipeR)library(jsonlite)POST("http://localhost:9454", body = toJSON(data.frame(x = 1:6, y = 2:7))) %&gt;&gt;% content("raw") %&gt;&gt;% writeBin("test.png")# test.png is a png]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>httpuv</tag>
        <tag>Web Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Windows環境下編譯GPU版本的mxnet]]></title>
    <url>%2Fposts%2F201704%2F2017-04-24-building-gpu-version-of-mxnet-in-windows.html</url>
    <content type="text"><![CDATA[本篇主要介紹如何在Windows環境下編譯GPU版本的mxnet 需要的components有： Visual Studio 2015 Update 3以上 OpenBLAS或是Intel MKL 2017 (我這用Intel MKL 2017) CUDA Toolkit 8.0 下載安裝 cuDNN 需要登入會員 CMake 下載Windows win64-x64 ZIP，然後解壓縮 OpenCV 下載最新版本的3.2.0 win pack，然後解壓縮 為了說明方便，假設在D槽開一個資料夾，叫做mxnet 先把整個mxnet repository clone到D:\mxnet\mxnet，然後開啟CMake\bin\cmake-gui.exe (可能開啟會錯誤，先檢查一下bin下面所有檔案，右鍵內容，右下角是否有解除鎖定的按鈕) 然後where is the source code選D:\mxnet\mxnet，然後按下configure 他會先問你要用什麼編譯，選VS 2015 Win64，然後問是否要開一個新資料夾for build，按下Yes繼續 接下來會說找不到BLAS，那我這裡要用MKL，所以BLAS那個選項選MKL，然後再按一次configure (如果要用OpenBLAS就直接把OpenBLAS_INCLUDE_DIR跟OpenBLAS_LIB修改上去即可) 然後會跳出INTEL_ROOT, MKL_INCLUDE, MKL_ROOT這三個選項，設定好相對的路徑後按下configure 接下來會問OpenCV的位置，一樣設定路徑之後再按一次configure，最後應該會看到下面這樣的配置 然後再按一次configure不會跳出任何錯誤後，按下Generate，下方出現Generate Done之後 按下Open Project就會打開Visual Studio 2015，接下來點方案&#39;mxnet&#39;右鍵選擇建置方案 大概等個一小時之後就build完了 再來是安裝R套件，請先在D:\mxnet\mxnet\R-package\inst建一個libs資料夾，裡面再建一個x64資料夾 然後把D:\mxnet\mxnet\Debug\libmxnet.dll, CUDA路徑下bin的cublas64_80.dll, cudart64_80.dll, cudnn64_5.dll, curand64_80.dll跟nvrtc64_80.dll以及opencv路徑下的bin\opencv_ffmpeg320_64.dll, x64\vc14\bin\opencv_world320.dll跟x64\vc14\bin\opencv_world320d.dll複製到剛剛建立的R-package\inst\x64裡面 然後把INTEL ROOT下面的redist\intel64_win\mkl\mkl_rt.dll, redist\intel64_win\mkl\mkl_intel_thread.dll, redist\intel64_win\mkl\mkl_avx.dll, redist\intel64_win\mkl\mkl_vml_avx.dll (不同電腦用的指令集不同，不一定是用這兩個DLL) 以及redist\intel64_win\mkl\libimalloc.dll放到R目錄下的bin\x64裡面 (要抓哪些DLL是根據dependencywalker找的，請查看dependencywalker，不過MKL部分是我自己試出來的) 再來是建立一個R-package\inst\include的資料夾，把D:\mxnet\mxnet\mshadow\mshadow, D:\mxnet\mxnet\dmlc-core\include\dmlc, D:\mxnet\mxnet\nnvm\include\nnvm, D:\mxnet\mxnet\include\mxnet複製到R-package\inst\include 跑下面這個script 1234567echo import(Rcpp) &gt; R-package/NAMESPACEecho import(methods) &gt;&gt; R-package/NAMESPACER CMD INSTALL R-packageRscript -e "require(mxnet); mxnet:::mxnet.export(\"R-package\")"rm -rf R-package/NAMESPACERscript -e "require(roxygen2); roxygen2::roxygenise(\"R-package\")"R CMD INSTALL R-package 安裝之後就到D:\mxnet\mxnet\example\image-classification試跑看看train_mnist.R --network mlp --gpus 0 或是簡單測一下下面的script: 12345678910111213141516171819202122232425require(mlbench)require(mxnet)data(Sonar, package = "mlbench")Sonar[,61] &lt;- as.numeric(Sonar[,61])-1train.ind &lt;- c(1:50, 100:150)train.x &lt;- data.matrix(Sonar[train.ind, 1:60])train.y &lt;- Sonar[train.ind, 61]test.x &lt;- data.matrix(Sonar[-train.ind, 1:60])test.y &lt;- Sonar[-train.ind, 61]mx.set.seed(0)# GPUmodel &lt;- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax", num.round = 20, array.batch.size=15, learning.rate=0.07, momentum=0.9, eval.metric=mx.metric.accuracy, device = mx.gpu(0))# CPUmodel &lt;- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax", num.round = 20, array.batch.size=15, learning.rate=0.07, momentum=0.9, eval.metric=mx.metric.accuracy, device = lapply(0:1, mx.cpu))preds &lt;- predict(model, test.x, mx.gpu(0))pred.label = max.col(t(preds))-1table(pred.label, test.y) 至於Python套件部分，到python的子資料夾，執行python setup.py install 然後把上面所說的mkl_rt.dll, mkl_core.dll, mkl_intel_thread.dll, pencv_ffmpeg320_64.dll, opencv_world320.dll以及opencv_world320d.dll放到Python的根目錄，然後到D:\mxnet\mxnet\example\image-classification 試跑下面的命令驗證看看GPU是否正常安裝： 1python train_mnist.py --network mlp --gpus 0 最後用python稍微看一下Performance差異： Performance of prebuilt GPU:1234567891011121314151617181920212223242526# GPUINFO:root:Epoch[0] Batch [100] Speed: 49999.99 samples/sec Train-accuracy=0.783261INFO:root:Epoch[0] Batch [200] Speed: 49999.99 samples/sec Train-accuracy=0.909062INFO:root:Epoch[0] Batch [300] Speed: 50393.66 samples/sec Train-accuracy=0.931875INFO:root:Epoch[0] Batch [400] Speed: 49999.99 samples/sec Train-accuracy=0.934375INFO:root:Epoch[0] Batch [500] Speed: 51200.00 samples/sec Train-accuracy=0.935781INFO:root:Epoch[0] Batch [600] Speed: 49999.99 samples/sec Train-accuracy=0.950469INFO:root:Epoch[0] Batch [700] Speed: 49612.42 samples/sec Train-accuracy=0.950469INFO:root:Epoch[0] Batch [800] Speed: 50000.08 samples/sec Train-accuracy=0.949531INFO:root:Epoch[0] Batch [900] Speed: 49612.33 samples/sec Train-accuracy=0.957812INFO:root:Epoch[0] Train-accuracy=0.957348INFO:root:Epoch[0] Time cost=1.629INFO:root:Epoch[0] Validation-accuracy=0.959793# CPUINFO:root:Epoch[0] Batch [100] Speed: 42666.71 samples/sec Train-accuracy=0.790687INFO:root:Epoch[0] Batch [200] Speed: 42105.21 samples/sec Train-accuracy=0.906250INFO:root:Epoch[0] Batch [300] Speed: 42384.10 samples/sec Train-accuracy=0.928438INFO:root:Epoch[0] Batch [400] Speed: 41830.03 samples/sec Train-accuracy=0.942656INFO:root:Epoch[0] Batch [500] Speed: 41025.67 samples/sec Train-accuracy=0.946250INFO:root:Epoch[0] Batch [600] Speed: 42105.28 samples/sec Train-accuracy=0.942344INFO:root:Epoch[0] Batch [700] Speed: 42105.28 samples/sec Train-accuracy=0.950469INFO:root:Epoch[0] Batch [800] Speed: 41025.60 samples/sec Train-accuracy=0.955937INFO:root:Epoch[0] Batch [900] Speed: 42384.10 samples/sec Train-accuracy=0.958594INFO:root:Epoch[0] Train-accuracy=0.953125INFO:root:Epoch[0] Time cost=1.853INFO:root:Epoch[0] Validation-accuracy=0.962082 Performance of built by myself (RelWithDebInfo):1234567891011121314151617181920212223242526# GPUINFO:root:Epoch[0] Batch [100] Speed: 52459.03 samples/sec Train-accuracy=0.781405INFO:root:Epoch[0] Batch [200] Speed: 54700.81 samples/sec Train-accuracy=0.907656INFO:root:Epoch[0] Batch [300] Speed: 56637.21 samples/sec Train-accuracy=0.925000INFO:root:Epoch[0] Batch [400] Speed: 55172.44 samples/sec Train-accuracy=0.940156INFO:root:Epoch[0] Batch [500] Speed: 51200.00 samples/sec Train-accuracy=0.941406INFO:root:Epoch[0] Batch [600] Speed: 55652.17 samples/sec Train-accuracy=0.944063INFO:root:Epoch[0] Batch [700] Speed: 54237.27 samples/sec Train-accuracy=0.952812INFO:root:Epoch[0] Batch [800] Speed: 55172.44 samples/sec Train-accuracy=0.954844INFO:root:Epoch[0] Batch [900] Speed: 55172.33 samples/sec Train-accuracy=0.960313INFO:root:Epoch[0] Train-accuracy=0.949324INFO:root:Epoch[0] Time cost=1.493INFO:root:Epoch[0] Validation-accuracy=0.952130# CPUINFO:root:Epoch[0] Batch [100] Speed: 42105.28 samples/sec Train-accuracy=0.790842INFO:root:Epoch[0] Batch [200] Speed: 41830.03 samples/sec Train-accuracy=0.907813INFO:root:Epoch[0] Batch [300] Speed: 41558.43 samples/sec Train-accuracy=0.927188INFO:root:Epoch[0] Batch [400] Speed: 42105.28 samples/sec Train-accuracy=0.934844INFO:root:Epoch[0] Batch [500] Speed: 42105.28 samples/sec Train-accuracy=0.944531INFO:root:Epoch[0] Batch [600] Speed: 42384.10 samples/sec Train-accuracy=0.944063INFO:root:Epoch[0] Batch [700] Speed: 42384.10 samples/sec Train-accuracy=0.950469INFO:root:Epoch[0] Batch [800] Speed: 42105.28 samples/sec Train-accuracy=0.957812INFO:root:Epoch[0] Batch [900] Speed: 42384.10 samples/sec Train-accuracy=0.957812INFO:root:Epoch[0] Train-accuracy=0.959882INFO:root:Epoch[0] Time cost=1.789INFO:root:Epoch[0] Validation-accuracy=0.962082 Performance of built by myself (Release with Intel C++ 2017, /O3 flags):1234567891011121314151617181920212223242526# GPUINFO:root:Epoch[0] Batch [100] Speed: 60377.39 samples/sec Train-accuracy=0.791151INFO:root:Epoch[0] Batch [200] Speed: 59259.24 samples/sec Train-accuracy=0.913438INFO:root:Epoch[0] Batch [300] Speed: 60952.37 samples/sec Train-accuracy=0.927031INFO:root:Epoch[0] Batch [400] Speed: 57657.63 samples/sec Train-accuracy=0.938906INFO:root:Epoch[0] Batch [500] Speed: 61538.41 samples/sec Train-accuracy=0.943594INFO:root:Epoch[0] Batch [600] Speed: 60377.39 samples/sec Train-accuracy=0.945000INFO:root:Epoch[0] Batch [700] Speed: 61538.55 samples/sec Train-accuracy=0.945781INFO:root:Epoch[0] Batch [800] Speed: 62135.82 samples/sec Train-accuracy=0.952500INFO:root:Epoch[0] Batch [900] Speed: 60952.37 samples/sec Train-accuracy=0.957031INFO:root:Epoch[0] Train-accuracy=0.960726INFO:root:Epoch[0] Time cost=1.386INFO:root:Epoch[0] Validation-accuracy=0.962878# CPUINFO:root:Epoch[0] Batch [100] Speed: 52458.92 samples/sec Train-accuracy=0.786665INFO:root:Epoch[0] Batch [200] Speed: 52892.64 samples/sec Train-accuracy=0.911094INFO:root:Epoch[0] Batch [300] Speed: 52892.54 samples/sec Train-accuracy=0.930781INFO:root:Epoch[0] Batch [400] Speed: 52032.56 samples/sec Train-accuracy=0.933594INFO:root:Epoch[0] Batch [500] Speed: 53333.38 samples/sec Train-accuracy=0.941094INFO:root:Epoch[0] Batch [600] Speed: 52892.64 samples/sec Train-accuracy=0.945781INFO:root:Epoch[0] Batch [700] Speed: 52892.54 samples/sec Train-accuracy=0.950156INFO:root:Epoch[0] Batch [800] Speed: 53333.28 samples/sec Train-accuracy=0.957031INFO:root:Epoch[0] Batch [900] Speed: 53333.38 samples/sec Train-accuracy=0.958125INFO:root:Epoch[0] Train-accuracy=0.953125INFO:root:Epoch[0] Time cost=1.506INFO:root:Epoch[0] Validation-accuracy=0.962480 可以看出我們自己用Intel C++ 2017 加上 O3 optimization的速度 在GPU方面比prebuilt每秒多了一千張，CPU部分則多了將近10000張 The performace gain worths your time!!]]></content>
      <categories>
        <category>MxNet</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>MxNet</tag>
        <tag>Windows</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduce to foreach and iterators]]></title>
    <url>%2Fposts%2F201704%2F2017-04-23-introduce-to-foreach-and-iterators.html</url>
    <content type="text"><![CDATA[Introduction *apply functions, Reduce and do.call我一直沒時間寫*apply系列函數的介紹 不過最近用了foreach跟iterators 發現其實這兩個套件就可以完全取代*apply系列函數 而且對新手來說寫起來也不容易搞錯 這裡先簡介一下*apply系列函數 apply 123m &lt;- matrix(1:6, 3)# input matrix, output vector/matrix/listapply(m, 1, sum) # output vector 1## [1] 5 7 9 1apply(m, 1, `*`, 2) # output matrix 123## [,1] [,2] [,3]## [1,] 2 4 6## [2,] 8 10 12 1apply(m, 1, function(v) rep(v[1], v[2])) # output list 12345678## [[1]]## [1] 1 1 1 1## ## [[2]]## [1] 2 2 2 2 2## ## [[3]]## [1] 3 3 3 3 3 3 lapply 12# input vector/list, output listlapply(1:3, `*`, 2) # input vector 12345678## [[1]]## [1] 2## ## [[2]]## [1] 4## ## [[3]]## [1] 6 1lapply(as.list(1:3), `*`, 2) # input list 12345678## [[1]]## [1] 2## ## [[2]]## [1] 4## ## [[3]]## [1] 6 sapply 12# input vector/list, output vector/matrix/listsapply(1:3, `*`, 2) # input vector, output vector 1## [1] 2 4 6 1sapply(1:3, rep, each = 2) # input vector, output matrix 123## [,1] [,2] [,3]## [1,] 1 2 3## [2,] 1 2 3 1sapply(1:3, rep, x = 2) # input vector, output list 12345678## [[1]]## [1] 2## ## [[2]]## [1] 2 2## ## [[3]]## [1] 2 2 2 1sapply(as.list(1:3), `*`, 2) # input list, output vector 1## [1] 2 4 6 1sapply(as.list(1:3), rep, each = 2) # input list, output matrix 123## [,1] [,2] [,3]## [1,] 1 2 3## [2,] 1 2 3 1sapply(as.list(1:3), rep, x = 2) # input list, output list 12345678## [[1]]## [1] 2## ## [[2]]## [1] 2 2## ## [[3]]## [1] 2 2 2 mapply 12# multiple inputs, output vector/matrix/list stick list output by using `SIMPLIFY = FALSE`mapply(function(x, y) x + y, 1:3, as.list(2:4)) # input vector and list, output vector 1## [1] 3 5 7 1mapply(function(x, y) rep(x * y, 2), 1:3, as.list(2:4)) # input vector and list, output matrix 123## [,1] [,2] [,3]## [1,] 2 6 12## [2,] 2 6 12 1mapply(function(x, y) x + y, 1:3, as.list(2:4), SIMPLIFY = FALSE) # input vector and list, output list 12345678## [[1]]## [1] 3## ## [[2]]## [1] 5## ## [[3]]## [1] 7 1mapply(function(x, y) rep(x, y), 1:3, as.list(2:4)) # input vector and list, output list 12345678## [[1]]## [1] 1 1## ## [[2]]## [1] 2 2 2## ## [[3]]## [1] 3 3 3 3 tapply 12# input two vectors, one is value and the other one is group. output vector/matrixtapply(1:6, rep(1:2, 3), sum) 12## 1 2 ## 9 12 1tapply(1:6, rep(1:2, 3), `+`, 1) 12345## $`1`## [1] 2 4 6## ## $`2`## [1] 3 5 7 簡單掃過apply系列之後，有沒有覺得光要弄懂輸出什麼格式就很折磨人了… 這裡額外提一下do.call這個指令 do.call是do a function call的意思 所以如果我們要把list的資料合併再一起 那我們就可以執行一個c/cbind/rbind的function call 把list裡面的element當成input丟入這個function call，例子如下： 1do.call(c, list(1, 2, 3)) 1## [1] 1 2 3 1do.call(cbind, list(1:2, 2:3, 3:4)) 123## [,1] [,2] [,3]## [1,] 1 2 3## [2,] 2 3 4 1do.call(rbind, list(1:2, 2:3, 3:4)) 1234## [,1] [,2]## [1,] 1 2## [2,] 2 3## [3,] 3 4 這裡可能有人學過Reduce這個函數 雖然寫法也差不多，像是：Reduce(c, list(1, 2, 3))這樣 但是其實差異還是滿大的，我這邊簡單呈現一下do.call跟Reduce的差異 假設一個list有四個elements，那Reduce跟do.call分別的做法如下： 1c(c(c(1, 2), 3), 4) # &lt;= Reduce的做法 1## [1] 1 2 3 4 1c(1, 2, 3, 4) # &lt;= do.call的做法 1## [1] 1 2 3 4 可以看出Reduce是兩兩做，所以他在內部處理的時候需要不斷擴充output vector/matrix的長度 但是do.call是一次做完，所以do.call就不需要擴充長度 效率方面可以自己動手測測看，會比較有感 那Reduce要幹嘛用？其實像是operator都只能輸入兩個參數，這情境就只能用Reduce 或是有一個list，element都是data.frame，你要根據每個element的id欄位做合併 那這時候你能用的函數也只有merge而已，簡單範例如下： 12listDF &lt;- list(data.frame(id = 1:5, V1 = rnorm(5)), data.frame(id = 1:5, V2 = rnorm(5)), data.frame(id = 1:5, V3 = rnorm(5)))Reduce(function(x, y) merge(x, y, by = "id"), listDF) 123456## id V1 V2 V3## 1 1 0.5019189 1.9036192 -1.6574498## 2 2 -1.2667358 0.3022502 0.1295988## 3 3 -0.7827332 -3.4698064 0.4290014## 4 4 -1.6435900 1.0705252 2.0689225## 5 5 -0.4572341 -0.1567469 -0.1389343 講那麼久，還沒到正題Orz foreach接下來，我們來進入正題吧！ foreach的用法相當直覺，我們從最簡單的case開始： 123foreach(x = 1:3) %do% &#123; x + 1&#125; 12345678## [[1]]## [1] 2## ## [[2]]## [1] 3## ## [[3]]## [1] 4 輸入一個vector: 1:3，對每一個element做加一的動作，然後輸出是list 除非調整foreach的參數，不然輸出絕對是list，這是foreach第一個特性 foreach這個函數會得到一個物件，而透過%do%這個operator就可以執行後面的命令 {}是可以省略的，但是只限於一行，而且那一行不能是用operator的運算，不然會出現像是下面的錯誤 123tryCatch(&#123; foreach(x = 1:3) %do% x + 1&#125;, error = function(e) e) 1## &lt;simpleError in foreach(x = 1:3) %do% x + 1: 二元運算子中有非數值引數&gt; 如果還是不要{}，可以用下面這樣，把operator當成函數來用即可： 1foreach(x = 1:3) %do% (x + 1) 12345678## [[1]]## [1] 2## ## [[2]]## [1] 3## ## [[3]]## [1] 4 再來，我們介紹怎麼改成vector/matrix輸出 我們只要在foreach這個命令裡面加上.combine這個屬性 那他裡面放的是我們要把結果合併的function，舉例如下： 123foreach(x = 1:3, .combine = c) %do% &#123; x + 1&#125; # vector 1## [1] 2 3 4 123foreach(x = 1:3, .combine = cbind) %do% &#123; c(x, x + 1)&#125; # matrix 123## result.1 result.2 result.3## [1,] 1 2 3## [2,] 2 3 4 123foreach(x = 1:3, .combine = rbind) %do% &#123; c(x, x + 1)&#125; # matrix 1234## [,1] [,2]## result.1 1 2## result.2 2 3## result.3 3 4 那這裡的.combine用的是Reduce，如果要用do.call的話，要在加上一個參數，如： 123foreach(x = 1:3, .combine = c, .multicombine = TRUE) %do% &#123; x + 1&#125; # vector 1## [1] 2 3 4 123foreach(x = 1:3, .combine = cbind, .multicombine = TRUE) %do% &#123; c(x, x + 1)&#125; # matrix 123## result.1 result.2 result.3## [1,] 1 2 3## [2,] 2 3 4 123foreach(x = 1:3, .combine = rbind, .multicombine = TRUE) %do% &#123; c(x, x + 1)&#125; # matrix 1234## [,1] [,2]## result.1 1 2## result.2 2 3## result.3 3 4 foreach還有幾個參數，稍微帶過，有些參數之後會再細講 .inorder是輸出是否要照順序，.maxcombine是最大結合數量 .errorhandling則是對錯誤的處理方式，verbose則是顯示更多訊息已提供user debug用 至於.package, .export跟.noexport，我們留到後面再解釋 剛剛上面看到foreach可以input vector, output list/vector/matrix了 但是我們還沒試過是不是list也行，舉個小範例： 1foreach(x = as.list(1:3)) %do% rep(x, x) # list 12345678## [[1]]## [1] 1## ## [[2]]## [1] 2 2## ## [[3]]## [1] 3 3 3 1foreach(x = as.list(1:3), .combine = c, .multicombine = TRUE) %do% rep(x, x) # vector 1## [1] 1 2 2 3 3 3 1foreach(x = as.list(1:3), .combine = cbind, .multicombine = TRUE) %do% rep(x, 2) # matrix 123## result.1 result.2 result.3## [1,] 1 2 3## [2,] 1 2 3 那這樣其實就可以完全取代sapply跟lapply的功能了 再來是foreach也可以支援多個input，把mapply的範例重新用foreach寫一次： 123foreach(x = 1:3, y = as.list(2:4)) %do% &#123; x + y&#125; 12345678## [[1]]## [1] 3## ## [[2]]## [1] 5## ## [[3]]## [1] 7 1foreach(x = 1:3, y = as.list(2:4)) %do% rep(x * y, 2) 12345678## [[1]]## [1] 2 2## ## [[2]]## [1] 6 6## ## [[3]]## [1] 12 12 123foreach(x = 1:3, y = as.list(2:4)) %do% &#123; x + y&#125; 12345678## [[1]]## [1] 3## ## [[2]]## [1] 5## ## [[3]]## [1] 7 1foreach(x = 1:3, y = as.list(2:4)) %do% rep(x, y) 12345678## [[1]]## [1] 1 1## ## [[2]]## [1] 2 2 2## ## [[3]]## [1] 3 3 3 3 但是foreach除了取代sapply跟lapply之外 其實foreach還有很棒的debug功能： 1print("i" %in% ls()) 1## [1] FALSE 1234result &lt;- foreach(i = 1:3) %do% &#123; i + 1&#125;if ("i" %in% ls()) print(i) 1## [1] 3 可以看出來foreach會自動assign i，讓我們能夠直接把i帶入{}裡面直接執行裡面的程式，看最後一次的執行結果 另外，foreach還會提示是在哪一個task中失敗的： 123456tryCatch(&#123; foreach(x = 1:3) %do% &#123; if (x == 2) stop("") &#125;&#125;, error = function(e) e) 1## &lt;simpleError in &#123; if (x == 2) stop(&quot;&quot;)&#125;: task 2 failed - &quot;&quot;&gt; 上面的輸出可以看的出來就是task 2 failed，所以我們只要用x=2帶進{}裡面就可以找到問題了！ 介紹到這，foreach還只是用到30%而已，這裡我要再介紹搭配同為Revolution R Analysis出的iterators來使用 iterators一開頭介紹的*apply系列函數還有兩個沒有用foreach取代 這節就是要來用iteratos的功能來取代apply跟tapply 要取代apply就是我們要讓foreach能一次存取一個列或是一個行 那麼就是用iter這個函數，然後藉由by這個參數來控制要算列還是行 123m &lt;- matrix(1:6, 3)# input matrix, output vector/matrix/listforeach(v = iter(m, by = "row"), .combine = c, .multicombine = TRUE) %do% sum(v) 1## [1] 5 7 9 1foreach(v = iter(m, by = "row"), .combine = cbind, .multicombine = TRUE) %do% (v * 2) 12## [,1] [,2] [,3] [,4] [,5] [,6]## [1,] 2 8 4 10 6 12 1foreach(v = iter(m, by = "row")) %do% rep(v[1], v[2]) 12345678## [[1]]## [1] 1 1 1 1## ## [[2]]## [1] 2 2 2 2 2## ## [[3]]## [1] 3 3 3 3 3 3 1foreach(v = iter(m, by = "column"), .combine = c, .multicombine = TRUE) %do% sum(v) 1## [1] 6 15 1foreach(v = iter(m, by = "column"), .combine = cbind, .multicombine = TRUE) %do% (v * 2) 1234## [,1] [,2]## [1,] 2 8## [2,] 4 10## [3,] 6 12 1foreach(v = iter(m, by = "column")) %do% rep(v[1], v[2]) 12345## [[1]]## [1] 1 1## ## [[2]]## [1] 4 4 4 4 4 如果是array的話就用iapply這個函數來取代iter，舉例如下： 1234arr &lt;- array(1:8, rep(2, 3))foreach(v = iapply(arr, 1)) %do% &#123; v&#125; # iterate over rows 123456789## [[1]]## [,1] [,2]## [1,] 1 5## [2,] 3 7## ## [[2]]## [,1] [,2]## [1,] 2 6## [2,] 4 8 123foreach(v = iapply(arr, 2)) %do% &#123; v&#125; # iterate over columns 123456789## [[1]]## [,1] [,2]## [1,] 1 5## [2,] 2 6## ## [[2]]## [,1] [,2]## [1,] 3 7## [2,] 4 8 123foreach(v = iapply(arr, 3)) %do% &#123; v&#125; # iterate over slice 123456789## [[1]]## [,1] [,2]## [1,] 1 3## [2,] 2 4## ## [[2]]## [,1] [,2]## [1,] 5 7## [2,] 6 8 123foreach(v = iapply(arr, 2:3)) %do% &#123; v&#125; # # iterate over all the columns of all the matrices 1234567891011## [[1]]## [1] 1 2## ## [[2]]## [1] 3 4## ## [[3]]## [1] 5 6## ## [[4]]## [1] 7 8 tapply的部分則是要引進isplit這個函數 只是要注意isplit的iterator跟前面使用方式不同 要用值的話，要用iterator的value，不能直接拿來用 它另外一個info是包含key，也就是用來分割的group value是什麼 1str(isplit(1:6, rep(1:2, 3))$nextElem()) 1234## List of 2## $ value: int [1:3] 1 3 5## $ key :List of 1## ..$ : int 1 123foreach(it = isplit(1:6, rep(1:2, 3)), .combine = c, .multicombine = TRUE) %do% &#123; sum(it$value)&#125; 1## [1] 9 12 123foreach(it = isplit(1:6, rep(1:2, 3))) %do% &#123; it$value + 1&#125; 12345## [[1]]## [1] 2 4 6## ## [[2]]## [1] 3 5 7 多組group vector的話，可以這樣做： 12splitList &lt;- list(rep(1:2, 3), c(1, 1, 1, 3, 3, 3))str(isplit(1:6, splitList)$nextElem()) 12345## List of 2## $ value: int [1:2] 1 3## $ key :List of 2## ..$ : int 1## ..$ : num 1 123foreach(it = isplit(1:6, splitList), .combine = c, .multicombine = TRUE) %do% &#123; sum(it$value)&#125; 1## [1] 4 2 5 10 123foreach(it = isplit(1:6, splitList)) %do% &#123; it$value + 1&#125; 1234567891011## [[1]]## [1] 2 4## ## [[2]]## [1] 3## ## [[3]]## [1] 6## ## [[4]]## [1] 5 7 但是isplit除了搭配vector之外，也可以搭配data.frame來使用： 12345DF &lt;- data.frame(x = rep(1:3, 3), y = c(rep(1:2, 4), 3))foreach(it = isplit(DF, with(DF, list(x = x, y = y)))) %do% &#123; print(it$key) nrow(it$value)&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253## $x## [1] 1## ## $y## [1] 1## ## $x## [1] 2## ## $y## [1] 1## ## $x## [1] 3## ## $y## [1] 1## ## $x## [1] 1## ## $y## [1] 2## ## $x## [1] 2## ## $y## [1] 2## ## $x## [1] 3## ## $y## [1] 2## ## $x## [1] 1## ## $y## [1] 3## ## $x## [1] 2## ## $y## [1] 3## ## $x## [1] 3## ## $y## [1] 3 1234567891011121314151617181920212223242526## [[1]]## [1] 2## ## [[2]]## [1] 1## ## [[3]]## [1] 1## ## [[4]]## [1] 1## ## [[5]]## [1] 2## ## [[6]]## [1] 1## ## [[7]]## [1] 0## ## [[8]]## [1] 0## ## [[9]]## [1] 1 這裡需要注意的是isplit不是現有組合有存在才會取值 而是將你輸入的group vector分別取unique後做expand.grid的動作展開 所以會有組合是沒有資料的，要記得在script裡面寫如果零列要怎麼處理： 123456DF &lt;- data.frame(x = rep(1:3, 3), y = c(rep(1:2, 4), 3))foreach(it = isplit(DF, with(DF, list(x = x, y = y))), .combine = rbind, .multicombine = TRUE) %do% &#123; if (nrow(it$value) == 0) return(NULL) data.frame(it$key, sum(it$value))&#125; 12345678## x y sum.it.value.## 1 1 1 4## 2 2 1 3## 3 3 1 4## 4 1 2 3## 5 2 2 8## 6 3 2 5## 7 3 3 6]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>do.call</tag>
        <tag>foreach</tag>
        <tag>iterators</tag>
        <tag>apply</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RcppArmadillo call F77 blas/lapack]]></title>
    <url>%2Fposts%2F201704%2F2017-04-23-RcppArmadillo-call-F77-blas-lapacl.html</url>
    <content type="text"><![CDATA[前幾篇有提過在Rcpp, RcppEigen中call F77 blas/lapack 最近也大概抓到RcppArmadillo call F77的方法 因為Armadillo本身就有宣告BLAS跟LAPACK functions 其實只需要include Armadillo宣告的header file即可 要include的是這三個檔案： 123#include &lt;armadillo_bits/typedef_elem.hpp&gt;#include &lt;armadillo_bits/def_blas.hpp&gt;#include &lt;armadillo_bits/def_lapack.hpp&gt; 不過我試了一下Armadillo並沒有import R全部的BLAS/LAPACK functions 所以遇到沒有的就要自己宣告，我這提供一個簡單例子宣告自己R有但Armadillo沒有的LAPACK function： 123456789101112131415161718192021222324252627// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;armadillo_bits/typedef_elem.hpp&gt;#include &lt;armadillo_bits/def_blas.hpp&gt;#include &lt;armadillo_bits/def_lapack.hpp&gt;#if !defined(ARMA_BLAS_CAPITALS)#define arma_dposv dposv#else#define arma_dposv DPOSV#endifextern "C" &#123; void arma_fortran(arma_dposv)(char* trans, blas_int* n, blas_int* nrhs, double* a, blas_int* lda, double* b, blas_int* ldb, blas_int* info);&#125;// [[Rcpp::export]]arma::vec wlssolver(arma::mat X, arma::vec w, arma::vec y)&#123; blas_int info = 0, nrhs = 1, k = blas_int(X.n_cols); char uplo = 'L'; arma::mat XWX = X.t() * (X.each_col() % w); arma::vec Xy = X.t() * (y % w); arma_fortran(arma_dposv)(&amp;uplo, &amp;k, &amp;nrhs, XWX.memptr(), &amp;k, Xy.memptr(), &amp;k, &amp;info); return(Xy);&#125; 上面這個script是用lapack中解正定對稱矩陣linear system的函數去求WLS的迴歸係數 簡單的R執行範例： 1234567n &lt;- 20p &lt;- 3X &lt;- matrix(rnorm(n * p), n , p)beta &lt;- rnorm(p)w &lt;- rgamma(nrow(X), 2, 0.5)y &lt;- 3 + X %*% beta + rnorm(n)wlssolver(X, w, y)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>BLAS</tag>
        <tag>LAPACK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MxNet所提供的csv data iterator]]></title>
    <url>%2Fposts%2F201704%2F2017-04-16-mxnet-io-csviter.html</url>
    <content type="text"><![CDATA[這篇是參考mxnet的一個example來的來源 而該篇所使用的資料是這一個比賽NDSB-II所提供的，請點我 登入Kaggle把Data裡面的四個zip下載下來 sample_submission_validate.csv.zip train.csv.zip train.zip validate.zip MxNet提供了一個Python去做Preprocessing 因為MxNet沒有提供R版本，所以我就寫了一版R的processing放在github上Repo連結 從Kaggle下載zip下來之後，放到一個資料夾(為了說明方便，這個資料夾就叫做root path)裡面 並從我的github repository clone下我建立的R專案，先跑preprocessing.R 接下來，我們就可以跑MxNet的train.R了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170# Train.R for Second Annual Data Science Bowl# Deep learning model with GPU support# Please refer to https://mxnet.readthedocs.org/en/latest/build.html#r-package-installation# for installation guiderequire(mxnet)require(data.table)##A lenet style net, takes difference of each frame as input.get.lenet &lt;- function() &#123; source &lt;- mx.symbol.Variable("data") source &lt;- (source-128) / 128 frames &lt;- mx.symbol.SliceChannel(source, num.outputs = 30) diffs &lt;- list() for (i in 1:29) &#123; diffs &lt;- c(diffs, frames[[i + 1]] - frames[[i]]) &#125; diffs$num.args = 29 source &lt;- mxnet:::mx.varg.symbol.Concat(diffs) net &lt;- mx.symbol.Convolution(source, kernel = c(5, 5), num.filter = 40) net &lt;- mx.symbol.BatchNorm(net, fix.gamma = TRUE) net &lt;- mx.symbol.Activation(net, act.type = "relu") net &lt;- mx.symbol.Pooling( net, pool.type = "max", kernel = c(2, 2), stride = c(2, 2) ) net &lt;- mx.symbol.Convolution(net, kernel = c(3, 3), num.filter = 40) net &lt;- mx.symbol.BatchNorm(net, fix.gamma = TRUE) net &lt;- mx.symbol.Activation(net, act.type = "relu") net &lt;- mx.symbol.Pooling( net, pool.type = "max", kernel = c(2, 2), stride = c(2, 2) ) # first fullc flatten &lt;- mx.symbol.Flatten(net) flatten &lt;- mx.symbol.Dropout(flatten) fc1 &lt;- mx.symbol.FullyConnected(data = flatten, num.hidden = 600) # Name the final layer as softmax so it auto matches the naming of data iterator # Otherwise we can also change the provide_data in the data iter return(mx.symbol.LogisticRegressionOutput(data = fc1, name = 'softmax'))&#125;network &lt;- get.lenet()batch_size &lt;- 32# CSVIter is uesed here, since the data can't fit into memorydata_train &lt;- mx.io.CSVIter( data.csv = "train-64x64-data.csv", data.shape = c(64, 64, 30), label.csv = "train-systole.csv", label.shape = 600, batch.size = batch_size)data_validate &lt;- mx.io.CSVIter( data.csv = "validate-64x64-data.csv", data.shape = c(64, 64, 30), batch.size = 1)# Custom evaluation metric on CRPS.mx.metric.CRPS &lt;- mx.metric.custom("CRPS", function(label, pred) &#123; pred &lt;- as.array(pred) label &lt;- as.array(label) for (i in 1:dim(pred)[2]) &#123; for (j in 1:(dim(pred)[1] - 1)) &#123; if (pred[j, i] &gt; pred[j + 1, i]) &#123; pred[j + 1, i] = pred[j, i] &#125; &#125; &#125; return(sum((label - pred) ^ 2) / length(label))&#125;)# Training the stytole netmx.set.seed(0)stytole_model &lt;- mx.model.FeedForward.create( X = data_train, ctx = mx.gpu(0), symbol = network, num.round = 65, learning.rate = 0.001, wd = 0.00001, momentum = 0.9, eval.metric = mx.metric.CRPS)# Predict stytolestytole_prob = predict(stytole_model, data_validate)# Training the diastole netnetwork = get.lenet()batch_size = 32data_train &lt;- mx.io.CSVIter( data.csv = "./train-64x64-data.csv", data.shape = c(64, 64, 30), label.csv = "./train-diastole.csv", label.shape = 600, batch.size = batch_size )diastole_model = mx.model.FeedForward.create( X = data_train, ctx = mx.gpu(0), symbol = network, num.round = 65, learning.rate = 0.001, wd = 0.00001, momentum = 0.9, eval.metric = mx.metric.CRPS)# Predict diastolediastole_prob = predict(diastole_model, data_validate)accumulate_result &lt;- function(validate_lst, prob) &#123; t &lt;- read.table(validate_lst, sep = ",") p &lt;- cbind(t[,1], t(prob)) dt &lt;- as.data.table(p) return(dt[, lapply(.SD, mean), by = V1])&#125;stytole_result = as.data.frame(accumulate_result("./validate-label.csv", stytole_prob))diastole_result = as.data.frame(accumulate_result("./validate-label.csv", diastole_prob))train_csv &lt;- read.table("./train-label.csv", sep = ',')# we have 2 person missing due to frame selection, use udibr's hist result insteaddoHist &lt;- function(data) &#123; res &lt;- rep(0, 600) for (i in 1:length(data)) &#123; for (j in round(data[i]):600) &#123; res[j] = res[j] + 1 &#125; &#125; return(res / length(data))&#125;hSystole = doHist(train_csv[, 2])hDiastole = doHist(train_csv[, 3])res &lt;- read.table("data/sample_submission_validate.csv", sep = ",", header = TRUE, stringsAsFactors = FALSE)submission_helper &lt;- function(pred) &#123; for (i in 2:length(pred)) &#123; if (pred[i] &lt; pred[i - 1]) &#123; pred[i] = pred[i - 1] &#125; &#125; return(pred)&#125;for (i in 1:nrow(res)) &#123; key &lt;- unlist(strsplit(res$Id[i], "_"))[1] target &lt;- unlist(strsplit(res$Id[i], "_"))[2] if (key %in% stytole_result$V1) &#123; if (target == 'Diastole') &#123; res[i, 2:601] &lt;- submission_helper(diastole_result[which(diastole_result$V1 == key), 2:601]) &#125; else &#123; res[i, 2:601] &lt;- submission_helper(stytole_result[which(stytole_result$V1 == key), 2:601]) &#125; &#125; else &#123; if (target == 'Diastole') &#123; res[i, 2:601] &lt;- hDiastole &#125; else &#123; res[i, 2:601] &lt;- hSystole &#125; &#125;&#125;write.table(res, file = "submission.csv", sep = ",", quote = FALSE, row.names = FALSE) 本篇的重點在於下面這段R，用MxNet提供的mx.io.CSVIter去batch的訓練Net模型 而這裡的train-64x64-data.csv，每一行都是經過resized的30張圖片，所以data.shape是64 x 64 x 30 而label則每一行是長度600的binary vector，其shape設定成600 然後給好batch.size，MxNet就可以批次的從csv抓資料出來train模型了 不用一股腦地把資料全部匯入到R裡面再做，不然再多的記憶體也用不完Orz 12345data_train &lt;- mx.io.CSVIter( data.csv = "train-64x64-data.csv", data.shape = c(64, 64, 30), label.csv = "train-systole.csv", label.shape = 600, batch.size = batch_size)]]></content>
      <categories>
        <category>MxNet</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>MxNet</tag>
        <tag>Deep Learning</tag>
        <tag>Data Iterator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transform Variables of R data.frame]]></title>
    <url>%2Fposts%2F201704%2F2017-04-14-transform-variables-of-data-frame.html</url>
    <content type="text"><![CDATA[有人在PTT問說，怎樣轉換R data.frame的多行column，連結：PTT文章 提醒：下面程式沒有判斷factor，如有factor請自行加入判斷式 12345678910111213141516171819202122232425262728# data.table做法：library(data.table)DT[ , lapply(.SD, function(x)iconv(x,"UTF8", "BIG5"))] # 如果有numeric或是integer column的話：DT[ , lapply(.SD, function(x)&#123; if (is.character(x)) iconv(x,"UTF8", "BIG5")&#125; else return(x)&#125;)] # dplyr做法：library(dplyr)DF %&gt;% mutate_each(funs(iconv(., "UTF8", "BIG5"))) # 如果有numeric或是integer column的話：DF %&gt;% mutate_if(is.character, funs(iconv(., "UTF8", "BIG5"))) # base函數解法：evalExpr &lt;- lapply(names(DF), function(x)&#123; bquote(iconv(.(as.symbol(x)), "UTF8", "BIG5"))&#125;)do.call(function(...) transform(DF, ...), evalExpr) # 如果有numeric或是integer column的話：evalExpr &lt;- lapply(names(DF)[sapply(DF, is.character)], function(x) bquote(iconv(.(as.symbol(x)), "UTF8", "BIG5")))do.call(function(...) transform(DF, ...), evalExpr) 當然也可以直接用column name帶入去做轉換： 123456789lapply(names(DF), function(x)&#123; iconv(DF[[x]], "UTF8", "BIG5")&#125;)# 或是lapply(names(DF)[sapply(DF, is.character)], function(x)&#123; iconv(DF[[x]], "UTF8", "BIG5")&#125;) 只是除了要多轉一次data.frame之外，第二個還要把numeric, integer column併回去]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.frame</tag>
        <tag>variable transformation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grid search in data.table]]></title>
    <url>%2Fposts%2F201704%2F2017-04-10-grid-search-in-data.table.html</url>
    <content type="text"><![CDATA[有人傳了一篇用tidyverse做grid search的blogger給我看 (Grid search in the tidyverse) 我想說那我來寫一篇for data.table的吧 code如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344library(data.table)library(pipeR)library(rpart)set.seed(245)dataDT &lt;- data.table(mtcars) %&gt;&gt;% `[`(j = am := factor(am, labels = c("Automatic", "Manual")))dataDT[ , trainFlag := !is.na(match(.I, sample.int(nrow(mtcars), floor(.8 * nrow(mtcars)))))]buildMdl &lt;- function(s, d) &#123; rpart(am ~ hp + mpg, dataDT[trainFlag == TRUE], control = rpart.control(minsplit = s, maxdepth = d))&#125;gs &lt;- CJ(minsplit = c(2, 5, 10), maxdepth = c(1, 3, 8))gs[ , mod := list(mapply(buildMdl, minsplit, maxdepth, SIMPLIFY = FALSE))]print(gs)# minsplit maxdepth mod# 1: 2 1 &lt;rpart&gt;# 2: 2 3 &lt;rpart&gt;# 3: 2 8 &lt;rpart&gt;# 4: 5 1 &lt;rpart&gt;# 5: 5 3 &lt;rpart&gt;# 6: 5 8 &lt;rpart&gt;# 7: 10 1 &lt;rpart&gt;# 8: 10 3 &lt;rpart&gt;# 9: 10 8 &lt;rpart&gt;calAccu &lt;- function(mod, testData, testLabel) &#123; mean(predict(mod, testData, type = "class") == testLabel)&#125;gs[ , `:=`(trainAccu = mapply(function(m) dataDT[trainFlag == TRUE] %&gt;&gt;% &#123;calAccu(m, ., .$am)&#125;, mod), testAccu = mapply(function(m) dataDT[trainFlag == FALSE] %&gt;&gt;% &#123;calAccu(m, ., .$am)&#125;, mod))]setorder(gs, -testAccu, -trainAccu)print(gs)# minsplit maxdepth mod trainAccu testAccu# 1: 2 8 &lt;rpart&gt; 1.00 0.8571429# 2: 2 3 &lt;rpart&gt; 0.92 0.8571429# 3: 5 3 &lt;rpart&gt; 0.88 0.8571429# 4: 5 8 &lt;rpart&gt; 0.88 0.8571429# 5: 2 1 &lt;rpart&gt; 0.84 0.7142857# 6: 5 1 &lt;rpart&gt; 0.84 0.7142857# 7: 10 1 &lt;rpart&gt; 0.84 0.7142857# 8: 10 3 &lt;rpart&gt; 0.84 0.7142857# 9: 10 8 &lt;rpart&gt; 0.84 0.7142857 我是覺得寫起來有點麻煩XD，改成用foreach + iterators我覺得會好很多，code如下： 1234567891011121314151617181920212223242526272829303132library(data.table)library(pipeR)library(rpart)library(foreach)library(iterators)set.seed(245)dataDT &lt;- data.table(mtcars) %&gt;&gt;% `[`(j = am := factor(am, labels = c("Automatic", "Manual")))dataDT[ , trainFlag := !is.na(match(.I, sample.int(nrow(mtcars), floor(.8 * nrow(mtcars)))))]resDT &lt;- CJ(minsplit = c(2, 5, 10), maxdepth = c(1, 3, 8)) %&gt;&gt;% &#123; foreach(i = isplit(., as.list(.)), .final = rbindlist) %do% &#123; mod &lt;- rpart(am ~ hp + mpg, dataDT[trainFlag == TRUE], control = do.call(rpart.control, i$value)) trainAccu &lt;- dataDT[trainFlag == TRUE] %&gt;&gt;% &#123;mean(predict(mod, ., type = "class") == .$am)&#125; testAccu &lt;- dataDT[trainFlag == FALSE] %&gt;&gt;% &#123;mean(predict(mod, ., type = "class") == .$am)&#125; return(cbind(i$value, data.table(mod = list(mod), trainAccu = trainAccu, testAccu = testAccu))) &#125; &#125; %&gt;&gt;% setorder(-testAccu, -trainAccu)print(resDT)# minsplit maxdepth mod trainAccu testAccu# 1: 2 8 &lt;rpart&gt; 1.00 0.8571429# 2: 2 3 &lt;rpart&gt; 0.92 0.8571429# 3: 5 3 &lt;rpart&gt; 0.88 0.8571429# 4: 5 8 &lt;rpart&gt; 0.88 0.8571429# 5: 2 1 &lt;rpart&gt; 0.84 0.7142857# 6: 5 1 &lt;rpart&gt; 0.84 0.7142857# 7: 10 1 &lt;rpart&gt; 0.84 0.7142857# 8: 10 3 &lt;rpart&gt; 0.84 0.7142857# 9: 10 8 &lt;rpart&gt; 0.84 0.7142857 isplit那裏是shadow copy，所以應該不會造成什麼問題 這樣寫會比用tidyverse或是直接data.table + mapply來的直覺，而且程式會相對精簡很多]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>Grid Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp object checking]]></title>
    <url>%2Fposts%2F201703%2F2017-03-31-Rcpp-object-check.html</url>
    <content type="text"><![CDATA[自己在編寫套件的時候，會想要把錯誤發生可能降到最低 所以常常做很多事前check，結果發現寫來寫去都是一樣的code在複製 所以終於下定決心來寫一個general的code去做這個 這樣C++就可以直接把所有輸入都改成用SEXP輸入做check 最後再轉成需要的class去做後續 C++ code如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include &lt;string&gt;#include &lt;sstream&gt;#include &lt;Rcpp.h&gt;template &lt;typename T&gt;std::string num2str(T Number) &#123; std::ostringstream ss; ss &lt;&lt; Number; return ss.str();&#125;// [[Rcpp::export]]void checkValue(SEXP x, const std::string varName = "x", const int RTYPE = 14, const int len = 1)&#123; int n = LENGTH(x); if (len &gt; 0) &#123; if (n != len) Rcpp::stop("The length of " + varName + " must be " + num2str(len) + "!\n"); &#125; if (TYPEOF(x) != RTYPE) &#123; switch(RTYPE) &#123; case LGLSXP: Rcpp::stop(varName + " must be logical type!\n"); case INTSXP: Rcpp::stop(varName + " must be integer type!\n"); case REALSXP: Rcpp::stop(varName + " must be double type!\n"); case STRSXP: Rcpp::stop(varName + " must be string type!\n"); case CPLXSXP: Rcpp::stop(varName + " must be complex type!\n"); default: Rcpp::stop("Not supported type!\n"); &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; switch(TYPEOF(x)) &#123; case LGLSXP: if (LOGICAL(x)[i] == NA_LOGICAL) Rcpp::stop(varName + " must not contain NA!\n"); break; case INTSXP: if (INTEGER(x)[i] == NA_INTEGER) Rcpp::stop(varName + " must not contain NA!\n"); break; case REALSXP: if (ISNA(REAL(x)[i]) || ISNAN(REAL(x)[i]) || !R_FINITE(REAL(x)[i])) Rcpp::stop(varName + " must not contain NA, NaN or Inf!\n"); break; case STRSXP: if (STRING_ELT(x, i) == NA_STRING) Rcpp::stop(varName + " must not contain NA!\n"); break; case CPLXSXP: if (ISNA(COMPLEX(x)[i].r) || ISNAN(COMPLEX(x)[i].r) || !R_FINITE(COMPLEX(x)[i].r) || ISNA(COMPLEX(x)[i].i) || ISNAN(COMPLEX(x)[i].i) || !R_FINITE(COMPLEX(x)[i].i)) Rcpp::stop(varName + " must not contain NA, NaN or Inf!\n"); break; default: Rcpp::stop("Not supported type!\n"); &#125; &#125;&#125;// [[Rcpp::export]]double testFunc(SEXP x)&#123; switch(TYPEOF(x)) &#123; case INTSXP: checkValue(x, "x", INTSXP, -1); // len &lt; 0 to skip checking length return(sum(Rcpp::as&lt;Rcpp::IntegerVector&gt;(x))); case REALSXP: checkValue(x, "x", REALSXP, -1); // len &lt; 0 to skip checking length return(sum(Rcpp::as&lt;Rcpp::NumericVector&gt;(x))); default: Rcpp::stop("Not supported type!\n"); &#125;&#125; 我們也create一個testFunc，去做對double/integer的vector做總和 那我們就在input時做一個checking 至於其他case，為了方便測試，我們這裡就用R來測試 但別忘了這個函數真正要用地方是在C++，測試的code如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Rcpp::sourceCpp("checkValue.cpp")testFunc(sample.int(10, 5)) # passtestFunc(rnorm(5)) # pass# check typeexpect_error(testFunc(c(TRUE, FALSE)), "Not supported type!")# cehck NAexpect_error(testFunc(c(sample.int(10, 3), NA_integer_)), "x must not contain NA!")# check NA, NaN, Inf, -Infexpect_error(testFunc(c(rnorm(3), NA_real_)), "x must not contain NA, NaN or Inf!")expect_error(testFunc(c(rnorm(3), NaN)), "x must not contain NA, NaN or Inf!")expect_error(testFunc(c(rnorm(3), Inf)), "x must not contain NA, NaN or Inf!")library(testthat)# values for RTYPE# LGLSXP 10 # logical variables# INTSXP 13 # integer variables# REALSXP 14 # real variables# CPLXSXP 15 # complex variables# STRSXP 16 # string variables# passcheckValue(c(1, 2), "x", 14, 2)checkValue(c(1L, 2L), "x", 13, 2)checkValue(c(TRUE, FALSE), "x", 10, 2)checkValue(c("1", "2"), "x", 16, 2)# check lengthexpect_error(checkValue(c(1, 2), "x", 14, 1), "The length of x must be 1!")expect_error(checkValue(c(1, 2), "x", 14, 3), "The length of x must be 3!")# check typeexpect_error(checkValue(c(1, 2), "x", 13, 2), "x must be integer type!")# check NA, NaN, Inf, -Infexpect_error(checkValue(c(1, NA_real_), "x", 14, 2), "x must not contain NA, NaN or Inf!")expect_error(checkValue(c(1, NaN), "x", 14, 2), "x must not contain NA, NaN or Inf!")expect_error(checkValue(c(1, Inf), "x", 14, 2), "x must not contain NA, NaN or Inf!")expect_error(checkValue(c(1, -Inf), "x", 14, 2), "x must not contain NA, NaN or Inf!") # check NA (NaN =&gt; NA, Inf =&gt; NA for integer type, so it is not necessary to check int with NaN or Inf.)expect_error(checkValue(c(TRUE, NA), "x", 10, 2), "x must not contain NA.")expect_error(checkValue(c(1L, NA_integer_), "x", 13, 2), "x must not contain NA.")expect_error(checkValue(c("1", NA_character_), "x", 16, 2), "x must not contain NA.")# check complexexpect_error(checkValue(complex(2, 1:2, 0:1), "x", 14, 2), "x must be double type!")expect_error(checkValue(complex(2, c(1, NA), 0:1), "x", 15, 2), "x must not contain NA, NaN or Inf!")expect_error(checkValue(complex(2, c(1, NaN), 0:1), "x", 15, 2), "x must not contain NA, NaN or Inf!")expect_error(checkValue(complex(2, c(1, Inf), 0:1), "x", 15, 2), "x must not contain NA, NaN or Inf!") 包成一個R函數for R check R object用： 123456789101112131415161718192021222324252627282930313233343536373839library(testthat)checkValueR &lt;- function(x, type, length = -1L, verbose = FALSE)&#123; stopifnot(type %in% c("logical", "integer", "double", "complex", "character")) funcCall &lt;- match.call() RTYPE &lt;- switch(type, "logical" = 10L, "integer" = 13L, "double" = 14L, "complex" = 15L, "character" = 16L) varName &lt;- as.character(funcCall$x) if (length(varName) &gt; 1L) &#123; if (verbose) message("x input is not a variable name instead function call, so use x as variable name.") varName &lt;- "x" &#125; checkValue(x, varName, RTYPE, length) invisible(NULL)&#125;# check lengthexpect_error(checkValueR(c(1, 2), "double", 1), "The length of x must be 1!")expect_error(checkValueR(c(1, 2), "double", 3), "The length of x must be 3!")# check typeexpect_error(checkValueR(c(1, 2), "integer", 2), "x must be integer type!")# check NA, NaN, Inf, -Infexpect_error(checkValueR(c(1, NA), "double", 2), "x must not contain NA, NaN or Inf!")expect_error(checkValueR(c(1, NaN), "double", 2), "x must not contain NA, NaN or Inf!")expect_error(checkValueR(c(1, Inf), "double", 2), "x must not contain NA, NaN or Inf!")expect_error(checkValueR(c(1, -Inf), "double", 2), "x must not contain NA, NaN or Inf!")# check NAexpect_error(checkValueR(c(TRUE, NA), "logical", 2), "x must not contain NA.")expect_error(checkValueR(c(1L, NA_integer_), "integer", 2), "x must not contain NA.")expect_error(checkValueR(c("1", NA_character_), "character", 2), "x must not contain NA.")# check complexexpect_error(checkValueR(complex(2, 1:2, 0:1), "double", 2), "x must be double type!")expect_error(checkValueR(complex(2, c(1, NA), 0:1), "complex", 2), "x must not contain NA, NaN or Inf!")expect_error(checkValueR(complex(2, c(1, NaN), 0:1), "complex", 2), "x must not contain NA, NaN or Inf!")expect_error(checkValueR(complex(2, c(1, Inf), 0:1), "complex", 2), "x must not contain NA, NaN or Inf!")]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>Type checking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using caret to tune the model parameters]]></title>
    <url>%2Fposts%2F201703%2F2017-03-14-using-caret-to-tune-model-parameters.html</url>
    <content type="text"><![CDATA[看到陳景翔老師在社團PO了caret這個套件(原文)，興起就去玩了一下 這套件東西滿多的，一時半會也沒辦法全部看懂 不過就以repeated cv方式tuning的方式，大概不會跳脫下面示範的範疇 12345678910111213141516171819202122232425262728293031323334353637383940414243444546library(caret)checkInstall("xgboost") # check xgboost is installed!numCV &lt;- 3LcvFold &lt;- 10LratioTrain &lt;- 0.6# 30 tuning model and 1 final modelnumSeeds &lt;- 4L * numCV# 4 is (tuneLength + 1L), where tuneLength is the parameter of train with default value 3set.seed(1)seeds &lt;- replicate(numCV * cvFold + 1L, sample.int(2^31-1, numSeeds), simplify = FALSE)# split training / testing settrIndex &lt;- createDataPartition(iris$Species, 1L, ratioTrain, FALSE)irisTrain &lt;- iris[trIndex, ]irisTest &lt;- iris[setdiff(1L:nrow(iris), trIndex), ]# 10-fold and repeat 3 times with seedsctrl &lt;- trainControl("repeatedcv", cvFold, numCV, seeds = seeds)#### auto-tuning ####xgFit1 &lt;- train(Species ~ ., data = irisTrain, trControl = ctrl, method = "xgbTree", num_class = nlevels(iris$Species))xgFit1# training result (auto-tuning)confusionMatrix(iris.Train$Species, predict(xgFit1, iris.Train))# testing result (auto-tuning)confusionMatrix(iris.Test$Species, predict(xgFit1, iris.Test))#### specific grid ##### check the model parametersmodelLookup("xgbTree")# expand training gridtrGrid &lt;- expand.grid(nrounds = c(50, 100), max_depth = c(10, 12), eta = c(0.1, 0.2), gamma = 0, colsample_bytree = c(0.5, 0.6), min_child_weight = 1)# start to train modelxgFit2 &lt;- train(Species ~ ., data = iris.Train, trControl = ctrl, method = "xgbTree", tuneGrid = trGrid, num_class = nlevels(iris$Species))xgFit2# training result (auto-tuning)confusionMatrix(iris.Train$Species, predict(xgFit2, iris.Train))# testing result (auto-tuning)confusionMatrix(iris.Test$Species, predict(xgFit2, iris.Test))]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>caret</tag>
        <tag>parameters tuning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp一些type轉換心得]]></title>
    <url>%2Fposts%2F201703%2F2017-03-12-Rcpp-type-template.html</url>
    <content type="text"><![CDATA[最近把RcppBlaze推上CRAN了 寫一下在做RcppBlaze::blaze_wrap學到的東西XD 主要是Rcpp::traits::r_sexptype_traits跟Rcpp::traits::storage_type的應用 先show一個Rcpp::traits::r_sexptype_traits簡單的例子 做兩個向量的相乘，在過程中轉成armadillo的column vector去做 因為Rcpp Export不能是一個template function 所以雖然你可以創一個像是eleMultiBase這樣的函數 可是沒辦法export它，所以只能另外在寫兩個函數去包裹… 然後在R裡面才判斷型別做輸出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;RcppArmadillo.h&gt;// [[Rcpp::depends(RcppArmadillo)]]template &lt;int RTYPE, typename T&gt;void copy(Rcpp::Vector&lt;RTYPE&gt;&amp; x, arma::Col&lt;T&gt;&amp; y) &#123; y.set_size(x.size()); for (int i = 0; i &lt; x.size(); ++i) y[i] = x[i];&#125;template &lt;int RTYPE, typename T&gt;SEXP eleMultiBase(Rcpp::Vector&lt;RTYPE&gt;&amp; x, Rcpp::Vector&lt;RTYPE&gt;&amp; y) &#123; arma::Col&lt;T&gt; x2, y2; copy&lt;RTYPE, T&gt;(x, x2); copy&lt;RTYPE, T&gt;(y, y2); /* or use following lines arma::Col&lt;T&gt; x2 = Rcpp::as&lt; arma::Col&lt;T&gt; &gt;(x), y2 = Rcpp::as&lt; arma::Col&lt;T&gt; &gt;(y); */ return Rcpp::wrap(x2 % y2);&#125;// [[Rcpp::export]]SEXP eleMulti_double(Rcpp::NumericVector x, Rcpp::NumericVector y) &#123; typedef typename Rcpp::NumericVector::stored_type valueType; const int RTYPE = Rcpp::traits::r_sexptype_traits&lt; valueType &gt;::rtype; return eleMultiBase&lt;RTYPE, valueType&gt;(x, y);&#125;// [[Rcpp::export]]SEXP eleMulti_int(Rcpp::IntegerVector x, Rcpp::IntegerVector y) &#123; typedef typename Rcpp::IntegerVector::stored_type valueType; const int RTYPE = Rcpp::traits::r_sexptype_traits&lt; valueType &gt;::rtype; return eleMultiBase&lt;RTYPE, valueType&gt;(x, y);&#125;/*** R eleMulti &lt;- function(x, y)&#123; if (typeof(x) == "integer") &#123; return(eleMulti_int(1L:5L, 2L:6L)) &#125; else if (typeof(x) == "double")&#123; return(eleMulti_double(1L:5L, 2L:6L)) &#125; else &#123; stop("Non-supported type!") &#125; &#125; eleMulti(1:5, 2:6) eleMulti(1L:5L, 2L:6L)*/ 而Rcpp::traits::storage_type其實在寫套件的時候比較方便 我這想不到一個實際例子用在R上面的 不過在處理RComplex的時候就很方便 如果y是Rcpp::ComplexVector，要轉到std::vector&lt;std::complex&gt;這個型別的話，做法如下： 123456789101112131415161718#include &lt;Rcpp.h&gt;#include &lt;vector&gt;#include &lt;complex&gt;// [[Rcpp::export]]std::vector&lt; std::complex&lt;double&gt; &gt; complexVec(Rcpp::ComplexVector y) &#123; std::vector&lt; std::complex&lt;double&gt; &gt; x(y.size()); const int RTYPE = Rcpp::traits::r_sexptype_traits&lt; std::complex&lt;double&gt; &gt;::rtype; for( size_t i=0UL; i&lt;(size_t)y.size(); ++i ) x[i] = Rcpp::internal::caster&lt; typename Rcpp::traits::storage_type&lt;RTYPE&gt;::type, std::complex&lt;double&gt; &gt;( y[i] ); return x;&#125;/*** R len &lt;- 5L y &lt;- complex(len, rnorm(len), rnorm(len)) complexVec(y)*/ 因為RComplex不是型別，所以只能透過caster跟storage_type去做適當轉換 想當初這個卡了我好久QQ，最後是爬RcppEigen的解法，才寫出對應的解決方法… 這篇分享到這，有任何疑問歡迎留言XD]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppBlaze</tag>
        <tag>Rcpp Types</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Windows環境下安裝GPU版本的mxnet]]></title>
    <url>%2Fposts%2F201703%2F2017-03-11-installation-mxnet-gpu-version-in-windows.html</url>
    <content type="text"><![CDATA[需要的components有： CUDA Toolkit 8.0 下載安裝 mxnet gpu prebuild 下載20170310_mxnet_x64_vc14_gpu.7z跟vc14 base package mxnet release zip 為了說明，我就在D槽開一個資料夾，叫做mxnet 先把整個mxnet repository clone到D:\mxnet\mxnet，解壓縮mxnet release的zip，其路徑應該是D:\mxnet\mxnet-版本 然後把20170310_mxnet_x64_vc14_gpu.7z跟prebuildbase_win10_x64_vc14.7z解壓縮到D:\mxnet\prebuild裡面 再來把D:\mxnet\prebuild\3rdparty裡面11個dll複製到D:\mxnet\mxnet-版本\R-package\inst\libs\x64(沒有資料夾就自己create) 另外要再建一個D:\mxnet\mxnet-版本\R-package\inst\include`資料夾 把D:\mxnet\nnvm\nnvm\include\nnvm, D:\mxnet\include\mxnet, D:\mxnet\dmlc-core\include\dlmc以及D:\mxnet\include\mshadow複製到裡面 最後，新增一個bat檔案 1234567echo import(Rcpp) &gt; R-package/NAMESPACEecho import(methods) &gt;&gt; R-package/NAMESPACER CMD INSTALL R-packageRscript -e "require(mxnet); mxnet:::mxnet.export(\"R-package\")"rm -rf R-package/NAMESPACERscript -e "require(roxygen2); roxygen2::roxygenise(\"R-package\")"R CMD INSTALL R-package 然後執行這個bat檔案就安裝完了 (roxygen2 0.6.1會有問題，請用舊版的roxygen2) 接下來就可以跑看看gpu可不可以用了(修改至mxnet/example/image-classfication裡面的R檔)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101library(mxnet)download_ &lt;- function(data_dir) &#123; dir.create(data_dir, showWarnings = FALSE) setwd(data_dir) if ((!file.exists('train-images-idx3-ubyte')) || (!file.exists('train-labels-idx1-ubyte')) || (!file.exists('t10k-images-idx3-ubyte')) || (!file.exists('t10k-labels-idx1-ubyte'))) &#123; download.file(url='http://data.mxnet.io/mxnet/data/mnist.zip', destfile='mnist.zip') unzip("mnist.zip") file.remove("mnist.zip") &#125; setwd("..")&#125;get_iterator &lt;- function(data_shape) &#123; get_iterator_impl &lt;- function() &#123; data_dir = 'mnist/' flat &lt;- TRUE if (length(data_shape) == 3) flat &lt;- FALSE train = mx.io.MNISTIter( image = paste0(data_dir, "train-images-idx3-ubyte"), label = paste0(data_dir, "train-labels-idx1-ubyte"), input_shape = data_shape, batch_size = 128, shuffle = TRUE, flat = flat) val = mx.io.MNISTIter( image = paste0(data_dir, "t10k-images-idx3-ubyte"), label = paste0(data_dir, "t10k-labels-idx1-ubyte"), input_shape = data_shape, batch_size = 128, flat = flat) ret = list(train=train, value=val) &#125; get_iterator_impl&#125;# multi-layer perceptronget_mlp &lt;- function() &#123; data &lt;- mx.symbol.Variable('data') fc1 &lt;- mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128) act1 &lt;- mx.symbol.Activation(data = fc1, name='relu1', act_type="relu") fc2 &lt;- mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64) act2 &lt;- mx.symbol.Activation(data = fc2, name='relu2', act_type="relu") fc3 &lt;- mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10) mlp &lt;- mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax') mlp&#125;get_lenet &lt;- function() &#123; data &lt;- mx.symbol.Variable('data') # first conv conv1 &lt;- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20) tanh1 &lt;- mx.symbol.Activation(data=conv1, act_type="tanh") pool1 &lt;- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2,2), stride=c(2,2)) # second conv conv2 &lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50) tanh2 &lt;- mx.symbol.Activation(data=conv2, act_type="tanh") pool2 &lt;- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2,2), stride=c(2,2)) # first fullc flatten &lt;- mx.symbol.Flatten(data=pool2) fc1 &lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=500) tanh3 &lt;- mx.symbol.Activation(data=fc1, act_type="tanh") # second fullc fc2 &lt;- mx.symbol.FullyConnected(data=tanh3, num_hidden=10) # loss lenet &lt;- mx.symbol.SoftmaxOutput(data=fc2, name='softmax') lenet&#125;data_loader &lt;- get_iterator(c(28, 28, 1))download_('mnist/')net &lt;- get_lenet()data &lt;- data_loader()train &lt;- data$trainval &lt;- data$value devs &lt;- mx.gpu(0)model &lt;- mx.model.FeedForward.create( X = train, eval.data = val, ctx = devs, symbol = net, begin.round = 0, eval.metric = mx.metric.top_k_accuracy, num.round = 10, learning.rate = 0.05, array.batch.size = 128, optimizer = "sgd", initializer = mx.init.Xavier(factor_type="in", magnitude=2), batch.end.callback = mx.callback.log.train.metric(50))]]></content>
      <categories>
        <category>MxNet</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>MxNet</tag>
        <tag>Windows</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wrapr範例]]></title>
    <url>%2Fposts%2F201703%2F2017-03-10-wrapr-examples.html</url>
    <content type="text"><![CDATA[原文是我在PTT回的一篇文章連結 其實是有人來問怎麼將character verctor放入函數中執行對應的動作 好讓函數變得簡單使用跟操作，那我提供了下面幾個解法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859library(dplyr)library(pipeR)library(ggplot2)library(data.table)data("diamonds", package = "ggplot2")# 一般寫法 (dplyr)df_group_fn &lt;- function(df, meanCol, col_1, col_2)&#123; df %&gt;&gt;% group_by_(.dots = c(col_1, col_2)) %&gt;&gt;% summarise_(.dots = c(n = "n()", mean = paste0("mean(", meanCol, ")"))) %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;df_group_fn(diamonds, "price", "cut", "color")# 一般寫法 (data.table)dt_group_fn &lt;- function(dt, meanCol, col_1, col_2)&#123; dt[ , .(n = .N, mean = eval(parse(text = paste0("mean(", meanCol, ")")))), by = c(col_1, col_2)] %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;dt_group_fn(data.table(diamonds), "price", "cut", "color")# wrapr + dplyrlibrary(wrapr)df_group_fn2 &lt;- function(df, meanCol, col_1, col_2)&#123; let(list(y = meanCol, c1 = col_1, c2 = col_2), &#123; df %&gt;&gt;% group_by(c1, c2) %&gt;&gt;% summarise(n = n(), mean = mean(y)) &#125;) %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;df_group_fn2(diamonds, "price", "cut", "color")# wrapr + data.tabledt_group_fn2 &lt;- function(dt, meanCol, col_1, col_2)&#123; let(list(y = meanCol, c1 = col_1, c2 = col_2), &#123; dt[ , .(n = .N, mean = mean(y)), by = .(c1, c2)] &#125;) %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;dt_group_fn2(data.table(diamonds), "price", "cut", "color")# 進階，不把欄位給死的方法：# dplyrdf_group_fn3 &lt;- function(df, meanCol, groupByCols)&#123; let(list(y = meanCol), &#123; df %&gt;&gt;% group_by_(.dots = groupByCols) %&gt;&gt;% summarise(n = n(), mean = mean(y)) &#125;) %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;df_group_fn3(diamonds, "price", c("cut", "color"))# data.tabledt_group_fn3 &lt;- function(dt, meanCol, groupByCols)&#123; let(list(y = meanCol), &#123; dt[ , .(n = .N, mean = mean(y)), by = groupByCols] &#125;) %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;dt_group_fn3(data.table(diamonds), "price", c("cut", "color")) 但是最漂亮的寫法應該是下面這樣： 12345678# data.table + ... + substitutedt_group_fn3 &lt;- function(dt, meanCol, ...)&#123; groupByCols &lt;- as.character(as.list(substitute(list(...)))[-1L]) y &lt;- substitute(meanCol) dt[ , .(n = .N, mean = mean(y)), by = groupByCols] %&gt;&gt;% &#123;ggplot(., aes(mean,n)) + geom_point()&#125;&#125;dt_group_fn3(data.table(diamonds), price, cut, color)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>wrapr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Ambari server與HDP repo快速部署Hadoop Cluster]]></title>
    <url>%2Fposts%2F201702%2F2017-02-15-use-ambari-and-hdp-deploy-hadoop-cluster.html</url>
    <content type="text"><![CDATA[本篇會從頭開始介紹怎麼使用Ambari server跟HDP repository來快速部署Hadoop Cluster 安裝CentOS以及基本部署 我使用VMware建立三台CentOS 7的電腦 三台的hostname分別為ambaritest01, ambaritest02 and ambaritest03 此處建議都用root帳號安裝，不然ambari有一個地方要額外設定 a. 網路 ambaritest01的網路設定(/etc/sysconfig/ifcfg-XXXXXX, XXXXXX是網路卡的名稱)： 1234567891011121314151617181920TYPE=EthernetBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=faa2688c-6d77-4e38-8c71-d65c67823dd5DEVICE=ens33ONBOOT=yesDNS1=192.168.0.1DNS2=8.8.8.8IPADDR=192.168.0.121PREFIX=24GATEWAY=192.168.0.1 ambaritest02跟ambaritest03的網路設定只需要改IPADDR即可 b. 對時 下面直接使用網路對時，如果是local LAN請在一台建立可以對時的server (建立方法請google) 然後修改/etc/ntp.conf讓全部機器都去自動與那台對時 1234yum install -y ntp ntpdate ntp-docntpdate pool.ntp.orgsystemctl start ntpdsystemctl enable ntpd 對時很重要，有時候Hadoop的問題就來自時間的不一致 c. 關閉SELinux 123sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinuxsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsetenforce 0 d. 關閉防火牆 12systemctl stop firewalldsystemctl disable firewalld e. enable SSH連線 每一台都要先跑下面命令： 12345678910111213ssh-keygen -t rsa -P ""cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 700 ~/chmod 700 ~/.sshchmod 644 ~/.ssh/authorized_keyschmod 600 ~/.ssh/id_rsasystemctl restart sshd tee -a /etc/hosts &lt;&lt; "EOF"192.168.0.121 ambaritest01192.168.0.122 ambaritest02192.168.0.123 ambaritest03EOF 在你要安裝ambari server那台執行，假設安裝在ambaritest01： 123ssh-copy-id -i ~/.ssh/id_rsa.pub ambaritest01ssh-copy-id -i ~/.ssh/id_rsa.pub ambaritest02ssh-copy-id -i ~/.ssh/id_rsa.pub ambaritest03 然後測試一下，確定都可以用ssh直接相連 f. 安裝Oracle Java 個人不愛用openjdk，會遇到一些奇怪的bug，所以我都安裝Oracle JAVA 這裡JAVA_HOME很重要，後面會用到 123456789101112131415curl -v -j -k -L -H &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u112-b15/jdk-8u112-linux-x64.rpm -o jdk-8u112-linux-x64.rpmyum install -y jdk-8u112-linux-x64.rpmscp jdk-8u112-linux-x64.rpm ambaritest02:~/ssh ambaritest02 yum install -y jdk-8u112-linux-x64.rpmscp jdk-8u112-linux-x64.rpm ambaritest03:~/ssh ambaritest03 yum install -y jdk-8u112-linux-x64.rpmtee -a /etc/bashrc &lt;&lt; &quot;EOF&quot;export JAVA_HOME=/usr/java/jdk1.8.0_112export PATH=$PATH:$JAVA_HOME/binEOFscp /etc/bashrc ambaritest02:/etcscp /etc/bashrc ambaritest03:/etcsource /etc/bashrc h. 安裝gcc-5.3, R(非必要) 因為我自己會用到R，之後也想要在Apache Zeppline上使用，所以順便一些紀錄 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# install R from EPEL (default BLAS is openblas)yum install wgetwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum install -y epel-release-latest-7.noarch.rpmyum install -y R R-devel R-java libxml2-devel libxml2-static tcl tcl-devel tk tk-devel libtiff-static libtiff-devel libjpeg-turbo-devel libpng12-devel cairo-tools libicu-devel openssl-devel libcurl-devel freeglut readline-static readline-devel cyrus-sasl-devel# install microsoft R open (default BLAS is MKL)wget https://mran.microsoft.com/install/mro/3.3.2/microsoft-r-open-3.3.2.tar.gztar zxvf microsoft-r-open-3.3.2.tar.gzyum install -y microsoft-r-open/rpm/microsoft-r-open-*# remove R from EPEL, and use microsoft R openrm -rf /usr/lib64/Rcp -r /usr/lib64/microsoft-r/3.3/lib64/R /usr/lib64# let user access library dir (not to use personal library)chmod -R 777 /usr/lib64/R/library# change default repostee -a /usr/lib64/R/etc/Rprofile.site &lt;&lt; EOFoptions(repos = &quot;https://cloud.r-project.org/&quot;)EOF# remove openjdkyum remove -y openjdk-*# config R-JavaR CMD javareconfRscript -e &quot;install.packages(&apos;rJava&apos;)&quot;# enable C++11 for microsoft R opensudo sed -i -e &apos;s/CXX1X =/CXX1X = g++/g&apos; /usr/lib64/R/etc/Makeconfsudo sed -i -e &apos;s/CXX1XFLAGS =/CXX1XFLAGS = -DU_STATIC_IMPLEMENTATIN -O2 -g/g&apos; /usr/lib64/R/etc/Makeconfsudo sed -i -e &apos;s/CXX1XPICFLAGS =/CXX1XPICFLAGS = -fpic/g&apos; /usr/lib64/R/etc/Makeconfsudo sed -i -e &apos;s/CXX1XSTD =/CXX1XSTD = -std=c++11/g&apos; /usr/lib64/R/etc/Makeconf# install rstudio serverwget https://download2.rstudio.org/rstudio-server-rhel-1.0.136-x86_64.rpmsudo yum install -y --nogpgcheck rstudio-server-rhel-1.0.136-x86_64.rpm# let user not to use personal librarysudo tee -a /etc/rstudio/rsession.conf &lt;&lt; EOFr-libs-user=/usr/lib64/R/libraryEOF# start rstudio-serversudo systemctl enable rstudio-serversudo systemctl start rstudio-server# user for rstudio serveruseradd rstudiopasswd rstudio install ambari-server 每台都要新增repo資料: 12wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.4.2.0/ambari.repo -O /etc/yum.repos.d/ambari.repowget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.5.3.0/hdp.repo -O /etc/yum.repos.d/hdp.repo 在ambaritest01安裝ambari-server: 1yum install ambari-server start ambari-server 先跑ambari-server setup，基本上選不用Customize 然後選Custom JDK，JAVA_HOME用上面設定的/usr/java/jdk1.8.0_112 再使用ambari-server start啟動即可 也要記得使用systemctl enable ambari-server讓電腦自動開啟ambari-server Install, configure and deploy an HDP cluster 在我電腦用瀏覽器登入http://192.168.0.121:8080就可以看到登入畫面，預設帳密為admin/admin 登入之後就可以看到下面的畫面： 接下來就是按下Launch Install Wizard開始安裝就好 步驟基本上照著官網手冊手就好 我只說明第五步，Target Hosts寫ambaritest[01-03]，Host Registration Information部分則用cat ~/.ssh/ id_rsa印出的資訊 接著第六步就會開始安裝ambari-agent，然後安裝完還會跑一個check，安裝以及check成功的畫面如下： 再下一頁就是選擇服務安裝了，其中Log Search跟SmartSense是Hortonworks的軟體，是要授權碼的，其他都是apache license license相關資訊請查詢這裡 成功之後就可以看到cluster畫面： (我有些service沒裝成功，可能還要看一下原因)]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
        <tag>Ambari</tag>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using gcc 5.3 with R in CentOS 7]]></title>
    <url>%2Fposts%2F201701%2F2017-01-04-using-gcc-5.3-with-R-in-centos-7.html</url>
    <content type="text"><![CDATA[只是記錄一下，方便日後check 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# install devtoolset-4 to get gcc-5.3yum install centos-release-sclyum-config-manager --enable rhel-server-rhscl-7-rpmsyum install -y devtoolset-4 --exclude=devtoolset-4-4.1-3.el7,devtoolset-4-ide-4.1-3.el7yum update -y# enable usage of gcc-5.3tee -a /etc/bashrc &lt;&lt; EOFsource /opt/rh/devtoolset-4/enableEOFsource /etc/bashrc# install R from EPEL (default BLAS is openblas)wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum install -y epel-release-latest-7.noarch.rpmyum install -y R R-devel R-java libxml2-devel libxml2-static tcl tcl-devel tk tk-devel libtiff-static libtiff-devel libjpeg-turbo-devel libpng12-devel cairo-tools libicu-devel openssl-devel libcurl-devel freeglut readline-static readline-devel cyrus-sasl-devel# install microsoft R open (default BLAS is MKL)wget https://mran.microsoft.com/install/mro/3.3.2/microsoft-r-open-3.3.2.tar.gztar zxvf microsoft-r-open-3.3.2.tar.gzyum install -y microsoft-r-open/rpm/microsoft-r-open-*# install Oracle Javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u112-b15/jdk-8u112-linux-x64.rpm -o jdk-8u112-linux-x64.rpmyum install -y jdk-8u112-linux-x64.rpmtee -a /etc/bashrc &lt;&lt; "EOF"export JAVA_HOME=/usr/java/jdk1.8.0_112export PATH=$PATH:$JAVA_HOME/binEOFsource /etc/bashrc# remove R from EPEL, and use microsoft R openrm -rf /usr/lib64/Rcp -r /usr/lib64/microsoft-r/3.3/lib64/R /usr/lib64# let user access library dir (not to use personal library)chmod -R 777 /usr/lib64/R/library# config R java envR CMD javareconf# check Java and g++ work well# (fail installation indicate configuration of java/g++ is wrong)Rscript -e "install.packages(c('rJava', 'Rcpp'))"Rscript -e "library(rJava); library(Rcpp)"# make C++11 enablesed -i -e 's/CXX1X =/CXX1X = g++/g' /usr/lib64/R/etc/Makeconfsed -i -e 's/CXX1XFLAGS =/CXX1XFLAGS = -DU_STATIC_IMPLEMENTATIN -O2 -g/g' /usr/lib64/R/etc/Makeconfsed -i -e 's/CXX1XPICFLAGS =/CXX1XPICFLAGS = -fpic/g' /usr/lib64/R/etc/Makeconfsed -i -e 's/CXX1XSTD =/CXX1XSTD = -std=c++11/g' /usr/lib64/R/etc/Makeconf# check g++ with CXX1X work well# (fail installation indicate configuration of g++ with CXX1X is wrong)Rscript -e "install.packages('RcppMLPACK')"## install rstudio-serverwget https://download2.rstudio.org/rstudio-server-rhel-1.0.136-x86_64.rpmyum install -y --nogpgcheck rstudio-server-rhel-1.0.136-x86_64.rpm# let user not to use personal librarytee -a /etc/rstudio/rsession.conf &lt;&lt; EOFr-libs-user=/usr/lib64/R/libraryEOF# let rstudio-server uses gcc-5.3tee -a /usr/lib/rstudio-server/R/ServerOptions.R &lt;&lt; EOFSys.setenv(PATH = "/opt/rh/devtoolset-4/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin",LD_LIBRARY_PATH = "/opt/rh/devtoolset-4/root/usr/lib64:/opt/rh/devtoolset-4/root/usr/lib:/usr/lib64/R/lib::/lib:/builddir/vendor/build/lib:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jre/lib/amd64/server")EOF# start rstudio-serversystemctl enable rstudio-serversystemctl start rstudio-server# open firewallfirewall-cmd --zone=public --add-port=8787/tcp --permanentfirewall-cmd --reload 後來看到文章說，直接編譯安裝是比較快的方法： 1234567yum install libmpc-devel mpfr-devel gmp-devel zlib-devel gcc gcc-c++ gcc-gfortran glibc-develwget ftp://ftp.mirrorservice.org/sites/sourceware.org/pub/gcc/releases/gcc-5.4.0/gcc-5.4.0.tar.gztar zxvf gcc-5.4.0.tar.gzcd gcc-5.4.0./configure --prefix=/usr --with-system-zlib --disable-multilib --enable-languages=c,c++,fortranmake -j 8make install]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>CentOS</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Link Blaze (part 2)]]></title>
    <url>%2Fposts%2F201701%2F2017-01-02-Rcpp-link-blaze-part-2.html</url>
    <content type="text"><![CDATA[如果include blaze/Blaze.h會發現出了好幾個錯誤，這裡提供一些解法： 改blaze/util/Time.h 原本的43行~55行改成下面這樣： 1234567891011121314151617181920#if defined(_MSC_VER)# ifndef NOMINMAX# define NOMINMAX# endif# include &lt;windows.h&gt;# include &lt;winsock.h&gt;# include &lt;time.h&gt;# include &lt;sys/timeb.h&gt;#else#if defined(__MINGW32__)# include &lt;windows.h&gt;# include &lt;winsock.h&gt;# include &lt;time.h&gt;# include &lt;sys/timeb.h&gt;#else# include &lt;sys/resource.h&gt;# include &lt;sys/time.h&gt;# include &lt;sys/types.h&gt;#endif#endif 在pre-processor中efine 三個變數 123#define STRICT_R_HEADERS#define BOOST_ERROR_CODE_HEADER_ONLY#define RCPP_HAS_LONG_LONG_TYPES 第一個只是避免掉C++11的編譯錯誤，第二個則是避免去linking boost::system RCPP_HAS_LONG_LONG_TYPES是為了讓Blaze可以輸出unsigned long long int的維度 不要使用boost::thread 可以在pre-processor中加入下面幾行，提醒自己： 123#if defined(BLAZE_USE_BOOST_THREADS)#error "Boost threads could not be used!"#endif 設定完之後，我們測試看看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546Sys.setenv("PKG_CXXFLAGS" = sprintf('-I"%s"', normalizePath(".", winslash = "/")))Rcpp::sourceCpp(code = '#define STRICT_R_HEADERS#define BOOST_ERROR_CODE_HEADER_ONLY// [[Rcpp::depends(BH)]]// [[Rcpp::plugins(cpp11)]]#include &lt;Rcpp.h&gt;#include &lt;blaze/Blaze.h&gt;using blaze::unaligned; using blaze::unpadded;using blaze::columnMajor;using blaze::CustomMatrix;using blaze::CustomVector;using blaze::DynamicVector;//[[Rcpp::export]]Rcpp::NumericVector blaze_mv(Rcpp::NumericMatrix X, Rcpp::NumericVector Y) &#123; // input cheching if (X.ncol() != Y.size()) Rcpp::stop("The size of Y must be equal to the number of columns of X."); // map and perform matrix-vector multiplication CustomMatrix&lt;double, unaligned, unpadded, columnMajor&gt; a(&amp;X[0], X.nrow(), X.ncol()); CustomVector&lt;double, unaligned, unpadded&gt; b(&amp;Y[0], Y.size()); DynamicVector&lt;double&gt; c = a * b; // output Rcpp::NumericVector out(c.size()); for (auto i = 0; i &lt; c.size(); ++i) out[i] = c[i]; return out;&#125;')library(microbenchmark)N &lt;- 5000LX &lt;- matrix(rnorm(N**2), N, N)y &lt;- rnorm(N)microbenchmark( blaze_mv = blaze_mv(X, y), R_mv = as.vector(X %*% y), times = 50L)# Unit: milliseconds# expr min lq mean median uq max neval# blaze_mv 10.09042 10.32564 11.70235 10.93375 12.88094 15.67995 50# R_mv 32.24726 33.52491 35.97026 35.66461 37.98600 42.05827 50 矩陣乘法快了三倍呢！]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>Blaze</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Update RcppEigen to 3.3.1]]></title>
    <url>%2Fposts%2F201701%2F2017-01-02-update-RcppEigen-to-3-3-1.html</url>
    <content type="text"><![CDATA[我花了一點時間把RcppEigen升級到3.3.1，然後我發現一件滿有趣的事情(細節請看 Using BLAS/LAPACK from Eigen) 因為是我手動升級的，檔案放在我的github: 連結請點我 就是#define EIGEN_USE_BLAS可以用了，設定之後，RcppEigen會使用R的BLAS去做計算 如此一來，之前講到的RcppEigen的慢就可以被改善了 拿前一篇kernel matrix computation來測試看看 新增兩個cpp檔案，其他之前幾篇就有，不再附上 R code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263library(kernlab)Rcpp::sourceCpp("kernel_matrix_arma.cpp")Rcpp::sourceCpp("kernel_matrix_eigen1.cpp")Rcpp::sourceCpp("kernel_matrix_eigen2.cpp")Rcpp::sourceCpp("kernel_matrix_arma_para.cpp")Rcpp::sourceCpp("kernel_matrix_eigen_para1.cpp")Rcpp::sourceCpp("kernel_matrix_eigen_para2.cpp")Sys.setenv("PKG_LIBS" = "$(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)")Rcpp::sourceCpp("kernel_matrix_eigen3.cpp")Rcpp::sourceCpp("kernel_matrix_eigen_para3.cpp") N &lt;- 5000Lp &lt;- 1000Lb &lt;- 500LX &lt;- matrix(rnorm(N*p), ncol = p)center &lt;- X[sample(N, b),]sigma &lt;- 2.5kernelMatrix_R &lt;- function(X, center, sigma)&#123; exp(sweep(sweep(X %*% t(center), 1, rowSums(X**2)/2), 2, rowSums(center**2)/2) / (sigma**2))&#125;res_kernlab &lt;- kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)@.Data# check results with kernlab::kernelMatrixall.equal(kernelMatrix_R(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_arma_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_omp_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_cpp2(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_arma_para_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_para_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_para_omp_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_para_cpp2(X, center, sigma), res_kernlab) # TRUElibrary(microbenchmark)microbenchmark( Rfun = kernelMatrix_R(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), RcppArmadillo = kernelMatrix_arma_cpp(X, center, sigma), RcppEigen = kernelMatrix_eigen_cpp(X, center, sigma), RcppEigen_Openmp = kernelMatrix_eigen_omp_cpp(X, center, sigma), RcppEigen2 = kernelMatrix_eigen_cpp2(X, center, sigma), RcppArmadillo_RcppParallel = kernelMatrix_arma_cpp(X, center, sigma), RcppEigen_RcppParallel = kernelMatrix_eigen_para_cpp(X, center, sigma), RcppEigen_RcppParallel_Openmp = kernelMatrix_eigen_para_omp_cpp(X, center, sigma), RcppEigen_RcppParallel2 = kernelMatrix_eigen_para_cpp2(X, center, sigma), times = 30L)# Unit: milliseconds# expr min lq mean median uq max neval# Rfun 215.8470 241.8069 272.6916 264.2367 302.9716 335.1085 30# kernlab 224.6712 245.8116 278.1123 264.4116 314.4725 351.3037 30# RcppArmadillo 162.1123 167.2791 185.8838 172.7111 199.5971 257.8169 30# RcppEigen 414.0345 416.6387 424.3981 418.9367 426.3183 489.0802 30# RcppEigen_Openmp 193.6153 195.3303 218.5779 200.1698 236.1647 313.2402 30# RcppEigen2 136.9555 140.3265 148.6040 144.2964 150.6931 203.9994 30# RcppArmadillo_RcppParallel 164.3119 171.0287 187.1966 182.7915 197.8019 251.6530 30# RcppEigen_RcppParallel 408.2332 410.5831 415.1298 412.5009 415.5977 447.3581 30# RcppEigen_RcppParallel_Openmp 188.2270 191.7382 241.4736 237.4829 271.3958 337.3198 30# RcppEigen_RcppParallel2 129.1949 135.4055 151.7909 148.0432 160.7522 193.2683 30 kernel_matrix_eigen3.cpp: 1234567891011121314151617#define EIGEN_USE_BLAS// [[Rcpp::depends(RcppEigen)]]#include &lt;RcppEigen.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_cpp2(MatrixXd x, MatrixXd center, double sigma) &#123; MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; kernelMat.rowwise() -= center_square_sum.transpose(); kernelMat.colwise() -= x_square_sum; kernelMat /= pow(sigma, 2.0); return kernelMat.array().exp().matrix();&#125; kernel_matrix_eigen_para3.cpp: 12345678910111213141516171819202122232425262728293031323334353637383940#define EIGEN_USE_BLAS// [[Rcpp::depends(RcppEigen, RcppParallel)]]#include &lt;RcppEigen.h&gt;#include &lt;RcppParallel.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;struct KernelComputeWorker: public RcppParallel::Worker &#123; const VectorXd&amp; x_square_sum; const VectorXd&amp; center_square_sum; const double&amp; sigma2; MatrixXd&amp; kernelMat; KernelComputeWorker(const VectorXd&amp; x_square_sum, const VectorXd&amp; center_square_sum, const double&amp; sigma2, MatrixXd&amp; kernelMat): x_square_sum(x_square_sum), center_square_sum(center_square_sum), sigma2(sigma2), kernelMat(kernelMat) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (int k = (int) begin; k &lt; (int) end; ++k) &#123; int j = k / kernelMat.rows(); int i = k - j * kernelMat.rows(); kernelMat(i, j) -= (x_square_sum(i) + center_square_sum(j)); kernelMat(i, j) = exp(kernelMat(i, j) / sigma2); &#125; &#125;&#125;;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_para_cpp2(MatrixXd x, MatrixXd center, double sigma) &#123; MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; double sigma2 = pow(sigma, 2.0); KernelComputeWorker worker_kc(x_square_sum, center_square_sum, sigma2, kernelMat); RcppParallel::parallelFor(0, kernelMat.size(), worker_kc); return kernelMat;&#125; 可以看到用了multi-threaded BLAS之後的Eigen更加強大 直接從原本的平均424.3981 ms表現衝到平均只要148.6040 ms 果然用對了武器就是不一樣啊(茶 至於#define EIGEN_USE_LAPACKE是無效的，畢竟R用的是Lapack.h不是lapacke.h(攤手 (LAPACK與LAPACKE的差異請看stackoverflow的解釋) 也許需要有人去更改Eigen的include file，讓Eigen能用RLAPACK的函數去處理~~]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>BLAS</tag>
        <tag>RcppEigen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Link Blaze]]></title>
    <url>%2Fposts%2F201701%2F2017-01-02-Rcpp-link-blaze.html</url>
    <content type="text"><![CDATA[補充：我把程式放到GitHub，可以到這下載來玩：Link Blaze claim it is fast. Link 所以我就來試試看Rcpp去link玩玩看 看看有沒有大神會把它弄成一個套件，叫做RcppBlaze之類 我試了一下blaze-3.0，怎樣都無法編譯Orz (後來有找到要用-std=c++1y才能編譯成功) 我就改用上一版的blaze-2.6，不過還是有些地方需要更動 要變動的地方有兩個，一個是blaze/util/Memory.h，另一個是blaze/math/adaptors/hermitianmatrix/HermitianValue.h Memory.h: 12345678// 在57行增加下面幾行#ifdef __MINGW32__ #define _aligned_malloc __mingw_aligned_malloc #define _aligned_free __mingw_aligned_free #endif// 84行跟112行原本是#if defined(_MSC_VER)改成：#if defined(_MSC_VER) || defined(__MINGW32__) HermitianValue.h: (blaze-3.0不用改) 12// 279, 299, 323, 347, 371, 395, 513行原本是 pos_-&gt;index() == index改成：pos_-&gt;index() == index_ 然後就可以快樂的開始玩blaze了 (Note: 其實還有一些要改才能完整使用Blaze，請參考下一篇文章) R code: 1234567891011121314151617181920212223242526272829303132333435363738useBlaze26 &lt;- FALSEif (useBlaze26 &amp;&amp; dir.exists("blaze26")) &#123; file.rename("blaze", "blaze3") file.rename("blaze26", "blaze") Sys.setenv("PKG_CXXFLAGS" = '-I.')&#125; else if (!useBlaze26 &amp;&amp; dir.exists("blaze3")) &#123; file.rename("blaze", "blaze26") file.rename("blaze3", "blaze") Sys.setenv("PKG_CXXFLAGS" = "-I. -DUSE_BLAZE3")&#125; else &#123; stop("Please use correct tag!")&#125;Rcpp::sourceCpp("test_blaze.cpp", rebuild = TRUE)test_blaze1(1:5)test_blaze1(rnorm(5))test_blaze2(matrix(1:9, 3, 3))test_blaze2(matrix(rnorm(9), 3, 3))X &lt;- matrix(rnorm(9), 3, 3)y &lt;- rnorm(3)all.equal(test_blaze3(X, y), as.vector(X %*% y)) # TRUElibrary(microbenchmark)M &lt;- 6e3LN &lt;- 3e4LX &lt;- matrix(rnorm(M * N), M, N)y &lt;- rnorm(N)microbenchmark( blaze = test_blaze3(X, y), R = as.vector(X %*% y), times = 50L)# Unit: microseconds# expr min lq mean median uq max neval# blaze 168.228 169.983 195.6315 173.640 202.750 312.172 30# R 554.126 559.099 632.9047 562.757 660.328 1506.730 30 test_blaze.cpp: 12345678910111213141516171819202122232425262728293031323334353637383940414243// [[Rcpp::depends(BH)]]#ifdef USE_BLAZE3// [[Rcpp::plugins(cpp14)]]#else// [[Rcpp::plugins(cpp11)]]#endif#include &lt;Rcpp.h&gt;#include &lt;blaze/Math.h&gt;//[[Rcpp::export]]Rcpp::NumericVector test_blaze1(Rcpp::NumericVector X)&#123; blaze::CustomVector&lt;double, blaze::unaligned, blaze::unpadded&gt; v( &amp;X[0], X.size() ); Rcpp::Rcout &lt;&lt; v[0] &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v[1] &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v[2] &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v[3] &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v[4] &lt;&lt; std::endl; return X;&#125;//[[Rcpp::export]]Rcpp::NumericMatrix test_blaze2(Rcpp::NumericMatrix X)&#123; blaze::CustomMatrix&lt;double, blaze::unaligned, blaze::unpadded, blaze::columnMajor&gt; v( &amp;X[0], X.nrow(), X.ncol() ); Rcpp::Rcout &lt;&lt; v(0, 0) &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v(0, 1) &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v(0, 2) &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v(1, 0) &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v(1, 1) &lt;&lt; std::endl; Rcpp::Rcout &lt;&lt; v(1, 2) &lt;&lt; std::endl; return X;&#125;//[[Rcpp::export]]Rcpp::NumericVector test_blaze3(Rcpp::NumericMatrix X, Rcpp::NumericVector y)&#123; blaze::CustomMatrix&lt;double, blaze::unaligned, blaze::unpadded, blaze::columnMajor&gt; a( &amp;X[0], X.nrow(), X.ncol() ); blaze::CustomVector&lt;double, blaze::unaligned, blaze::unpadded&gt; b( &amp;y[0], y.size() ); blaze::DynamicVector&lt;double&gt; c = a * b; Rcpp::NumericVector out(c.size()); for (auto i = 0; i &lt; c.size(); ++i) out[i] = c[i]; return out;&#125; 後記： blaze-3.0的話，其實只要改好blaze/util/Memory.h 然後在cpp file中使用// [[Rcpp::plugins(cpp14)]]即可 Note: 其實還有cpp1y for C++14 and C++17 standard under development Rcpp的plugins可以看GitHub Rcpp Issue Link 我這裡測試是blaze-3.0跟blaze-2.6效能滿接近的 個人是覺得因為R的Policy問題，所以還是用2.6就好 用太新的C++ std通常都會吃土]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>Blaze</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kernal matrix computation in Rcpp]]></title>
    <url>%2Fposts%2F201701%2F2017-01-01-kernal-matrix-computation-in-Rcpp.html</url>
    <content type="text"><![CDATA[會有這篇是剛好想到之前在PTT PO的文章 文章1，文章2 就想說順便來把這兩個拿來比較一下，於是就有了這一篇的誕生 這篇剛好也是我試到一個RcppEigen比較慢的例子 當做之前的RcppArmadillo vs RcppEigen的延伸 我這次也觀察到一個現象是RcppArmadillo在這個計算上會使用BLAS 所以它這裡會用我R的Multi-threaded MKL BLAS去算 而RcppEigen機制不太確定，但是有趣的事情是Eigen還可以透過include omp配上Eigen::initParallel來加速 但是這裡RcppArmadillo毫無疑問的直接打趴RcppEigen(攤手 下一篇會再把RcppParallel拉進來一起玩 R code: 1234567891011121314151617181920212223242526272829303132333435363738394041library(kernlab)Rcpp::sourceCpp("kernel_matrix_arma.cpp")Rcpp::sourceCpp("kernel_matrix_eigen1.cpp")Rcpp::sourceCpp("kernel_matrix_eigen2.cpp") N &lt;- 5000Lp &lt;- 1000Lb &lt;- 500LX &lt;- matrix(rnorm(N*p), ncol = p)center &lt;- X[sample(N, b),]sigma &lt;- 2.5kernelMatrix_R &lt;- function(X, center, sigma)&#123; exp(sweep(sweep(X %*% t(center), 1, rowSums(X**2)/2), 2, rowSums(center**2)/2) / (sigma**2))&#125;res_kernlab &lt;- kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)@.Data# check results with kernlab::kernelMatrixall.equal(kernelMatrix_R(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_arma_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_omp_cpp(X, center, sigma), res_kernlab) # TRUElibrary(microbenchmark)microbenchmark( Rfun = kernelMatrix_R(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), RcppArmadillo = kernelMatrix_arma_cpp(X, center, sigma), RcppEigen = kernelMatrix_eigen_cpp(X, center, sigma), RcppEigen_Openmp = kernelMatrix_eigen_omp_cpp(X, center, sigma), times = 30L)# Unit: milliseconds# expr min lq mean median uq max neval# Rfun 224.3423 245.0635 258.5608 256.1671 267.4266 324.6571 30# kernlab 229.4637 239.9462 265.5297 265.6573 282.5231 348.2940 30# RcppArmadillo 165.3531 174.5219 188.2199 184.6343 200.6425 226.9207 30# RcppEigen 416.0304 417.4070 424.4944 418.7338 426.4974 464.8256 30# RcppEigen_Openmp 202.4775 208.1299 248.6041 229.6451 290.2074 333.7440 30 kernel_matrix_arma.cpp: 12345678910111213141516// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using arma::mat;using arma::vec;using arma::rowvec;// [[Rcpp::export]]mat kernelMatrix_arma_cpp(mat x, mat center, double sigma) &#123; mat kernelMat(x * center.t()); vec x_square_sum = sum(square(x), 1) / 2.0; rowvec center_square_sum = (sum(square(center), 1)).t() / 2.0; kernelMat.each_row() -= center_square_sum; kernelMat.each_col() -= x_square_sum; kernelMat /= pow(sigma, 2.0); return exp(kernelMat);&#125; kernel_matrix_eigen1.cpp: 12345678910111213141516// [[Rcpp::depends(RcppEigen)]]#include &lt;RcppEigen.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_cpp(MatrixXd x, MatrixXd center, double sigma) &#123; MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; kernelMat.rowwise() -= center_square_sum.transpose(); kernelMat.colwise() -= x_square_sum; kernelMat /= pow(sigma, 2.0); return kernelMat.array().exp().matrix();&#125; kernel_matrix_eigen2.cpp: 12345678910111213141516171819202122// [[Rcpp::depends(RcppEigen)]]// [[Rcpp::plugins(openmp, cpp11)]]#include &lt;RcppEigen.h&gt;#include &lt;omp.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_omp_cpp(MatrixXd x, MatrixXd center, double sigma) &#123; int n = omp_get_max_threads(); omp_set_num_threads(n); Eigen::setNbThreads(n); Eigen::initParallel(); MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; kernelMat.rowwise() -= center_square_sum.transpose(); kernelMat.colwise() -= x_square_sum; kernelMat /= pow(sigma, 2.0); return kernelMat.array().exp().matrix();&#125;]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>RcppParallel</tag>
        <tag>RcppEigen</tag>
        <tag>Kernel Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp call F77 blas/lapack]]></title>
    <url>%2Fposts%2F201701%2F2017-01-01-Rcpp-call-F77-blas-lapack.html</url>
    <content type="text"><![CDATA[剛好有點時間來研究一下Rcpp怎樣直接使用底層Fortran 77的BLAS跟LAPACK函數 我覺得最難的還是要去讀BLAS或LAPACK的文件，然後配置適當的input餵進去 我這裡就簡單demo一下怎麼樣用LAPACK的dsyevr去計算symmetric matrix的eigenvalues跟eigenvectors (BLAS部分其實很接近，有興趣的人可以自己改成用BLAS的函數去做，一樣用F77_CALL即可) 裡面還是有不少配置，我沒有好好活用，不過我覺得就先這樣吧，等到有需要再慢慢深入去寫 畢竟我現在直接使用BLAS/LAPACK的場合真的不多，寫那麼底層對我真的有點困難Orz 我還是乖乖去用RcppEigen跟RcppArmadillo好了~”~ 不過直接用BLAS，可以gain到一些performance，也有比較多flexible的設定 深入去玩的話，我覺得對程式效能改進有一定幫助 R code: 1234567891011121314151617Sys.setenv("PKG_LIBS" = "$(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)")Rcpp::sourceCpp("eigen_sym_cpp.cpp")n_row &lt;- 1e4Ln_col &lt;- 5e1LA &lt;- matrix(rnorm(n_row * n_col), n_row, n_col)S &lt;- cov(A)library(microbenchmark)microbenchmark( Rcpp_BLAS = eigen_sym_cpp(S), R = eigen(S), times = 100L)# Unit: microseconds# expr min lq mean median uq max neval# Rcpp_BLAS 789.644 874.196 904.5647 891.4575 943.974 1545.057 100# R 1194.267 1344.793 1444.1375 1404.7695 1516.385 3655.062 100 C++ code: 123456789101112131415161718192021#include &lt;Rcpp.h&gt;#include &lt;R_ext/Lapack.h&gt;#include &lt;R_ext/BLAS.h&gt; //[[Rcpp::export]]Rcpp::List eigen_sym_cpp(Rcpp::NumericMatrix X, int num_eig = -1, bool eigenvalues_only = false, double tol = 1.5e-8)&#123; Rcpp::NumericMatrix A = Rcpp::clone(X); // perform deep copy of input char jobz = eigenvalues_only?'N':'V', range = (num_eig == -1)?'A':'I', uplo = 'U'; int N = A.nrow(), lda = std::max(1, N), il = 1, iu = (num_eig == -1)?N:num_eig; int m = (range == 'A')?N:(iu-il+1), ldz = std::max(1, N), lwork = std::max(1, 26*N), liwork = std::max(1, 10*N), info = 0; double abstol = tol, vl = 0.0, vu = 0.0; Rcpp::IntegerVector isuppz(2 * std::max(1, m)), iwork(std::max(1, lwork)); Rcpp::NumericVector work(std::max(1, lwork)), W(N); Rcpp::NumericMatrix Z(ldz, std::max(1, m)); F77_CALL(dsyevr)(&amp;jobz, &amp;range, &amp;uplo, &amp;N, A.begin(), &amp;lda, &amp;vl, &amp;vu, &amp;il, &amp;iu, &amp;abstol, &amp;m, W.begin(), Z.begin(), &amp;ldz, isuppz.begin(), work.begin(), &amp;lwork, iwork.begin(), &amp;liwork, &amp;info); return Rcpp::List::create(Rcpp::Named("info") = info, Rcpp::Named("vectors") = Z, Rcpp::Named("values") = W);&#125; 後記：歡迎留言，我最近才剛把DISQUS，改成可以guest留言，加上不用審查 之前設定錯誤，造成一些問題，害我PTT信箱收了不少信Orz 希望大家新的一年，能夠善用DISQUS留言，不要再塞爆我信箱了XDD 新的一年第一篇部落格，請大家多多指教]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>BLAS</tag>
        <tag>LAPACK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp call F77 blas/lapack (continued)]]></title>
    <url>%2Fposts%2F201701%2F2017-01-01-Rcpp-call-F77-blas-lapack-continued.html</url>
    <content type="text"><![CDATA[想一想還是繼續把上一篇補完 所以這次就活用了LAPACK查詢，去得到the optimal sizes of the WORK array and IWORK array R code沒什麼更動，只是多了一些check results的動作 也試試看第2,3個input得到的結果是否正確 R code: 123456789101112131415161718192021222324252627282930313233343536373839Sys.setenv("PKG_LIBS" = "$(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)")Rcpp::sourceCpp("eigen_sym_cpp.cpp")n_row &lt;- 1e4Ln_col &lt;- 1e1LA &lt;- matrix(rnorm(n_row * n_col), n_row, n_col)S &lt;- cov(A)eig_res_cpp1 &lt;- eigen_sym_cpp(S)eig_res_cpp2 &lt;- eigen_sym_cpp(S, 5L)eig_res_cpp3 &lt;- eigen_sym_cpp(S, -1L, TRUE)eig_res_r &lt;- eigen(S)# check eigenvaluesall.equal(eig_res_cpp1$values, eig_res_r$values) # TRUEall.equal(eig_res_cpp2$values, eig_res_r$values[1L:5L]) # TRUEall.equal(eig_res_cpp3$values, eig_res_r$values) # TRUE# check eigenvectorssign_vectors_cpp1 &lt;- colSums(eig_res_r$vectors * eig_res_cpp1$vectors)eig_res_cpp1$vectors &lt;- sweep(eig_res_cpp1$vectors, 2, sign_vectors_cpp1, '*')all.equal(eig_res_cpp1$vectors, eig_res_r$vectors) # TRUE# check eigenvectorssign_vectors_cpp2 &lt;- colSums(eig_res_r$vectors[ , 1L:5L] * eig_res_cpp2$vectors)eig_res_cpp2$vectors &lt;- sweep(eig_res_cpp2$vectors, 2, sign_vectors_cpp2, '*')all.equal(eig_res_cpp2$vectors, eig_res_r$vectors[ , 1L:5L]) # TRUElibrary(microbenchmark)microbenchmark( Rcpp_BLAS = eigen_sym_cpp(S), R = eigen(S), times = 100L)# Unit: microseconds# expr min lq mean median uq max neval# Rcpp_BLAS 788.180 852.546 872.9262 860.4455 874.196 1000.293 100# R 1203.336 1271.505 1363.7922 1281.4520 1391.457 2432.417 100 C++程式部分採用了RcppEigen做一些輔助的動作 然後加上註解讓整個程式比較容易被讀懂 再來是，增加上面提到的查詢，然後再使用查詢結果去計算eigenvalues跟eigenvectors 並且簡化掉一些不會用到的參數，ex: lda, ldz, iu等 C++ code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// [[Rcpp::depends(RcppEigen)]]#include &lt;RcppEigen.h&gt;#include &lt;R_ext/Lapack.h&gt;#include &lt;R_ext/BLAS.h&gt;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;//[[Rcpp::export]]Rcpp::List eigen_sym_cpp(Rcpp::NumericMatrix xr, int num_eig = -1, bool eigenvalues_only = false, double tol = 1.5e-8)&#123; MatrixXd A = MatrixXd::Map(xr.begin(), xr.nrow(), xr.ncol()); // settings char jobz = eigenvalues_only ? 'N' : 'V', range = (num_eig == -1) ? 'A' : 'I', uplo = 'U'; int N = A.rows(), info = 0; // deside the number of eigenvalues int il = (num_eig == -1) ? 1 : (N - num_eig + 1), M = N - il + 1; // the tolerance and not used arguments vl, vu double abstol = tol, vl = 0.0, vu = 0.0; // query the optimal size of the WORK array and IWORK array int lwork = -1, liwork = -1, iwork_query; VectorXi isuppz(2 * M); VectorXd W(M); MatrixXd Z(N, M); // perform dsyerv and get the optimal size of the WORK array and IWORK array double work_qeury; F77_CALL(dsyevr)(&amp;jobz, &amp;range, &amp;uplo, &amp;N, A.data(), &amp;N, &amp;vl, &amp;vu, &amp;il, &amp;N, &amp;abstol, &amp;M, W.data(), Z.data(), &amp;N, isuppz.data(), &amp;work_qeury, &amp;lwork, &amp;iwork_query, &amp;liwork, &amp;info); // get the optimal sizeㄋ of the WORK array and IWORK array lwork = (int) work_qeury; liwork = iwork_query; VectorXd work(lwork); VectorXi iwork(liwork); // perform dsyerv and get the results of eigen decomposition F77_CALL(dsyevr)(&amp;jobz, &amp;range, &amp;uplo, &amp;N, A.data(), &amp;N, &amp;vl, &amp;vu, &amp;il, &amp;N, &amp;abstol, &amp;M, W.data(), Z.data(), &amp;N, isuppz.data(), work.data(), &amp;lwork, iwork.data(), &amp;liwork, &amp;info); // reverse the eigenvalues to sort in the descending order W.reverseInPlace(); // return eigenvalues only if (eigenvalues_only) return Rcpp::List::create( Rcpp::Named("LAPACK_info") = info, Rcpp::Named("values") = W ); // reverse the eigenvectors to sort in the order of eigenvalues MatrixXd Z2 = Z.rowwise().reverse(); // reutrn eigenvalues and eigenvectors return Rcpp::List::create( Rcpp::Named("LAPACK_info") = info, Rcpp::Named("vectors") = Z2, Rcpp::Named("values") = W );&#125; 這樣整個就算完工了~~]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>BLAS</tag>
        <tag>LAPACK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kernal matrix computation in Rcpp (continued)]]></title>
    <url>%2Fposts%2F201701%2F2017-01-01-kernal-matrix-computation-in-Rcpp-continued.html</url>
    <content type="text"><![CDATA[kernal matrix computation這個主題不只被我用了一次 其實我在PTT分享RcppParallel也是用了這個當例子，文章連結 那這裡就延續上篇的程式把RcppParallel一起拉進來比較一下吧 因為我已經知道p大的時候，我每一個都逐一算其實很慢 那我這裡的RcppParallel就改變一下做法 讓RcppParallel不會因為p改變而使得計算效率改變 這裡新增三個cpp檔案，分別是：RcppArmadillo 1.RcppArmadillo with RcppParallel RcppEigen with RcppParallel RcppEigen and OpenMP with RcppParallel 其他三個就在前一篇都有了，這裡就不重複貼了 R code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253library(kernlab)Rcpp::sourceCpp("kernel_matrix_arma.cpp")Rcpp::sourceCpp("kernel_matrix_eigen1.cpp")Rcpp::sourceCpp("kernel_matrix_eigen2.cpp")Rcpp::sourceCpp("kernel_matrix_arma_para.cpp")Rcpp::sourceCpp("kernel_matrix_eigen_para1.cpp")Rcpp::sourceCpp("kernel_matrix_eigen_para2.cpp") N &lt;- 5000Lp &lt;- 1000Lb &lt;- 500LX &lt;- matrix(rnorm(N*p), ncol = p)center &lt;- X[sample(N, b),]sigma &lt;- 2.5kernelMatrix_R &lt;- function(X, center, sigma)&#123; exp(sweep(sweep(X %*% t(center), 1, rowSums(X**2)/2), 2, rowSums(center**2)/2) / (sigma**2))&#125;res_kernlab &lt;- kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)@.Data# check results with kernlab::kernelMatrixall.equal(kernelMatrix_R(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_arma_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_omp_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_arma_para_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_para_cpp(X, center, sigma), res_kernlab) # TRUEall.equal(kernelMatrix_eigen_para_omp_cpp(X, center, sigma), res_kernlab) # TRUElibrary(microbenchmark)microbenchmark( Rfun = kernelMatrix_R(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), RcppArmadillo = kernelMatrix_arma_cpp(X, center, sigma), RcppEigen = kernelMatrix_eigen_cpp(X, center, sigma), RcppEigen_Openmp = kernelMatrix_eigen_omp_cpp(X, center, sigma), RcppArmadillo_RcppParallel = kernelMatrix_arma_cpp(X, center, sigma), RcppEigen_RcppParallel = kernelMatrix_eigen_para_cpp(X, center, sigma), RcppEigen_RcppParallel_Openmp = kernelMatrix_eigen_para_omp_cpp(X, center, sigma), times = 30L)# Unit: milliseconds# expr min lq mean median uq max neval# Rfun 238.5369 255.9128 273.9832 268.1477 284.9538 348.0322 30# kernlab 226.1284 248.5015 267.4762 261.4748 275.2627 372.2815 30# RcppArmadillo 173.2054 188.7308 199.1266 193.7259 207.7513 284.9052 30# RcppEigen 416.1434 419.6085 425.7891 422.5549 429.4397 456.2896 30# RcppEigen_Openmp 204.2332 210.2853 225.0568 214.8849 224.6437 324.5009 30# RcppArmadillo_RcppParallel 170.5573 196.8640 204.4135 207.8597 212.0638 224.6665 30# RcppEigen_RcppParallel 411.3365 416.3367 425.0875 419.2989 429.4684 459.8628 30# RcppEigen_RcppParallel_Openmp 203.5772 211.5603 246.5694 224.0399 277.4804 377.0851 30 kernel_matrix_arma_para.cpp: 123456789101112131415161718192021222324252627282930313233343536373839// [[Rcpp::depends(RcppArmadillo, RcppParallel)]]#include &lt;RcppArmadillo.h&gt;#include &lt;RcppParallel.h&gt;using arma::mat;using arma::vec;using arma::rowvec;struct KernelComputeWorker: public RcppParallel::Worker &#123; const vec&amp; x_square_sum; const rowvec&amp; center_square_sum; const double&amp; sigma2; mat&amp; kernelMat; KernelComputeWorker(const vec&amp; x_square_sum, const rowvec&amp; center_square_sum, const double&amp; sigma2, mat&amp; kernelMat): x_square_sum(x_square_sum), center_square_sum(center_square_sum), sigma2(sigma2), kernelMat(kernelMat) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (int k = (int) begin; k &lt; (int) end; ++k) &#123; int j = k / kernelMat.n_rows; int i = k - j * kernelMat.n_rows; kernelMat(i, j) -= (x_square_sum(i) + center_square_sum(j)); kernelMat(i, j) = exp(kernelMat(i, j) / sigma2); &#125; &#125;&#125;;// [[Rcpp::export]]mat kernelMatrix_arma_para_cpp(mat x, mat center, double sigma) &#123; mat kernelMat(x * center.t()); vec x_square_sum = sum(square(x), 1) / 2.0; rowvec center_square_sum = (sum(square(center), 1)).t() / 2.0; double sigma2 = pow(sigma, 2.0); KernelComputeWorker worker_kc(x_square_sum, center_square_sum, sigma2, kernelMat); RcppParallel::parallelFor(0, kernelMat.n_elem, worker_kc); return kernelMat;&#125; kernel_matrix_eigen_para1.cpp: 123456789101112131415161718192021222324252627282930313233343536373839// [[Rcpp::depends(RcppEigen, RcppParallel)]]#include &lt;RcppEigen.h&gt;#include &lt;RcppParallel.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;struct KernelComputeWorker: public RcppParallel::Worker &#123; const VectorXd&amp; x_square_sum; const VectorXd&amp; center_square_sum; const double&amp; sigma2; MatrixXd&amp; kernelMat; KernelComputeWorker(const VectorXd&amp; x_square_sum, const VectorXd&amp; center_square_sum, const double&amp; sigma2, MatrixXd&amp; kernelMat): x_square_sum(x_square_sum), center_square_sum(center_square_sum), sigma2(sigma2), kernelMat(kernelMat) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (int k = (int) begin; k &lt; (int) end; ++k) &#123; int j = k / kernelMat.rows(); int i = k - j * kernelMat.rows(); kernelMat(i, j) -= (x_square_sum(i) + center_square_sum(j)); kernelMat(i, j) = exp(kernelMat(i, j) / sigma2); &#125; &#125;&#125;;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_para_cpp(MatrixXd x, MatrixXd center, double sigma) &#123; MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; double sigma2 = pow(sigma, 2.0); KernelComputeWorker worker_kc(x_square_sum, center_square_sum, sigma2, kernelMat); RcppParallel::parallelFor(0, kernelMat.size(), worker_kc); return kernelMat;&#125; kernel_matrix_eigen_para2.cpp: 123456789101112131415161718192021222324252627282930313233343536373839404142434445// [[Rcpp::depends(RcppEigen, RcppParallel)]]// [[Rcpp::plugins(openmp)]]#include &lt;RcppEigen.h&gt;#include &lt;omp.h&gt;#include &lt;RcppParallel.h&gt;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::VectorXi;struct KernelComputeWorker: public RcppParallel::Worker &#123; const VectorXd&amp; x_square_sum; const VectorXd&amp; center_square_sum; const double&amp; sigma2; MatrixXd&amp; kernelMat; KernelComputeWorker(const VectorXd&amp; x_square_sum, const VectorXd&amp; center_square_sum, const double&amp; sigma2, MatrixXd&amp; kernelMat): x_square_sum(x_square_sum), center_square_sum(center_square_sum), sigma2(sigma2), kernelMat(kernelMat) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (int k = (int) begin; k &lt; (int) end; ++k) &#123; int j = k / kernelMat.rows(); int i = k - j * kernelMat.rows(); kernelMat(i, j) -= (x_square_sum(i) + center_square_sum(j)); kernelMat(i, j) = exp(kernelMat(i, j) / sigma2); &#125; &#125;&#125;;// [[Rcpp::export]]MatrixXd kernelMatrix_eigen_para_omp_cpp(MatrixXd x, MatrixXd center, double sigma) &#123; int n = omp_get_max_threads(); omp_set_num_threads(n); Eigen::setNbThreads(n); Eigen::initParallel(); MatrixXd kernelMat = x * center.transpose(); VectorXd x_square_sum = x.array().pow(2.0).rowwise().sum() / 2.0; VectorXd center_square_sum = center.array().pow(2.0).rowwise().sum() / 2.0; double sigma2 = pow(sigma, 2.0); KernelComputeWorker worker_kc(x_square_sum, center_square_sum, sigma2, kernelMat); RcppParallel::parallelFor(0, kernelMat.size(), worker_kc); return kernelMat;&#125; 這個結果可以看出幾件事情： 直接用Multi-threaded BLAS的RcppArmadillo在這個問題上，用了RcppParallel還是略輸一籌，BLAS還是比較強大XD 相較RcppEigen_RcppParallel跟RcppEigen_RcppParallel_Openmp來看，因為沒平行的部分只剩下一開始計算kernelMat、x_square_sum以及center_square_sum，所以這裡RcppEigen用它本身的BLAS庫來做乘除就慢上了不少，而後面的計算其實也是用colwise, rowwise以及element-wise的操作，那邊已經被平行做掉了，還是跟只單用RcppEigen差不多快，所以其實element-wise, colwise以及rowwise這幾個操作應該不至於造成瓶頸，因此，最大的瓶頸是在大矩陣的乘法，需要openmp使用multi-thread來加速 RcppEigen_RcppParallel_Openmp還是比RcppEigen_Openmp比，所以其實後段的處理是白費的，用原本的方法比較快 (RcppArmadillo的部分同理)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>RcppParallel</tag>
        <tag>RcppEigen</tag>
        <tag>Kernel Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Weighted Least-Square Algorithm]]></title>
    <url>%2Fposts%2F201612%2F2016-12-31-weighted-least-square-algorithm.html</url>
    <content type="text"><![CDATA[最近一直work on locally weighted least-square 結果發現使用gausvar這個kernel的時候，weight會出現負的 我原本的解法是直接在input跟output都乘上根號的weight 結果這招就行不通了 另外還有再一些case下，解不是穩健的，有可能跑出虛數，但是虛數的係數其實很小… 所以就下定決心來研究一下各種WLS的解法 稍微GOOGLE了一下，發現不外乎下面四種解法： 直接解，就是利用(X^T * W * X)^(-1) * X^T * W * y去解出迴歸係數 再來就是把inverse部分用pseudo inverse代替，以避免rank不足的問題出現 Cholesky Decomposition (LDL Decomposition) QR Decomposition 效率的話，4 &gt; 3 &gt; 1 &gt; 2，但是QR在一些情況下會跑出虛數 所以我這裡會偏向以3為主 下面是用程式去實作各種解法： R code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546Rcpp::sourceCpp("wls.cpp")n &lt;- 500p &lt;- 150X &lt;- matrix(rnorm(n * p), n , p)beta &lt;- rnorm(p)w &lt;- sqrt(rowMeans(X**2) - (n / (n-1)) * rowMeans(X)**2)y &lt;- 3 + X %*% beta + rnorm(n)library(microbenchmark)microbenchmark( eigen_llt = eigen_llt(X, w, y), eigen_ldlt = eigen_ldlt(X, w, y), eigen_fullLU = eigen_fullLU(X, w, y), eigen_HHQR = eigen_HHQR(X, w, y), eigen_colPivHHQR = eigen_colPivHHQR(X, w, y), eigen_fullPivHHQR = eigen_fullPivHHQR(X, w, y), eigen_chol_llt1 = eigen_chol_llt1(X, w, y), eigen_chol_llt2 = eigen_chol_llt2(X, w, y), eigen_chol_llt3 = eigen_chol_llt3(X, w, y), arma_qr = arma_qr(X, w, y), # can't run if n is too big arma_pinv = arma_pinv(X, w, y), arma_chol1 = arma_chol1(X, w, y), arma_chol2 = arma_chol2(X, w, y), arma_direct1 = arma_direct1(X, w, y), arma_direct2 = arma_direct2(X, w, y), r_lm = coef(lm(y ~ -1 + X, weights = w)), times = 30L)# Unit: microseconds# expr min lq mean median uq max neval# eigen_llt 1787.601 1814.225 2341.2993 2287.1645 2889.126 2912.239 30# eigen_ldlt 1812.763 1846.408 2292.7815 2089.9725 2928.916 3020.197 30# eigen_fullLU 3112.649 3133.129 3673.1350 3242.1115 4610.021 4725.294 30# eigen_HHQR 2334.999 2401.120 3095.5537 3073.1525 3820.669 3920.142 30# eigen_colPivHHQR 2411.945 2423.941 2969.3488 2691.4950 3756.888 4029.855 30# eigen_fullPivHHQR 3449.397 3477.776 4179.7097 3585.0035 5282.639 5363.389 30# eigen_chol_llt1 3302.234 3359.286 4111.7262 3828.5675 5297.852 5362.510 30# eigen_chol_llt2 3268.004 3308.379 4065.1882 3418.3850 5253.674 5296.390 30# eigen_chol_llt3 3354.020 3397.027 4130.9969 3857.0925 4929.800 5425.121 30# arma_qr 1868.936 2167.357 2330.0549 2316.5675 2419.552 2829.442 30# arma_pinv 4137.229 4723.245 5024.8460 4877.1375 5474.272 5722.957 30# arma_chol1 702.167 959.337 1042.1535 1041.1100 1147.167 1291.404 30# arma_chol2 780.869 1046.523 1121.4886 1125.5160 1234.645 1423.645 30# arma_direct1 4473.977 4636.645 4956.8919 4701.4490 5481.294 5867.193 30# arma_direct2 676.129 898.482 963.5788 965.4805 1060.565 1184.615 30# r_lm 6257.189 6817.459 11962.0246 8113.9820 9301.084 123398.876 30 C++ code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139// [[Rcpp::depends(RcppArmadillo, RcppEigen)]]#include &lt;RcppArmadillo.h&gt;#include &lt;RcppEigen.h&gt;using namespace arma;// [[Rcpp::export]]Eigen::VectorXd eigen_fullPivHHQR(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).fullPivHouseholderQr().solve(X.transpose() * w.asDiagonal() * y);; return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_colPivHHQR(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).colPivHouseholderQr().solve(X.transpose() * w.asDiagonal() * y);; return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_HHQR(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).householderQr().solve(X.transpose() * w.asDiagonal() * y); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_fullLU(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).fullPivLu().solve(X.transpose() * w.asDiagonal() * y); return beta;&#125; // [[Rcpp::export]]Eigen::VectorXd eigen_llt(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).llt().solve(X.transpose() * w.asDiagonal() * y); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_ldlt(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd beta = (X.transpose() * w.asDiagonal() * X).ldlt().solve(X.transpose() * w.asDiagonal() * y); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_chol_llt1(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::MatrixXd XWX = X.transpose() * w.asDiagonal() * X; Eigen::MatrixXd R = XWX.llt().matrixU(); Eigen::VectorXd XWY = X.transpose() * (w.array() * y.array()).matrix(); Eigen::VectorXd beta = R.householderQr().solve(R.transpose().householderQr().solve(XWY)); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_chol_llt2(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::MatrixXd XW = X.transpose() * w.asDiagonal(); Eigen::MatrixXd R = (XW * X).llt().matrixU(); Eigen::VectorXd beta = R.householderQr().solve(R.transpose().householderQr().solve(XW * y)); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_chol_llt3(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::MatrixXd XW(X.cols(), X.rows()); for (unsigned int i = 0; i &lt; X.cols(); ++i) XW.row(i) = X.col(i).array() * w.array(); Eigen::MatrixXd R = (XW * X).llt().matrixU(); Eigen::VectorXd beta = R.householderQr().solve(R.transpose().householderQr().solve(XW * y)); return beta;&#125;// [[Rcpp::export]]Eigen::VectorXd eigen_colPivHHQR2(const Eigen::Map&lt;Eigen::MatrixXd&gt; &amp; X, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; w, const Eigen::Map&lt;Eigen::VectorXd&gt; &amp; y) &#123; Eigen::VectorXd sw = w.cwiseSqrt(); Eigen::MatrixXd XW(X.rows(), X.cols()); for (unsigned int i = 0; i &lt; X.cols(); ++i) XW.col(i) = X.col(i).array() * sw.array(); Eigen::VectorXd beta = XW.colPivHouseholderQr().solve(y.cwiseProduct(sw)); return beta;&#125;// [[Rcpp::export]]arma::vec arma_qr(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; mat Q, R; arma::qr_econ(Q, R, X.each_col() % sqrt(w)); vec p = solve(R, Q.t() * (y % sqrt(w))); return p;&#125;// [[Rcpp::export]]arma::vec arma_pinv(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; vec p = pinv(X.t() * (repmat(w, 1, X.n_cols) % X)) * X.t() * (w % y); return p;&#125;// [[Rcpp::export]]arma::vec arma_chol1(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; mat R = chol((X.each_col() % w).t() * X); vec p = solve(R, solve(R.t(), X.t() * (w % y), solve_opts::fast), solve_opts::fast); return p;&#125;// [[Rcpp::export]]arma::vec arma_chol2(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; mat XW = (X.each_col() % w).t(); mat R = chol(XW * X); vec p = solve(R, solve(R.t(), XW * y, solve_opts::fast), solve_opts::fast); return p;&#125;// [[Rcpp::export]]arma::vec arma_direct1(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; vec sw = sqrt(w); vec p = solve(X.each_col() % sw, y % sw); return p;&#125;// [[Rcpp::export]]arma::vec arma_direct2(const arma::mat&amp; X, const arma::vec&amp; w, const arma::vec&amp; y) &#123; vec p = solve((X.each_col() % w).t() * X, X.t() * (w % y)); return p;&#125; 這裡的QR解得很慢，我不知道要怎麼樣讓armadillo只輸出跟rank一樣多的Q，R矩陣就好 而直接解會是最快的，我猜這原因是裡面某部分有被優化過了… 不然以程式來看，Cholesky Decomposition的performance是最好的 只是我也不解的是Eigen也用一樣的方法去做 卻比Armadillo手動去寫慢了好幾倍 (eigen_chol_llt vs arma_chol) 不確定是不是Eigen在solve linear system時用不一樣的LAPACK函數 或是Eigen在這做了比較多check 這裡就留給後人慢慢玩賞QQ 2017/4/20補充： 我後來試了一些簡單的case 發現其實在p不大(大概小於80)，n也不大(小於200)的情況 RcppEigen擁有比較好的performance，我的猜測是SSE指令集帶來的好處 測試script如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748library(iterators)library(foreach)library(data.table)library(pipeR)library(Rcpp)library(RcppArmadillo)library(RcppEigen)library(microbenchmark)Rcpp::sourceCpp("wls.cpp")trainFunc &lt;- function(n, p)&#123; X &lt;- matrix(rnorm(n * p), n , p) beta &lt;- rnorm(p) w &lt;- rgamma(nrow(X), 2, 0.5) y &lt;- 3 + X %*% beta + rnorm(n) microRes &lt;- microbenchmark( eigen_llt = eigen_llt(X, w, y), eigen_ldlt = eigen_ldlt(X, w, y), eigen_chol_llt1 = eigen_chol_llt1(X, w, y), eigen_chol_llt2 = eigen_chol_llt2(X, w, y), eigen_chol_llt3 = eigen_chol_llt3(X, w, y), arma_chol1 = arma_chol1(X, w, y), arma_chol2 = arma_chol2(X, w, y), arma_direct1 = arma_direct1(X, w, y), arma_direct2 = arma_direct2(X, w, y), r_lm = coef(lm(y ~ -1 + X, weights = w)), times = 30L ) m &lt;- tapply(microRes$time, microRes$expr, median) / 1000 return(data.table(n = n, p = p, method = names(m), median_time = m))&#125;paraList &lt;- CJ(p = c(1:20, seq(25, 100, 5)), n = c(20, 30, 50, 75, 100, 200)) %&gt;&gt;% `[`(n &gt; p)paraResDT &lt;- foreach(v = iter(paraList, by = "row"), .final = rbindlist) %do% trainFunc(v$n, v$p)library(lattice)xyplot(median_time ~ p | factor(n, c(20, 30, 50, 75, 100, 200)), paraResDT, groups = method, type = "o", auto.key = list(points = TRUE, columns = 3), scales = list(x = list(relation = "free"), y = list(relation = "free")))xyplot(median_time ~ p | factor(n, c(20, 30, 50, 75, 100, 200)), paraResDT[!method %in% c("r_lm", "arma_direct1", "eigen_chol_llt1", "eigen_chol_llt2", "eigen_chol_llt3")], groups = method, type = "o", auto.key = list(points = TRUE, columns = 3), scales = list(x = list(relation = "free"), y = list(relation = "free"))) 基於上面的結論，所以我會建議這樣去寫wls的solver: 1234567891011121314151617// [[Rcpp::depends(RcppArmadillo, RcppEigen)]]#include &lt;RcppArmadillo.h&gt;#include &lt;RcppEigen.h&gt;// [[Rcpp::export]]arma::vec fastSolve(arma::mat X, arma::vec w, arma::vec y) &#123; if (X.n_rows * X.n_cols &lt;= 6000) &#123; Eigen::MatrixXd X2 = Eigen::MatrixXd::Map(X.memptr(), X.n_rows, X.n_cols); Eigen::VectorXd w2 = Eigen::VectorXd::Map(w.memptr(), w.size()); Eigen::VectorXd y2 = Eigen::VectorXd::Map(y.memptr(), y.size()); Eigen::VectorXd out = (X2.transpose() * w2.asDiagonal() * X2).llt().solve(X2.transpose() * w2.asDiagonal() * y2); arma::vec p(out.data(), out.rows(), false); return p; &#125; else &#123; return arma::solve((X.each_col() % w).t() * X, X.t() * (w % y)); &#125;&#125;]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>RcppArmadillo</tag>
        <tag>RcppEigen</tag>
        <tag>WLS</tag>
        <tag>Least-Square</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RcppEigen Work with RcppParallel]]></title>
    <url>%2Fposts%2F201612%2F2016-12-31-RcppEigen-Work-With-RcppParall.html</url>
    <content type="text"><![CDATA[最近在比較RcppEigen跟RcppArmadillo，比較的結果寫到下一篇好了 就想說也來試試看RcppEigen跟RcppParallel結合看看有什麼火花 這裡是改之前的RcppArmadillo with RcppParallel的程式 - fastLOOCV 順便也把RcppEigen vs RcppArmadillo 跟 Openmp vs RcppParallel 一起放入混合比較一下 R code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344Rcpp::sourceCpp("RcppLOOCV.cpp") N &lt;- 600Lp &lt;- 150LX &lt;- matrix(rnorm(N*p), N)b &lt;- rnorm(p, rnorm(2L), rgamma(2L,3,7))y &lt;- 1.5 + as.vector(X %*% b) + rnorm(N) system.time(&#123; out &lt;- sapply(1L:N, function(i)&#123; idx &lt;- setdiff(1L:N, i) b &lt;- coef(lm(y[idx] ~ X[idx , ])) return((y[i] - crossprod(c(1, X[i, ]), b))**2) &#125;)&#125;)# user system elapsed# 8.83 0.28 9.20 # check resultsfor (i in 1L:5L) stopifnot(abs(mean(out) - arma_fastLOOCV1(y, X)) &lt; 1e-8)for (i in 1L:5L) stopifnot(abs(mean(out) - arma_fastLOOCV2(y, X)) &lt; 1e-8)for (i in 1L:5L) stopifnot(abs(mean(out) - eigen_fastLOOCV1(y, X)) &lt; 1e-8)for (i in 1L:5L) stopifnot(abs(mean(out) - eigen_fastLOOCV2(y, X)) &lt; 1e-8)for (i in 1L:5L) stopifnot(abs(mean(out) - eigen_fastLOOCV3(y, X)) &lt; 1e-8)for (i in 1L:5L) stopifnot(abs(mean(out) - eigen_fastLOOCV4(y, X)) &lt; 1e-8) library(microbenchmark)microbenchmark( arma_fastLOOCV1(y, X), # RcppArmadillo with RcppPrallel arma_fastLOOCV2(y, X), # RcppArmadillo with openmp eigen_fastLOOCV1(y, X), # RcppEigen with RcppPrallel (Reduce) eigen_fastLOOCV2(y, X), # RcppEigen with RcppPrallel (For) eigen_fastLOOCV3(y, X), # RcppEigen with openmp eigen_fastLOOCV4(y, X), # RcppEigen without parallism times = 30L)# Unit: milliseconds# expr min lq mean median uq max neval# arma_fastLOOCV1(y, X) 1009.9239 1037.4614 1052.9272 1045.9733 1068.5668 1101.5399 30# arma_fastLOOCV2(y, X) 1020.5068 1033.6180 1051.7171 1046.8751 1064.4512 1139.6371 30# eigen_fastLOOCV1(y, X) 623.7900 628.0837 639.1062 635.0657 644.8178 716.8571 30# eigen_fastLOOCV2(y, X) 620.1337 624.2227 634.4193 630.3217 641.1066 682.0993 30# eigen_fastLOOCV3(y, X) 620.0021 630.6396 643.2877 638.7085 655.9284 706.5759 30# eigen_fastLOOCV4(y, X) 2627.1159 2648.5870 2657.8932 2654.8032 2663.2681 2707.9168 30 C++ code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210// [[Rcpp::depends(RcppArmadillo, RcppEigen, RcppParallel)]]// [[Rcpp::plugins(openmp)]]#include &lt;RcppArmadillo.h&gt;#include &lt;RcppEigen.h&gt;#include &lt;omp.h&gt;#include &lt;RcppParallel.h&gt;using arma::mat;using arma::vec;using arma::uvec;using arma::uword;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd; struct arma_MSE_Compute: public RcppParallel::Worker &#123; const mat&amp; X; const vec&amp; y; const uvec&amp; index; double mse; arma_MSE_Compute(const mat&amp; X, const vec&amp; y, const uvec&amp; index): X(X), y(y), index(index), mse(0.0) &#123;&#125; arma_MSE_Compute(const arma_MSE_Compute&amp; arma_MSE_worker, RcppParallel::Split): X(arma_MSE_worker.X), y(arma_MSE_worker.y), index(arma_MSE_worker.index), mse(0.0) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (uword i = begin; i &lt; end; ++i) &#123; uvec idx = arma::find(index != i); mse += pow(y(i) - arma::dot(X.row(i), arma::solve(X.rows(idx), y.elem(idx))), 2.0); &#125; &#125; void join(const arma_MSE_Compute&amp; rhs) &#123; mse += rhs.mse; &#125;&#125;; // [[Rcpp::export]]double arma_fastLOOCV1(const arma::vec&amp; y, const arma::mat&amp; X) &#123; mat X_with_ones = arma::join_rows(arma::ones&lt;vec&gt;(X.n_rows), X); uvec index = arma::linspace&lt;uvec&gt;(0, y.n_elem - 1, y.n_elem); arma_MSE_Compute mseResults(X_with_ones, y, index); RcppParallel::parallelReduce(0, y.n_elem, mseResults); return mseResults.mse / y.n_elem;&#125; // [[Rcpp::export]]double arma_fastLOOCV2(const arma::vec&amp; y, const arma::mat&amp; X) &#123; mat X_with_ones = arma::join_rows(arma::ones&lt;vec&gt;(X.n_rows), X); uvec index = arma::linspace&lt;uvec&gt;(0, y.n_elem - 1, y.n_elem); vec mse = arma::zeros&lt;vec&gt;(y.n_elem); uword i = 0; #pragma omp parallel for private(i) for (i = 0; i &lt; y.n_elem; ++i) &#123; uvec idx = arma::find(index != i); mse(i) = pow(y(i) - arma::dot(X_with_ones.row(i), arma::solve(X_with_ones.rows(idx), y.elem(idx))), 2.0); &#125; return mean(mse);&#125; struct eigen_MSE_Compute: public RcppParallel::Worker &#123; MatrixXd X; VectorXd y; double mse; eigen_MSE_Compute(MatrixXd X, VectorXd y): X(X), y(y), mse(0.0) &#123;&#125; eigen_MSE_Compute(const eigen_MSE_Compute&amp; eigen_MSE_worker, RcppParallel::Split): X(eigen_MSE_worker.X), y(eigen_MSE_worker.y), mse(0.0) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (unsigned int i = begin; i &lt; end; ++i) &#123; MatrixXd tmpX(X.rows() - 1, X.cols()); VectorXd tmpY(y.size() - 1); if (i == 0) &#123; tmpX = X.bottomRows(X.rows() - 1); tmpY = y.tail(y.size() - 1); &#125; else if (i == X.rows() - 1) &#123; tmpX = X.topRows(X.rows() - 1); tmpY = y.head(y.size() - 1); &#125; else &#123; tmpX &lt;&lt; X.topRows(i), X.bottomRows(X.rows() - i - 1); tmpY &lt;&lt; y.head(i), y.tail(y.size() - i - 1); &#125; mse += pow(y(i) - (tmpX.colPivHouseholderQr().solve(tmpY)).dot(X.row(i)), 2.0); &#125; &#125; void join(const eigen_MSE_Compute&amp; rhs) &#123; mse += rhs.mse; &#125;&#125;; // [[Rcpp::export]]double eigen_fastLOOCV1(const Eigen::Map&lt;VectorXd&gt;&amp; y, const Eigen::Map&lt;MatrixXd&gt;&amp; X) &#123; MatrixXd X_with_ones(X.rows(), X.cols() + 1); X_with_ones &lt;&lt; MatrixXd::Ones(X.rows(), 1), X; eigen_MSE_Compute mseResults(X_with_ones, y); RcppParallel::parallelReduce(0, y.size(), mseResults); return mseResults.mse / y.size();&#125; struct eigen_MSE_Compute2: public RcppParallel::Worker &#123; const MatrixXd&amp; X; const VectorXd&amp; y; VectorXd&amp; mse; eigen_MSE_Compute2(const MatrixXd&amp; X, const VectorXd&amp; y, VectorXd&amp; mse): X(X), y(y), mse(mse) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (unsigned int i = begin; i &lt; end; ++i) &#123; MatrixXd tmpX(X.rows() - 1, X.cols()); VectorXd tmpY(y.size() - 1); if (i == 0) &#123; tmpX = X.bottomRows(X.rows() - 1); tmpY = y.tail(y.size() - 1); &#125; else if (i == X.rows() - 1) &#123; tmpX = X.topRows(X.rows() - 1); tmpY = y.head(y.size() - 1); &#125; else &#123; tmpX &lt;&lt; X.topRows(i), X.bottomRows(X.rows() - i - 1); tmpY &lt;&lt; y.head(i), y.tail(y.size() - i - 1); &#125; mse(i) = pow(y(i) - (tmpX.colPivHouseholderQr().solve(tmpY)).dot(X.row(i)), 2.0); &#125; &#125;&#125;; // [[Rcpp::export]]double eigen_fastLOOCV2(Rcpp::NumericVector yin, const Eigen::Map&lt;MatrixXd&gt;&amp; X) &#123; // Eigen::Map&lt;VectorXd&gt;&amp; object in RcppParall::Worker would cause crash // but we can input Rcpp::NumericVector, and use VectorXd::Map to convert to VectorXd VectorXd y = VectorXd::Map(yin.begin(), yin.size()); MatrixXd X_with_ones(X.rows(), X.cols() + 1); X_with_ones &lt;&lt; MatrixXd::Ones(X.rows(), 1), X; VectorXd mse = VectorXd::Zero(y.size()); eigen_MSE_Compute2 mseResults(X_with_ones, y, mse); RcppParallel::parallelFor(0, y.size(), mseResults); return mse.mean();&#125; // [[Rcpp::export]]double eigen_fastLOOCV3(const Eigen::Map&lt;VectorXd&gt;&amp; y, const Eigen::Map&lt;MatrixXd&gt;&amp; X) &#123; MatrixXd X_with_ones(X.rows(), X.cols() + 1); X_with_ones &lt;&lt; MatrixXd::Ones(X.rows(), 1), X; VectorXd mse = VectorXd::Zero(y.size()); unsigned int i = 0; #pragma omp parallel for private(i) for (i = 0; i &lt; X.rows(); ++i) &#123; MatrixXd tmpX(X.rows() - 1, X_with_ones.cols()); VectorXd tmpY(y.size() - 1); if (i == 0) &#123; tmpX = X_with_ones.bottomRows(X_with_ones.rows() - 1); tmpY = y.tail(y.size() - 1); &#125; else if (i == X_with_ones.rows() - 1) &#123; tmpX = X_with_ones.topRows(X_with_ones.rows() - 1); tmpY = y.head(y.size() - 1); &#125; else &#123; tmpX &lt;&lt; X_with_ones.topRows(i), X_with_ones.bottomRows(X_with_ones.rows() - i - 1); tmpY &lt;&lt; y.head(i), y.tail(y.size() - i - 1); &#125; mse(i) = pow(y(i) - (tmpX.colPivHouseholderQr().solve(tmpY)).dot(X_with_ones.row(i)), 2.0); &#125; return mse.mean();&#125; // [[Rcpp::export]]double eigen_fastLOOCV4(const Eigen::Map&lt;VectorXd&gt;&amp; y, const Eigen::Map&lt;MatrixXd&gt;&amp; X) &#123; MatrixXd X_with_ones(X.rows(), X.cols() + 1); X_with_ones &lt;&lt; MatrixXd::Ones(X.rows(), 1), X; double mse = 0.0; for (unsigned int i = 0; i &lt; X.rows(); ++i) &#123; MatrixXd tmpX(X.rows() - 1, X_with_ones.cols()); VectorXd tmpY(y.size() - 1); if (i == 0) &#123; tmpX = X_with_ones.bottomRows(X_with_ones.rows() - 1); tmpY = y.tail(y.size() - 1); &#125; else if (i == X_with_ones.rows() - 1) &#123; tmpX = X_with_ones.topRows(X_with_ones.rows() - 1); tmpY = y.head(y.size() - 1); &#125; else &#123; tmpX &lt;&lt; X_with_ones.topRows(i), X_with_ones.bottomRows(X_with_ones.rows() - i - 1); tmpY &lt;&lt; y.head(i), y.tail(y.size() - i - 1); &#125; mse += pow(y(i) - (tmpX.colPivHouseholderQr().solve(tmpY)).dot(X_with_ones.row(i)), 2.0); &#125; return mse / y.size();&#125; 結論： Armadillo的程式雖然簡短漂亮，但是Performance真的不如Eigen來的好 至於Openmp與RcppParallel這裡比起來根本沒什麼差異 回到這篇的重點，Eigen::Map&lt;VectorXd&gt;這個的背後還是Rcpp::NumericVector或是R的SEXP 在input進去RcppParallel的時候，會出問題，所以都必須確定轉成Eigen::VectorXd才行 不然R會crash或是得到錯誤的答案，我為了測試這個花了好多心力Orz]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>RcppParallel</tag>
        <tag>RcppEigen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences between RcppEigen and RcppArmadillo]]></title>
    <url>%2Fposts%2F201612%2F2016-12-31-Differences-between-RcppEigen-and-RcppArmadillo.html</url>
    <content type="text"><![CDATA[R maillist有討論過這個問題，而且回的非常長 我稍微看了一下，發問者試了RcppArmadillo跟RcppEigen 發現RcppEigen的SVD decomposition不是那麼合用，而且不太好去做coding RcppArmadillo比較好去coding 接下來的回文就是RcppEigen會快一點(我的文章也證實了這點) 而RcppEigen是用自己的BLAS and LAPACK，RcppArmadillo會使用R session所使用的 然後接下來就跳到RcppArmadillo的svd表現不好的問題： 123456789101112131415161718192021222324library(inline)library(RcppArmadillo)library(RcppEigen)arma_body &lt;- 'using namespace arma; NumericMatrix Xr(Xs); mat X = Rcpp::as&lt;mat&gt;(Xr), U, V; vec s; svd(U, s, V, X); return wrap(s);'arma_svd &lt;- cxxfunction(signature(Xs = "numeric"), body = arma_body, plugin = "RcppArmadillo")eigen_body &lt;- 'using Eigen::MatrixXd; using Eigen::Map;Map&lt;MatrixXd&gt; x(Rcpp::as&lt;Map&lt;MatrixXd&gt; &gt;(Xs));Eigen::JacobiSVD&lt;MatrixXd&gt; svd(x, Eigen::ComputeThinU | Eigen::ComputeThinV);return wrap(svd.singularValues());'eigen_svd &lt;- cxxfunction(signature(Xs = "numeric"), body = eigen_body, plugin = "RcppEigen")library(microbenchmark)N &lt;- 1000LA &lt;- matrix(rnorm(N^2), N)microbenchmark(svd(A), arma_svd(A), eigen_svd(A), times = 20L)# Unit: milliseconds# expr min lq mean median uq max neval# svd(A) 436.0527 441.6736 444.1783 443.7081 445.4228 454.5612 10# arma_svd(A) 354.7630 359.3738 365.7064 365.5194 367.7162 383.8402 10# eigen_svd(A) 56648.3330 56700.3132 57499.2423 57345.0418 58318.3001 58883.5604 10 我在我電腦(i7-3770K@4.2GHz with MRO 3.3.1)上測試 其實RcppArmadillo是最快的 不知道是不是Armadillo做了一些改進 因為討論中提到R只有算sigular values沒算U跟V，而Armadillo有 這造就了底層的LAPACK用的函數就不同了，所以performance會差很多 而RcppEigen因為用自帶的LAPACK比我R用的MKL慢不少，所以performance很差 再來，我也測試看看兩個套件中fastLmPure的表現 1234567891011121314151617181920212223242526n &lt;- 3e3p &lt;- 250X &lt;- matrix(rnorm(n * p), n , p)beta &lt;- rnorm(p)y &lt;- 3 + X %*% beta + rnorm(n)library(microbenchmark)microbenchmark(eigen_colPivQR = RcppEigen::fastLmPure(X, y, method = 0L), eigen_HHQR = RcppEigen::fastLmPure(X, y, method = 1L), eigen_LLT = RcppEigen::fastLmPure(X, y, method = 2L), eigen_LLDT = RcppEigen::fastLmPure(X, y, method = 3L), eigen_SVD = RcppEigen::fastLmPure(X, y, method = 4L), eigen_eig = RcppEigen::fastLmPure(X, y, method = 5L), armadillo = RcppArmadillo::fastLmPure(X, y), times = 30L)# Unit: milliseconds# expr min lq mean median uq max neval# eigen_colPivQR 77.37665 78.46618 85.91465 82.41118 89.88866 120.38286 30# eigen_HHQR 47.47491 47.83243 52.40245 48.70604 54.42752 70.18852 30# eigen_LLT 17.68025 18.07902 37.30092 18.50793 21.12671 544.98383 30# eigen_LDLT 19.52899 19.95702 22.62692 20.12774 26.57289 36.44651 30# eigen_SVD 1019.39032 1051.11184 1072.05842 1061.67417 1080.98286 1201.88356 30# eigen_eig 62.97932 63.51531 71.18810 64.34445 79.16513 99.23830 30# arma 47.26046 48.48105 50.61176 49.99042 51.71892 57.41407 30 這裡也可以看出來RcppArmadillo的performace平平，RcppEigen的llt, ldlt速度真的很快，比RcppArmadillo快上不少 可是這裡沒辦法加weights，所以我也測試了一下WLS，RcppArmadillo就快上不少，可以看一下我今天發的其他篇文章 看了兩個Benchmark之後，總結一下： RcppEigen有自己的LAPACK，在某些case下可以比RcppArmadillo快 而SVD方面的表現卻是不盡人意，非常的慢 再來是連結到我的文章，WLS的計算上RcppArmadillo快(連結，LOOCV with RcppParallel/OpenMP則是RcppEigen快(連結 所以求速度的話，使用RcppEigen基本上沒錯，只是svd的performance真的很差，這部分需要謹記(其他雷目前未知) 但是RcppArmadillo有不少方便的功能 submatrix view可以支援用uword vector拉出 element-wise, column/row-wise的操作也相較起RcppEigen來的強大 舉例來說，mat m = p.each_col() % v，其中v是vector，m, p都是矩陣 而這個在RcppEigen就要用迴圈來處理了，但是performance還是RcppEigen快一些 所以求coding方便性，希望有很多簡單的方式去處理資料，使用RcppArmadillo就好 延伸閱讀： kernal matrix computation in Rcpp kernal matrix computation in Rcpp (continued) pdate RcppEigen to 3.3.1]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>RcppArmadillo</tag>
        <tag>RcppEigen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RcppEigen Work with RcppParallel (Part 2)]]></title>
    <url>%2Fposts%2F201612%2F2016-12-31-RcppEigen-Work-With-RcppParall-part-2.html</url>
    <content type="text"><![CDATA[我在把RcppEigen跟RcppParallel合體的時候 看了下面這篇的程式： C++ code，R code 我也有發現他在R mail群裡面回答說這個程式時不時會crash 我就想到我第一部分提到的Rcpp::NumericVector會在RcppParallel會crash的問題 那我就修改了一下，發現程式就穩定了，我也把我修改後的程式放上來 (稍微改的漂亮一些) R code: 1234567891011121314151617181920Rcpp::sourceCpp("parallel_boot_lm.cpp") # could be found at http://pastebin.com/Xir4LYei set.seed(1234)N &lt;- 1e3Lp &lt;- 50LR &lt;- 500Ldmat &lt;- cbind(1, matrix(rnorm(N * p), ncol = p))beta &lt;- c(2, runif(p))yvec &lt;- as.vector(dmat %*% beta + rnorm(N, sd = 3))dall &lt;- cbind(y = yvec, as.data.frame(dmat[,-1]))myindex &lt;- matrix(sample(0:(N - 1), N * R, TRUE), ncol = R) system.time(res1 &lt;- parallelFit(dmat, yvec, myindex))# user system elapsed# 1.00 0.00 0.14system.time(res2 &lt;- apply(myindex, 2, function(i) coef(lm(y ~ ., data = dall[i+1, ]))))# user system elapsed# 4.59 0.00 4.59 stopifnot(all(abs(res1 - res2) &lt; 1e-8)) C++ code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// [[Rcpp::depends(RcppParallel)]]// [[Rcpp::depends(RcppEigen)]]#include &lt;Rcpp.h&gt;#include &lt;RcppEigen.h&gt;#include &lt;RcppParallel.h&gt; using namespace std;using namespace Rcpp;using namespace RcppParallel; using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;using Eigen::MatrixXi;using Eigen::VectorXi; template&lt;typename Derived&gt;inline MatrixXd matIdx(const Eigen::MatrixBase&lt;Derived&gt;&amp; X, const VectorXi&amp; index)&#123; MatrixXd X2(index.size(), X.cols()); for (unsigned int i = 0; i &lt; index.size(); ++i) X2.row(i) = X.row(index(i)); return X2;&#125; struct CVLM : public Worker&#123; // design matrix and outcome const MatrixXd&amp; X; const VectorXd&amp; y; const MatrixXi&amp; index; // output MatrixXd&amp; betamat; // initialize with input and output CVLM(const MatrixXd&amp; X, const VectorXd&amp; y, const MatrixXi&amp; index, MatrixXd&amp; betamat) : X(X), y(y), index(index), betamat(betamat) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for(unsigned int i = begin; i &lt; end; ++i) &#123; MatrixXd Xi = matIdx(X, index.col(i)); VectorXd yi = matIdx(y, index.col(i)); betamat.col(i) = Xi.colPivHouseholderQr().solve(yi); &#125; &#125;&#125;; // [[Rcpp::export]]Eigen::MatrixXd parallelFit(Rcpp::NumericMatrix xr, Rcpp::NumericVector dvr, Rcpp::IntegerMatrix indexr) &#123; MatrixXd x = MatrixXd::Map(xr.begin(), xr.nrow(), xr.ncol()); VectorXd dv = VectorXd::Map(dvr.begin(), dvr.size()); MatrixXi index = MatrixXi::Map(indexr.begin(), indexr.nrow(), indexr.ncol()); // allocate the output matrix MatrixXd betamat = MatrixXd::Zero(x.cols(), index.cols()); // pass input and output CVLM cvLM(x, dv, index, betamat); // parallelFor to do it parallelFor(0, index.cols(), cvLM); return betamat;&#125;]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>RcppParallel</tag>
        <tag>RcppEigen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Locally quantile regression in R]]></title>
    <url>%2Fposts%2F201612%2F2016-12-10-locally-quantile-regression-in-R.html</url>
    <content type="text"><![CDATA[最近稍微找了一下locally quantile regression的資訊 結果發現Quantile LOESS這篇可以得到比quantreg更好的效果 不過這方法沒有直覺的方式去選擇bandwidth，所以我就自己加了一點東西進去 結合我開發套件中的rfda(rfda可以在這裡找到)裡面的locPoly1d 就可以很自然使用bandwidth去調整線的平滑程度了，程式如下： 1234567891011121314151617181920212223242526272829303132333435363738data(airquality)library(quantreg)library(ggplot2)library(data.table)# source Quantile LOESSsource("https://www.r-statistics.com/wp-content/uploads/2010/04/Quantile.loess_.r.txt")airquality2 &lt;- na.omit(airquality[ , c(1, 4)])# quantreg::rqrq_fit &lt;- rq(Ozone ~ Temp, 0.95, airquality2)rq_fit_df &lt;- data.table(t(coef(rq_fit)))names(rq_fit_df) &lt;- c("intercept", "slope")# quantreg::lprqlprq_fit &lt;- lapply(1:3, function(bw)&#123; fit &lt;- lprq(airquality2$Temp, airquality2$Ozone, h = bw, tau = 0.95) return(data.table(x = fit$xx, y = fit$fv, bw = paste0("bw=", bw), fit = "quantreg::lprq"))&#125;)# Quantile LOESSql_fit &lt;- Quantile.loess(airquality2$Ozone, jitter(airquality2$Temp), window.size = 10, the.quant = .95, window.alignment = c("center"))ql_fit_df &lt;- data.table(x = ql_fit$x, y = ql_fit$y.loess, bw = "bw=1", fit = "Quantile LOESS")# rfda::locQuantPoly1dxout &lt;- seq(min(airquality2$Temp), max(airquality2$Temp), length.out = 30)locQuantPoly1d_fit &lt;- lapply(1:3, function(bw)&#123; fit &lt;- rfda::locQuantPoly1d(bw, 0.95, airquality2$Temp, airquality2$Ozone, rep(1, length(airquality2$Temp)), xout, "gauss", 0, 1) return(data.table(x = xout, y = fit, bw = paste0("bw=", bw), fit = "rfda::locQuantPoly1d"))&#125;)graphDF &lt;- rbindlist(c(lprq_fit, list(ql_fit_df), locQuantPoly1d_fit))ggplot(airquality2, aes(Temp, Ozone)) + geom_point() + labs(title = "Predicting the 95% Ozone level according to Temperature", colour = "Methods", linetype = "Bandwidth") + geom_abline(aes(intercept = intercept, slope = slope, colour ="quantreg::rq", linetype = "bw=1"), rq_fit_df, show.legend = TRUE) + geom_line(aes(x, y, colour = fit, linetype = bw), graphDF, show.legend = TRUE) 結果圖：]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Locally Quantile Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A comparison for R parallel packages]]></title>
    <url>%2Fposts%2F201611%2F2016-11-26-R-parallel-pkgs.html</url>
    <content type="text"><![CDATA[有些程式不能全部靠RcppParallel加速 所以想說只能靠R的一些平行套件來解決 但是平行套件其實不少，那哪一個又有比較好的performance? 下面是Benchmark的R script: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091require(doParallel)library(foreach)require(snowfall)library(microbenchmark)sfInit(parallel = TRUE, cpus = 6L)cl &lt;- sfGetCluster()registerDoParallel(cl)a &lt;- rnorm(1e3)b &lt;- rnorm(1e4)d &lt;- 0.5e &lt;- rnorm(1e5) # unused variables# f is not a good method to get that result, it is just for benchmarkf &lt;- function(x) &#123; sum &lt;- 0 for (i in seq(1, x)) sum &lt;- sum + (mean(a) - mean(b))*d*i return(sum)&#125;sfExport("a", "b", "d")clusterExport(cl, c("a", "b", "d"))g1 &lt;- function(x) &#123; out1 &lt;- vector("numeric", length = 100) for (i in 1:1000) out1[[i]] &lt;- f(i) return(out1)&#125;g2 &lt;- function(x) &#123; out2 &lt;- sapply(1:1000, f) return(out2)&#125;g3 &lt;- function(x) &#123; out3 &lt;- sfSapply(1:1000, f) return(out3)&#125;g4 &lt;- function(x) &#123; out4 &lt;- parSapply(cl, 1:1000, f) return(out4)&#125;g5 &lt;- function(x) &#123; out5 &lt;- foreach(i = 1:1000, .combine = c, .export = "f") %dopar% f(i) return(out5)&#125;microbenchmark(g1(), g2(), g3(), g4(), g5(), times = 20L)# Unit: seconds# expr min lq mean median uq max neval# g1() 12.197837 12.289874 12.364563 12.351551 12.460248 12.549038 20# g2() 12.149057 12.213601 12.274867 12.262235 12.318187 12.548602 20# g3() 3.846017 3.983526 4.065733 4.047937 4.075122 4.530810 20# g4() 3.933617 3.973013 4.030316 4.016203 4.074077 4.224589 20# g5() 2.764633 2.800048 2.861919 2.845468 2.914351 3.008172 20# find the main differencelibrary(profvis)profvis(g3()) # using clusterApplyprofvis(g5()) # using clusterApplyLB# clusterApplyLB is a load balancing version of clusterApply. If the length p of seq is not greater # than the number of nodes n, then a job is sent to p nodes. Otherwise the first n jobs are placed # in order on the n nodes. When the first job completes, the next job is placed on the node that # has become free; this continues until all jobs are complete. Using clusterApplyLB can result in # better cluster utilization than using clusterApply, but increased communication can reduce performance. # Furthermore, the node that executes a particular job is non-deterministic.g6 &lt;- function(x) &#123; out6 &lt;- clusterApplyLB(cl, 1:1000, f) return(out6)&#125;g7 &lt;- function(x) &#123; out7 &lt;- parSapplyLB(cl, 1:1000, f) return(out7)&#125;library(plyr)g8 &lt;- function(x) &#123; out8 &lt;- laply(1:1000, f, .parallel = TRUE) return(out8)&#125;microbenchmark(g5(), g6(), g7(), g8(), times = 20L)# Unit: seconds# expr min lq mean median uq max neval# g5() 2.811597 2.840651 2.899999 2.860215 2.925780 3.193207 20# g6() 2.627146 2.640597 2.692733 2.680902 2.734413 2.816154 20# g7() 3.949628 3.978429 4.045335 4.029595 4.068357 4.384041 20# g8() 2.866590 2.883808 2.927684 2.909651 2.957875 3.070483 20sfStop()rm(cl) 結論，如果要用比較底層的parallel function 考慮使用有loading balancing的函數可能會gain到比較多平行的效果 但是也要考慮communication帶來的 因此，在某些場合下，寫起來可能就不是那麼方便了 不過也非一定要學foreach這個套件 基本上，Hadley的plyr已經把foreach直接包在裡面 只要register一個parallel cluster (or multicore) (multicore這個套件只能用在linux上) 並在*ply的函數後面加上.parallel = TRUE就可以直接享受foreach幫你自動調整的速度了 不過我手上沒有loading balancing帶來比較差效能的例子，未來遇上再補上 額外補充，foreach有可能遇到的雷： 如果input的長度是不一定的，有可能是1的話，會帶來一些麻煩 當output是向量的時候，foreach的.combine使用cbind 會導致長度1的時候輸出的不是matrix，而是vector 在使用foreach時，這一點要特別注意 12345678910111213# without parallelf2 &lt;- function(x) rnorm(5)o1 &lt;- foreach(i = 1:2, .combine = cbind) %do% f2(i)o2 &lt;- sapply(cl, 1:2, f2)o3 &lt;- foreach(i = 1, .combine = cbind) %do% f2(i)o4 &lt;- sapply(cl, 1, f2)class(o1) # "matrix"class(o2) # "matrix"class(o3) # "matrix"class(o4) # "numeric" 2016/11/28補充： 後來發現一個整合還不錯的套件 - parallelMap 1234567891011121314151617181920212223242526require2(parallelMap)parallelStart("socket", 6L)a &lt;- rnorm(1e3)b &lt;- rnorm(1e4)d &lt;- 0.5e &lt;- rnorm(1e5) # unused variables# f is not a good method to get that result, it is just for benchmarkf &lt;- function(x) &#123; sum &lt;- 0 for (i in seq(1, x)) sum &lt;- sum + (mean(a) - mean(b))*d*i return(sum)&#125;parallelExport("a", "b", "d")g9 &lt;- function(x) &#123; out9 &lt;- parallelSapply(1:1000, f) return(out9)&#125;library(microbenchmark)microbenchmark(g9(), times = 20L)# Unit: seconds# expr min lq mean median uq max neval# g9() 2.921699 2.931741 3.002052 2.941502 3.018 3.317669 20parallelStop() 表現也跟前面用loading balancing的函數差不多]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>parallel</tag>
        <tag>snowfall</tag>
        <tag>foreach</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[plyr::ddply vs data.table::rbindlist]]></title>
    <url>%2Fposts%2F201611%2F2016-11-05-ddply-vs-rbindlist.html</url>
    <content type="text"><![CDATA[最近遇到在計算functinoal data的cross-covariance surface的時候 發現plyr::ddply裡面的list_to_dataframe有點慢 反而利用plyr::dlply加上data.table的rbindlist可以快上不少 而且plyr::ddply消耗的記憶體相對起rbindlist高上不少 會發現這些都要感謝rstudio新出的套件profvis提供了良好的performance視覺化 其中profvis可以在github找到 下面是Benchmark的R script: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586library(plyr)library(data.table)library(pipeR)N &lt;- 2000nt &lt;- 100p &lt;- 4dataDT &lt;- data.table(subId = rep(1:(N/nt), p, each = nt), variable = rep(1:p, each = N), timePnt = rep(seq(0, 10, length.out = nt), p*N/nt), value = rnorm(N*p))getRawCrCov1 &lt;- function(demeanDataDT)&#123; # geneerate the all combinations of t1,t2 and varaibles baseDT &lt;- demeanDataDT[ , .(t1 = rep(timePnt, length(timePnt)), t2 = rep(timePnt, each=length(timePnt)), value.var1 = rep(value, length(timePnt))), by = .(variable, subId)] # calculation of raw cross-covariance rawCrCovDT &lt;- do.call("dlply", list(demeanDataDT, "variable", function(df)&#123; merge(baseDT[variable &gt;= df$variable[1]], df, suffixes = c("1", "2"), by.x = c("subId", "t2"), by.y = c("subId", "timePnt")) &#125;)) %&gt;&gt;% rbindlist %&gt;&gt;% setnames("value", "value.var2") %&gt;&gt;% `[`(j = .(sse = sum(value.var1 * value.var2), cnt = .N), by = .(variable1, variable2, t1, t2)) %&gt;&gt;% setorder(variable1, variable2, t1, t2) %&gt;&gt;% `[`(j = weight := 1) return(rawCrCovDT)&#125;getRawCrCov2 &lt;- function(demeanDataDT)&#123; # geneerate the all combinations of t1,t2 and varaibles baseDT &lt;- demeanDataDT[ , .(t1 = rep(timePnt, length(timePnt)), t2 = rep(timePnt, each=length(timePnt)), value.var1 = rep(value, length(timePnt))), by = .(variable, subId)] # calculation of raw cross-covariance rawCrCovDT &lt;- do.call("ddply", list(demeanDataDT, "variable", function(df)&#123; merge(baseDT[variable &gt;= df$variable[1]], df, suffixes = c("1", "2"), by.x = c("subId", "t2"), by.y = c("subId", "timePnt")) &#125;)) %&gt;&gt;% setDT %&gt;&gt;% `[`(j = variable := NULL) %&gt;&gt;% setnames("value", "value.var2") %&gt;&gt;% `[`(j = .(sse = sum(value.var1 * value.var2), cnt = .N), by = .(variable1, variable2, t1, t2)) %&gt;&gt;% setorder(variable1, variable2, t1, t2) %&gt;&gt;% `[`(j = weight := 1) return(rawCrCovDT)&#125;getRawCrCov3 &lt;- function(demeanDataDT)&#123; # geneerate the all combinations of t1,t2 and varaibles baseDT &lt;- demeanDataDT[ , .(t1 = rep(timePnt, length(timePnt)), t2 = rep(timePnt, each=length(timePnt)), value.var1 = rep(value, length(timePnt))), by = .(variable, subId)] # calculation of raw cross-covariance rawCrCovDT &lt;- do.call("llply", list(split(demeanDataDT, by = "variable"), function(df)&#123; merge(baseDT[variable &gt;= df$variable[1]], df, suffixes = c("1", "2"), by.x = c("subId", "t2"), by.y = c("subId", "timePnt")) &#125;)) %&gt;&gt;% rbindlist %&gt;&gt;% setnames("value", "value.var2") %&gt;&gt;% `[`(j = .(sse = sum(value.var1 * value.var2), cnt = .N), by = .(variable1, variable2, t1, t2)) %&gt;&gt;% setorder(variable1, variable2, t1, t2) %&gt;&gt;% `[`(j = weight := 1) return(rawCrCovDT)&#125;getRawCrCov4 &lt;- function(demeanDataDT)&#123; # geneerate the all combinations of t1,t2 and varaibles baseDT &lt;- demeanDataDT[ , .(t1 = rep(timePnt, length(timePnt)), t2 = rep(timePnt, each=length(timePnt)), value.var1 = rep(value, length(timePnt))), by = .(variable, subId)] # set the keys of data.table setkey(baseDT, "subId", "t2") setkey(demeanDataDT, "subId", "timePnt") # calculation of raw cross-covariance rawCrCovDT &lt;- do.call("llply", list(split(demeanDataDT, by = "variable"), function(df)&#123; merge(baseDT[variable &gt;= df$variable[1]], df, suffixes = c("1", "2"), by.x = c("subId", "t2"), by.y = c("subId", "timePnt")) &#125;)) %&gt;&gt;% rbindlist %&gt;&gt;% setnames("value", "value.var2") %&gt;&gt;% `[`(j = .(sse = sum(value.var1 * value.var2), cnt = .N), by = .(variable1, variable2, t1, t2)) %&gt;&gt;% setorder(variable1, variable2, t1, t2) %&gt;&gt;% `[`(j = weight := 1) return(rawCrCovDT)&#125;x1 &lt;- getRawCrCov1(dataDT)x2 &lt;- getRawCrCov2(dataDT)x3 &lt;- getRawCrCov3(dataDT)x4 &lt;- getRawCrCov4(dataDT)all.equal(x1, x2) # TRUEall.equal(x1, x3) # TRUEall.equal(x1, x4) # TRUElibrary(microbenchmark)microbenchmark(rbindlist = getRawCrCov1(dataDT), ddply = getRawCrCov2(dataDT), split.DT = getRawCrCov3(dataDT), setkey.first = getRawCrCov4(dataDT), times = 50L)# Unit: milliseconds# expr min lq mean median uq max neval# rbindlist 657.1571 700.8676 726.3146 716.2397 741.4616 911.0223 50# ddply 2951.0253 3196.2281 3292.1848 3283.4185 3415.7735 3638.6447 50# split.DT 653.3183 699.5276 732.4667 720.5936 747.7091 1016.1446 50# setkey.first 496.7661 542.6954 562.4172 554.0295 584.7101 701.3526 50 速度整整差了近5倍(3347 / 709 ~= 4.72) 因此，建議以後plyr系列，盡量避開*dply系列的函數 用到plyr:::list_to_dataframe這個函數的效能都不好 盡量去使用data.table::rbindlist 用dlply + rbindlist跟用split.data.table + llply + rbindlist 其實最後兩者速度差不多，時間並沒有太大的區別 (726ms vs 732ms) 其實我覺得可以直接都統一用split.data.table + llply + rbindlist 統一減少使用*dply或是d*ply 而在迴圈中做merge前，建議全部都先setkey 這樣拆分之後的data.table，還是有key merge時就不用再建key了 而這一點也是透過profvis這個套件發現 多看幾次profvis套件出來的結果，可以對每個程式怎麼運作更加了解 就能因時因地制宜了，程式自然會更有效率]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>plyr</tag>
        <tag>list</tag>
        <tag>data.frame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab Server與RStudio結合管理程式碼]]></title>
    <url>%2Fposts%2F201610%2F2016-10-28-gitlab-with-rstudio.html</url>
    <content type="text"><![CDATA[因為不想要用公開的程式碼管理 想要用local server做管理 又想要用類似github的功能 所以找了一下，發現gitlab又提供類似功能 而且RStudio可以直接使用 安裝的話，直接參考：https://about.gitlab.com/downloads/#centos7 那安裝完之後，可以先連去 http://&lt;伺服器IP&gt; 一開始會先讓你改密碼，然後你就可以自己create一個新帳號了 進去會看到一片空白，上面有個狐狸頭 我們可以先新創一個專案，叫做my-first-project 這裡創專案可以用Group或是Individual的型式來建立，我們這用Individual來建立，如下圖： 先在本機端裝好Git(官網)，看是要下載Portable還是安裝版皆可 再來就可以打開RStudio了 先進到Tools-&gt;Global Options...裡面的’Git/SVN的分頁，點選Create RSA Key…` 建立之後，可以到使用者資料夾下的.ssh裡面看到自己的SSH key 在GitLab上設定SSH Key，之後Commit就不用輸入帳號密碼了 GitLab的設定在你登入後點右上方小圖案進去的Profile Settings 裡面有一個SSH Keys的分頁，把你的id_ras.pub裡面 那串ssh-rsa開頭的文字貼到Key，並給個名字，然後Add即可 最後，就是在本地端開一個資料夾 根據建立好的專案後面的指令做一次Clone，如下圖： PS: 我這裡把Hostname都換成IP了，因為我沒有設定Hostname… 最後，就可以在Rstudio開新的專案來使用Git管理專案，流程如下： 創好專案之後，切換到Git分頁，點Commit，就會跳出下面視窗 你可以勾選左邊有變更的檔案，然後輸入你要Commit的訊息就可以留下一個record了 結束之後按下Push就會成功上傳到GitLab上了 去網頁就可以看到你上傳的檔案 PS: 建議不要用New Project裡面的Version Control會認不到，要去更改remote.url的設定]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>GitLab</tag>
        <tag>Git</tag>
        <tag>private</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of OpenCPU in centos]]></title>
    <url>%2Fposts%2F201610%2F2016-10-25-installation-of-opencpu-in-centos.html</url>
    <content type="text"><![CDATA[上次只是用R的opencpu套件小試一下 這次就直接在server上建立opencpu server 讓local的R可以去call server服務 12345678910111213141516171819202122232425262728# install Microsoft R Opensu -c 'rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm'sudo yum updatesudo yum install gcc-c++ R R-java libapreq2-devel R-devel libcurl-devel protobuf-devel openssl-devel libxml2-devel libicu-devel libssh2-devel## remove Rsudo rm -rf /usr/lib64/R# install Microsoft R Opencurl -v -j -k -L https://mran.microsoft.com/install/mro/3.3.1/microsoft-r-open-3.3.1.tar.gz -o microsoft-r-open-3.3.1.tar.gztar zxvf microsoft-r-open-3.3.1.tar.gzsudo yum install -y microsoft-r-open/rpm/*sudo chmod -R 777 /usr/lib64/microsoft-r/3.3/lib64/R # to use Microsoft R Open with opencpusudo cp -r /usr/lib64/microsoft-r/3.3/lib64/R /usr/lib64# install gitsudo yum install git# install opencpugit clone https://github.com/jeroenooms/opencpu-server.gitchmod +rx ~/chmod +x opencpu-server/rpm/*.shopencpu-server/rpm/buildscript.sh# 最後會在rpmbuild/RPMS/x86_64/看到編譯好的rpm 最後使用下面指令開port就可以順利在網頁上登入http://&lt;your_ip_address&gt;/ocpu/test 12sudo iptables -I INPUT -p tcp --dport 80 -j ACCEPTsudo service iptables save]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>CentOS</tag>
        <tag>Web Service</tag>
        <tag>opencpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R web service]]></title>
    <url>%2Fposts%2F201610%2F2016-10-20-R-web-service.html</url>
    <content type="text"><![CDATA[R也可以開啟一個簡單的server 讓使用者透過PUT或是GET去query想要的資訊 以下是實作 使用RStudio的httpuv這個套件來做 先用一個給什麼Query都回成JSON的app 12345678910111213141516171819202122232425262728library(httpuv)library(stringr)library(RCurl)library(httr)library(pipeR)app &lt;- list(call = function(request) &#123; query &lt;- request$QUERY_STRING %&gt;&gt;% str_replace_all("^\\?", "") output &lt;- httr:::parse_query(query) %&gt;&gt;% lapply(URLdecode) %&gt;&gt;% `names&lt;-`(sapply(names(.), URLdecode)) %&gt;&gt;% toJSON if (query == "" || !is.character(query)) &#123; return(list(status = 200L, headers = list('Content-Type' = 'text/json'), body = "")) &#125; else &#123; return(list(status = 200L, headers = list('Content-Type' = 'text/json'), body = output)) &#125;&#125;)openServer &lt;- function(app)&#123; tryCatch(&#123; server &lt;- startServer("127.0.0.1", 8988, app=app) on.exit(stopServer(server)) while(TRUE) &#123; service() Sys.sleep(0.001) &#125; &#125;)&#125;openServer(app) 在一個R跑上面這個script，然後再開一個R去跑下面的script就可以看到成果： 1234567library(httr)library(pipeR)"http://127.0.0.1:8988/?hello=world&amp;hello2=celest" %&gt;&gt;% URLencode %&gt;&gt;% GET %&gt;&gt;% content("text")# text_content() deprecated. Use content(x, as = 'text')# No encoding supplied: defaulting to UTF-8.# [1] "&#123;\"hello\":[\"world\"],\"hello2\":[\"celest\"]&#125;" 下面給一個或許可以用來拉資料的方案XD 12345678910111213141516171819202122232425262728app2 &lt;- list(call = function(request) &#123; query &lt;- request$QUERY_STRING %&gt;&gt;% str_replace_all("^\\?", "") if (str_detect(query, "data=")) &#123; dataName &lt;- httr:::parse_query(query) %&gt;&gt;% `[[`("data") %&gt;&gt;% URLdecode &#125; if (query == "" || !is.character(query)) &#123; return(list(status = 200L, headers = list('Content-Type' = 'text/json'), body = "")) &#125; else &#123; return(list(status = 200L, headers = list('Content-Type' = 'text/json'), body = toJSON(get(dataName)))) &#125;&#125;)openServer(app2)# test codelibrary(httr)library(pipeR)library(jsonlite)paste0("http://127.0.0.1:8988/?data=", "iris") %&gt;&gt;% GET %&gt;&gt;% content("text") %&gt;&gt;% fromJSON %&gt;&gt;% head# Sepal.Length Sepal.Width Petal.Length Petal.Width Species# 1 5.1 3.5 1.4 0.2 setosa# 2 4.9 3.0 1.4 0.2 setosa# 3 4.7 3.2 1.3 0.2 setosa# 4 4.6 3.1 1.5 0.2 setosa# 5 5.0 3.6 1.4 0.2 setosa# 6 5.4 3.9 1.7 0.4 setosa]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>CentOS</tag>
        <tag>httpuv</tag>
        <tag>Web Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R web service with opencpu]]></title>
    <url>%2Fposts%2F201610%2F2016-10-21-R-web-service-with-opencpu.html</url>
    <content type="text"><![CDATA[昨天後來再google發現opencpu這個更強大的套件 昨天太累就沒試了XD 我把前幾天弄得unnest函數在配上opencpu 就可以輕鬆做到unnested json的轉換了 先列參考來源： Taiwan R User Group - Opencpu sharing 其實投影片講的非常清楚 照著做就可以出來只是路徑要看清楚XDD (因為路徑少打R/試了超久) 首先，安裝opencpu，然後安裝devtools 123456789# 安裝我的套件devtools::install_github("ChingChuan-Chen/test.opencpu")# 啟動opencpulibrary(opencpu) # library之後會出現下面的訊息，http://localhost:8850/ocpu 就是你可以連過去做測試的網址# Initiating OpenCPU server...# Using config: C:/Users/Celest/Documents/.opencpu.conf# OpenCPU started.# [httpuv] http://localhost:8850/ocpu# OpenCPU single-user server ready. 我的測試檔案：GitHub 原始資料長相： opencpu web UI操作POST： 回傳的結果： 但是這樣還不夠完美，所以可以透過回傳的內容有連結資訊 (由上角小框框裡面是POST回傳的結果) 來用R來取得，其程式碼就會像下面這樣： 12345678910111213141516171819202122232425library(pipeR)library(httr)library(stringr)library(jsonlite)POST("http://localhost:8850/ocpu/library/test.opencpu/R/convertJSON", body = list(input = upload_file("funcdata.json")) %&gt;&gt;% content("text") %&gt;&gt;% str_split("\r\n") %&gt;&gt;% `[[`(1) %&gt;&gt;% `[`(1) %&gt;&gt;% str_c("/json") %&gt;&gt;% sprintf(fmt="http://localhost:8850%s") %&gt;&gt;% GET %&gt;&gt;% content("text") %&gt;&gt;% fromJSON# sampleID t y# 1 1 9.37 -0.0515# 2 1 4.06 -0.3743# 3 1 8.92 1.2116# 4 1 8.83 0.4139# 5 1 1.27 1.7179# 6 1 5.86 -1.3720# 7 1 4.69 -2.4096# 8 1 9.55 -0.7041# 9 1 2.62 1.9196# 10 1 7.93 1.5362# 11 1 4.42 -0.1172# 12 1 8.38 0.1368# 13 1 9.19 -0.9449# 14 1 4.33 -1.5905# 15 1 9.73 -0.3867 最後放一張完成圖：]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>CentOS</tag>
        <tag>Web Service</tag>
        <tag>opencpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unnest in R data.table]]></title>
    <url>%2Fposts%2F201610%2F2016-10-19-unnest-in-R-datatable.html</url>
    <content type="text"><![CDATA[我們今天可能會遇到json parse出來的資料長下面這樣 1234567library(data.table)DT &lt;- data.table(a = list(c(1:5), c(2:4), c(1:5)), b = 1:3, c = list(c(0:4), c(6:8), c(7:11)), d = 2:4)# a b c d# 1: 1,2,3,4,5 1 0,1,2,3,4 2# 2: 2,3,4 2 6,7,8 3# 3: 1,2,3,4,5 3 7, 8, 9,10,11 4 那在這種情況下，可以直接選擇用tidyr的unnest去做，如下面所示 12345678910111213141516library(tidyr)unnest(DT, a, c)# b d a c# 1: 1 2 1 0# 2: 1 2 2 1# 3: 1 2 3 2# 4: 1 2 4 3# 5: 1 2 5 4# 6: 2 3 2 6# 7: 2 3 3 7# 8: 2 3 4 8# 9: 3 4 1 7# 10: 3 4 2 8# 11: 3 4 3 9# 12: 3 4 4 10# 13: 3 4 5 11 但是這時候我們很難的去自動解析這種表格，必須讓使用者自行處理 (PS: 其實這裡可以直接用unnest_ + 下面的autoFind裡面的一部分就可以很輕易做到，下面會實做) 所以如果我們能用簡單的方式去自動辨別需要轉換就更好了 基於此，我就用data.table去開發了這樣想的程式，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556extendTbl &lt;- function(DT, unnestCols = NULL)&#123; # check the columns to unnest if (is.null(unnestCols)) &#123; unnestCols &lt;- names(DT)[sapply(DT, function(x) any(class(x) %in% "list"))] message("Automatically recognize the nested columns: ", paste0(unnestCols, collapse = ", ")) &#125; # check unnestCols is in the DT if (any(!unnestCols %in% names(DT))) stop(sprintf("The columns, %s, does not in the DT.", paste0(unnestCols[!unnestCols %in% names(DT)], collapse = ", "))) # get the group by variable groupbyVar &lt;- setdiff(names(DT), unnestCols) # generate the expression to remove group by variable chkExpr &lt;- paste0(groupbyVar, "=NULL", collapse = ",") %&gt;&gt;% (paste0("`:=`(", ., ")")) # check the lengths of each cell in list-column are all the same chkLenAllEqual &lt;- DT[ , lapply(.SD, function(x) sapply(x, length)), by = groupbyVar] %&gt;&gt;% `[`(j = eval(parse(text = chkExpr))) %&gt;&gt;% as.matrix %&gt;&gt;% apply(1, diff) %&gt;&gt;% `==`(0) %&gt;&gt;% all if (!chkLenAllEqual) stop("The length in each cell is not equal.") # generate unnest expression expr &lt;- unnestCols %&gt;&gt;% (paste0(., "=unlist(", ., ")")) %&gt;&gt;% paste0(collapse = ",") %&gt;&gt;% (paste0(".(", ., ")")) # return unnested data.table return(DT[ , eval(parse(text = expr)), by = groupbyVar])&#125;extendTbl(DT)# b d a c# 1: 1 2 1 0# 2: 1 2 2 1# 3: 1 2 3 2# 4: 1 2 4 3# 5: 1 2 5 4# 6: 2 3 2 6# 7: 2 3 3 7# 8: 2 3 4 8# 9: 3 4 1 7# 10: 3 4 2 8# 11: 3 4 3 9# 12: 3 4 4 10# 13: 3 4 5 11extendTbl(DT, c("b", "d"))# b d a c# 1: 1 2 1 0# 2: 1 2 2 1# 3: 1 2 3 2# 4: 1 2 4 3# 5: 1 2 5 4# 6: 2 3 2 6# 7: 2 3 3 7# 8: 2 3 4 8# 9: 3 4 1 7# 10: 3 4 2 8# 11: 3 4 3 9# 12: 3 4 4 10# 13: 3 4 5 11 我們可以來比較一下tidyr跟data.table的速度 會要求速度最主要的原因是因為我遇到的資料都非常大 不用快一點的方法，真的會等很久 123456789101112131415161718192021222324library(microbenchmark)N &lt;- 1e5dbColsCnt &lt;- 300arrColCnt &lt;- 50sizePerRow &lt;- sapply(1:N, function(x) sample(2:25, 1))arrCols &lt;- replicate(arrColCnt, lapply(sizePerRow, rnorm)) %&gt;&gt;% data.table %&gt;&gt;% setnames(paste0("V", (dbColsCnt+1):(dbColsCnt+arrColCnt)))DT &lt;- data.table(matrix(rnorm(dbColsCnt*N), N), arrCols)autoFind_unnest &lt;- function(DT)&#123; names(DT)[sapply(DT, function(x) any(class(x) %in% "list"))]&#125;microbenchmark(unnest = unnest_(DT, autoFind_unnest(DT)), datatable = extendTbl(DT), times = 20L)autoFind_unnest &lt;- function(DT)&#123; names(DT)[sapply(DT, function(x) any(class(x) %in% "list"))]&#125;microbenchmark(unnest = unnest_(DT, autoFind_unnest(DT)), datatable = extendTbl(DT), times = 20L)# Unit: seconds# expr min lq mean median uq max neval# unnest 15.806110 16.29989 16.62312 16.75588 16.88089 17.4564 20# datatable 9.362995 10.25902 10.82319 10.45128 10.94098 14.0597 20]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>unnest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用R做Oracle跟mongodb的loader]]></title>
    <url>%2Fposts%2F201610%2F2016-10-15-using-R-to-load-data-into-mongodb-from-oracle.html</url>
    <content type="text"><![CDATA[這篇是用R做一個loader，把Oracle資料倒去Mongodb中 並且使用設定去將特定column改成list的方式做儲存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195library(ROracle)library(stringr)library(data.table)library(pipeR)library(mongolite)Sys.setenv(TZ = "Asia/Taipei", ORA_SDTZ = "Asia/Taipei")numRows &lt;- 5e7host &lt;- "192.168.0.120"port &lt;- 1521sid &lt;- "orcl"connectString &lt;- paste0("(DESCRIPTION=", "(ADDRESS=(PROTOCOL=TCP)(HOST=", host, ")(PORT=", port, "))", "(CONNECT_DATA=(SID=", sid, ")))")con &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString)# first commitdiff_seconds_90days &lt;- difftime(Sys.time(), Sys.Date()-90, units = "secs") %&gt;&gt;% as.integersizePerGroup &lt;- sample(11:2500, ceiling(numRows / mean(1:2500)) * 1.2, TRUE) %&gt;&gt;% `[`(. &gt; 10) %&gt;&gt;% `[`(1:which(cumsum(.) &gt;= numRows)[1])sizePerGroup[length(sizePerGroup)] &lt;- sizePerGroup[length(sizePerGroup)] + numRows - sum(sizePerGroup)var2 &lt;- sprintf("A%05i", 1:length(sizePerGroup))var3 &lt;- sprintf("B%03i", 1:800)dat &lt;- mapply(function(ss, v2value)&#123; as.data.table(list(t = Sys.time() - sample(diff_seconds_90days, 1, TRUE), v2 = v2value, v3 = sample(var3, ss, TRUE), v4 = rnorm(ss)))&#125;, sizePerGroup, var2, SIMPLIFY = FALSE) %&gt;&gt;% rbindlist %&gt;&gt;% setnames(toupper(names(.))) # 記得全部欄位都要大寫st &lt;- proc.time()# write data to OracledbWriteTable(con, "TEST_BIG_TABLE", dat)proc.time() - st# user system elapsed # 29.97 3.83 1138.38## 需要19分鐘才能夠把資料都上傳完畢# check data sizecntDF &lt;- dbGetQuery(con, "SELECT COUNT(*) ROWCNT FROM TEST_BIG_TABLE")cat(sprintf("%8i", cntDF$ROWCNT))# 50000000## 確定是五千萬筆dbDisconnect(con)rm(dat, var2, var3, sizePerGroup)gc()## 從oracle到mongodb# 這裡上傳的想法是根據t, v3拆成小小的資料集來做# 資料會取得v3部分子集(這裡設定一次200個)，t一個# 舉例來說，v3會取得B001-B200，t1選2016-07-18# 我就從Oracle拿出v3 in B001-B200的跟t是介在2016-07-18 00:00:00到2016-07-18 23:59:59的資料# 然後根據t, v2的組合(一個t只會對應一個v2)去把v3,v4合併成一個array，並產生新的column# 合併出來的一筆資料就會像：# t: '2016-08-19 08:56:13'# v2: 'A0001'# v3: ['B001', 'B096', ...]# v4: [-0.972354, 0.456785, ...]# 再把這樣的資料傳到mongodb就大功告成了## 取得分批的參數paras &lt;- list(sprintf("B%03i", 1:800), seq.Date(Sys.Date()-90, Sys.Date(), "days") %&gt;&gt;% format("'%Y-%m-%d %H:%M:%S'"))parasName &lt;- paste0(":para", 1:length(paras))# 分批的sizebatchSize &lt;- 200# group by 的變數groupbyVars &lt;- "t,v2"# sqlsql &lt;- paste("SELECT tbl.* FROM TEST_BIG_TABLE tbl WHERE tbl.v3 in :para1", "AND tbl.t &gt;= to_date(:para2, 'YYYY-MM-DD HH24:MI:SS')", "AND tbl.t &lt; to_date(:para2, 'YYYY-MM-DD HH24:MI:SS') + 1", sep = "\n")# check sql is validstopifnot(all(unique(str_extract_all(sql, ":para\\d")[[1]]) %in% parasName))# 分割出para1 setparas1_set &lt;- split(paras[[1]], rep(1:(ceiling(length(paras[[1]]) / batchSize)), each = batchSize, length = length(paras[[1]]))) %&gt;&gt;% sapply(function(set) paste0("('", paste0(set, collapse = "','"), "')"))# control the upload progressloc &lt;- rep(1, length(paras))st &lt;- proc.time()contUpload &lt;- TRUEwhile(contUpload)&#123; message(sprintf("Now loc is %s ...", paste0(loc, collapse = ","))) # generate sql to query the subset sql_implement &lt;- sql for (i in 2:length(paras)) sql_implement &lt;- str_replace_all(sql_implement, paste0(":para", i), paras[[i]][loc[i]]) sql_implement &lt;- str_replace_all(sql_implement, ":para1", paras1_set[loc[1]]) # query data con &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString) oraDT &lt;- dbGetQuery(con, sql_implement) %&gt;&gt;% data.table %&gt;&gt;% setnames(tolower(names(.))) dbDisconnect(con) if (nrow(oraDT) &gt; 0)&#123; # binding variables groupbyVarsSplit &lt;- str_split(groupbyVars, ",")[[1]] bindingVars &lt;- setdiff(names(oraDT), groupbyVarsSplit) expr &lt;- paste(bindingVars, paste0("list(", bindingVars, ")"), sep = "=") %&gt;&gt;% c("numRecords=.N") %&gt;&gt;% paste(collapse = ",") %&gt;&gt;% (paste0(".(", ., ")")) mongoDT &lt;- oraDT[ , eval(parse(text = expr)), by = groupbyVars] rm(oraDT); gc() # upload data to mongo ignoreOutput &lt;- capture.output(mongoConn &lt;- mongo("test_big_table", "big_table", "mongodb://drill:drill@192.168.0.128:27017")) # check the data timeClassNames &lt;- c("Date", "POSIXct", "POSIXt", "POSIXlt") timeCol &lt;- sapply(mongoDT, function(x) any(class(x) %in% timeClassNames)) %&gt;&gt;% (names(.[.])) groupbyVars2 &lt;- setdiff(groupbyVarsSplit, timeCol) fieldStr &lt;- sprintf('&#123;"_id": 0, "numRecords": 1, %s&#125;', paste(sprintf('"%s": 1', groupbyVars2), collapse = ",")) groupbyVarSet &lt;- sapply(groupbyVars2, function(x) unique(mongoDT[[x]]) %&gt;&gt;% (paste0("\"", ., "\"")) %&gt;&gt;% paste(collapse = ",")) queryInfo &lt;- paste0("\"", groupbyVars2 ,"\": &#123;\"$in\": [", groupbyVarSet, "]&#125;") %&gt;&gt;% paste(collapse = ",") %&gt;&gt;% (paste0("&#123;", ., "&#125;")) chkDT &lt;- mongoConn$find(queryInfo, fieldStr) # filter the data not upload if (nrow(chkDT) &gt; 0) &#123; uploadDT &lt;- merge(mongoDT, chkDT, all.x = TRUE, by = groupbyVars2, suffixes = c("", ".chk")) %&gt;&gt;% `[`(numRecords != numRecords.chk | is.na(numRecords.chk)) %&gt;&gt;% `[`(j = numRecords.chk := NULL) &#125; else &#123; uploadDT &lt;- mongoDT &#125; rm(mongoDT); gc() # start to insert data numFail &lt;- 0 if (nrow(uploadDT) &gt; 0)&#123; mapply(function(i, j) paras[[i]][j], 2:length(paras), tail(loc, length(loc)-1)) %&gt;&gt;% paste(collapse = ",") %&gt;&gt;% (paste0("para1: \"", paras1_set[loc[1]], "\",", .)) %&gt;&gt;% sprintf(fmt = "Now upload data with set: %s") %&gt;&gt;% message while (numFail &lt;= 10) &#123; uploadStatus &lt;- mongoConn$insert(uploadDT) if (uploadStatus) break Sys.sleep(0.1) numFail &lt;- numFail + 1 &#125; if (numFail &gt; 10) &#123; mongoConn$drop() write(sprintf("Fail to upload the data with sql:\n%s", sql_implement), 'fail_upload_DB', append = file.exists('fail_upload_DB')) &#125; &#125; rm(uploadDT); gc() # disconnect to mongo remove(mongoConn) &#125; loc[1] &lt;- loc[1] + 1 if (loc[1] &gt; length(paras1_set))&#123; loc[1] &lt;- 1 loc[2] &lt;- loc[2] + 1 &#125; for (i in 2:length(loc))&#123; if (loc[i] &gt; length(paras[[i]]))&#123; loc[i] &lt;- 1 if (i == length(loc))&#123; contUpload &lt;- FALSE &#125; else &#123; loc[i+1] &lt;- loc[i+1] + 1 &#125; &#125; &#125;&#125;proc.time() - st# user system elapsed # 253.87 5.33 2971.89## 拆成360份，上傳大概是2.5倍的時間，50分鐘mongoConn &lt;- mongo("test_big_table", "big_table", "mongodb://drill:drill@192.168.0.128:27017")# count data sizesprintf("%6i", mongoConn$count()) # 305174## 最後資料量從5000萬降到剩下30.5萬# get data and convert to normal tableoutDT &lt;- mongoConn$find(limit = 50) %&gt;&gt;% data.tableoutDT_trans &lt;- outDT[ , .(v3 = unlist(v3), v4 = unlist(v4)), by = "t,v2"]# disconnect to mongoremove(mongoConn) 因為我的mongodb router server跟config server都只有一台 所以上傳過程中兩個server都非常忙碌 如果能有夠多機器去分擔router server跟config server應該還可以提升資料上傳速度]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>mongodb</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建置Mongodb Sharding Server]]></title>
    <url>%2Fposts%2F201610%2F2016-10-14-buildup-mongodb-sharding-server.html</url>
    <content type="text"><![CDATA[業務擴展到找尋適合的NoSQL server了 這篇來試Mongodb sharding server 基本上都是參考下面幾篇去建置的： Install MongoDB Community Edition on CentOS Linux MongoDB Replica Set in CentOS 6.x MongoDB Replica Set 高可用性架構搭建 MongoDB Sharding 分散式儲存架構建置 (概念篇) MongoDB Sharding 分散式儲存架構建置 (實作篇) github scripts to create mongodb sharding 我根據我的需求把第6點的scripts改成我要，可以在我的github找到 我的伺服器分布規劃： 1234replica set 1: 192.168.0.121 (primary), 192.168.0.122, 192.168.0.123replica set 2: 192.168.0.124 (primary), 192.168.0.125, 192.168.0.126config server: 192.168.0.127router: 192.168.0.128 但是為了fit我的分布，我做了一些修改 我刪掉config_primary.sh第82行，81行改成config={_id: &quot;crepset&quot;, configsvr: true, members: [{_id: 0, host: &quot;192.168.0.127:27019&quot;}]} router1.sh中的crepset/192.168.0.127:27019,192.168.0.128:27019,192.168.0.129:2701都取代成crepset/192.168.0.127:27019 config_secondary.sh跟router2.sh就沒跑了 接下來就直接在各台先取得root權限(用su)，然後跑./install_mongodb.sh 該台IP安裝mongodb PS: rpms可以在https://repo.mongodb.org/yum/redhat/7Server/mongodb-org/3.2/x86_64/RPMS/找到 接著在192.168.0.121跟192.168.0.124跑./replica_primary.sh set名稱 該台IP 使用者名稱 並同時在replica set secondary的電腦上跑./replica_secondary.sh` 都完成之後，使用config_primary.sh部署config server 最後再用router1.sh部署router server 到此，mongodb sharding server就部署完畢了]]></content>
      <categories>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[library多個套件，並自動安裝沒安裝的套件]]></title>
    <url>%2Fposts%2F201610%2F2016-10-04-library-R-packages-with-install-automatically.md.html</url>
    <content type="text"><![CDATA[廢話不多說，直接上code 12345678library(pipeR)library_mul &lt;- function(..., lib.loc = NULL, quietly = FALSE, warn.conflicts = TRUE)&#123; pkgs &lt;- as.list(substitute(list(...))) %&gt;&gt;% sapply(as.character) %&gt;&gt;% setdiff("list") if (any(!pkgs %in% installed.packages())) install.packeges(pkgs[!pkgs %in% installed.packages()]) sapply(pkgs, library, character.only = TRUE, lib.loc = lib.loc, quietly = quietly) %&gt;&gt;% invisible&#125;library_mul(httr, pipeR, data.table)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>library</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark and Apache Hive]]></title>
    <url>%2Fposts%2F201609%2F2016-09-26-spark-and-hive.html</url>
    <content type="text"><![CDATA[關於Spark操作Hive資料庫的一些心得 動態分割表 建議在Hive setup的時候修改這個參數： 123456789&lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;nonstrict&lt;/value&gt; &lt;description&gt; In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions. In nonstrict mode all partitions are allowed to be dynamic. &lt;/description&gt;&lt;/property&gt; 從Spark寫入Hive spark-shell mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos開啟spark-shell 123456789101112131415161718192021222324252627282930313233343536373839404142import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("spark on hive"). config("spark.sql.warehouse.dir", "hdfs://hc1/spark"). enableHiveSupport().getOrCreate() spark.sql("CREATE TABLE test_df (v1 STRING, v2 STRING, v3 DOUBLE)" + "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE")spark.sql("LOAD DATA LOCAL INPATH '/home/tester/test_df.csv' OVERWRITE INTO TABLE test_df")val hiveDF = spark.sql("select * from test_df")hiveDF.show(2)# +-------------------+---+----+# | v1| v2| v3|# +-------------------+---+----+# |-0.0170655635959667| e|null|# | 0.441907768470412| d|null|# +-------------------+---+----+# only showing top 2 rows# 試著換欄位位置val hiveDF2 = hiveDF.select($"v3", $"v2", $"v1")hiveDF2.show(2)# +----+---+-------------------+# | v3| v2| v1|# +----+---+-------------------+# |null| e|-0.0170655635959667|# |null| d| 0.441907768470412|# +----+---+-------------------+# only showing top 2 rows# 寫進test_dfhiveDF2.write.insertInto("test_df")spark.sql("select * from test_df").show(2)#+-------------------+---+----+#| v1| v2| v3|#+-------------------+---+----+#|-0.0170655635959667| e|null|#| 0.441907768470412| d|null|#+-------------------+---+----+#only showing top 2 rows 所以在寫入的時候要注意，欄位順序避免發生這種問題 利用select這個函數去調換column的位置以避免寫入問題 1234567891011121314151617181920212223242526spark.sql("drop table test_df")spark.sql("CREATE TABLE test_df (v1 STRING, v2 STRING, v3 DOUBLE)" + "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE")spark.sql("LOAD DATA LOCAL INPATH '/home/tester/test_df.csv' OVERWRITE INTO TABLE test_df")val hiveDF = spark.sql("select * from test_df")hiveDF.show(2)# +---+---+-------------------+# | v1| v2| v3|# +---+---+-------------------+# | b| e|-0.0170655635959667|# | a| d| 0.441907768470412|# +---+---+-------------------+# only showing top 2 rowsval cols_order = hiveDF.columnsval hiveDF2 = hiveDF.select($"v3", $"v2", $"v1")hiveDF2.select(cols_order.head, cols_order.tail: _*).write.insertInto("test_df")spark.sql("select * from test_df").show(2)# +---+---+-------------------+# | v1| v2| v3|# +---+---+-------------------+# | b| e|-0.0170655635959667|# | a| d| 0.441907768470412|# +---+---+-------------------+# only showing top 2 rows]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hive</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Performnace benchmark of the SQL tools on Hadoop]]></title>
    <url>%2Fposts%2F201609%2F2016-09-23-performance-benchmark-of-the-sql-tools-on-Hadoop.html</url>
    <content type="text"><![CDATA[就我手上現有的SQL on Hadoop工具 我做了一個簡單的benchmark去看效能 資料量不大，一個998177列、11欄的數據 (1欄字串、2欄double、7欄整數、一欄邏輯值) 存成json檔案(178MB), csv檔案(63MB)放置於hdfs 我們分別用HBase, Hive, Spark SQL, Drill去測試幾個簡單case 配備： VM: 2-core with 4G ram x 3 版本： Hadoop HBase Hive Drill Spark 版本 2.7.3 1.2.2 2.1.0 1.8.0 2.0.0 設備疲乏，可能只能給出一個簡單的比較 如果有更多機器，當然希望可以給出更多的比較 Hive可以使用mapreduce, tez或是Spark，儲存媒介也可以選hdfs或是hbase 但是這裡沒想測試那麼多，所以只測試mapreduce + hdfs的模式 而Spark就直接用Hive上的資料做Spark SQL 至於Drill則分別使用hdfs的json, csv檔案，以及透過HBase, Hive等方式去做 先稍微看一下資料： 123456789101112131415161718192021222324252627282930313233343536373839404142head tw5_df.csv# nfbVD-5S-9.063,30,2015,1,1,0,0,5,TRUE,row1# nfbVD-5N-9.013,11,2015,1,1,0,0,5,TRUE,row2# nfbVD-5S-9.063,30,2015,1,1,0,1,5,TRUE,row3# nfbVD-5N-9.013,11,2015,1,1,0,1,5,TRUE,row4# nfbVD-5S-9.063,18,2015,1,1,0,2,5,TRUE,row5# nfbVD-5N-9.013,5,2015,1,1,0,2,5,TRUE,row6# nfbVD-5S-9.063,24,2015,1,1,0,3,5,TRUE,row7# nfbVD-5N-9.013,6,2015,1,1,0,3,5,TRUE,row8# nfbVD-5S-9.063,24,2015,1,1,0,4,5,TRUE,row9# nfbVD-5N-9.013,6,2015,1,1,0,4,5,TRUE,row10head tw5_df_hbase.csv# nfbVD-5S-9.063,30,2015,1,1,0,0,5,TRUE,row0000001# nfbVD-5N-9.013,11,2015,1,1,0,0,5,TRUE,row0000002# nfbVD-5S-9.063,30,2015,1,1,0,1,5,TRUE,row0000003# nfbVD-5N-9.013,11,2015,1,1,0,1,5,TRUE,row0000004# nfbVD-5S-9.063,18,2015,1,1,0,2,5,TRUE,row0000005# nfbVD-5N-9.013,5,2015,1,1,0,2,5,TRUE,row0000006# nfbVD-5S-9.063,24,2015,1,1,0,3,5,TRUE,row0000007# nfbVD-5N-9.013,6,2015,1,1,0,3,5,TRUE,row0000008# nfbVD-5S-9.063,24,2015,1,1,0,4,5,TRUE,row0000009# nfbVD-5N-9.013,6,2015,1,1,0,4,5,TRUE,row0000010head tw5_df.json# [# &#123;"vdid":"nfbVD-5S-9.063","volume":30,"date_year":2015,"date_month":1,"date_day":1,"time_hour":0,"time_minute":0,# "weekday":5,"holiday":true,"speed":"84.26666667","laneoccupy":"12.13333333"&#125;,# &#123;"vdid":"nfbVD-5N-9.013","volume":11,"date_year":2015,"date_month":1,"date_day":1,"time_hour":0,"time_minute":0,# "weekday":5,"holiday":true,"speed":"87.36363636","laneoccupy":"4.00000000"&#125;,# &#123;"vdid":"nfbVD-5S-9.063","volume":30,"date_year":2015,"date_month":1,"date_day":1,"time_hour":0,"time_minute":1,# "weekday":5,"holiday":true,"speed":"84.26666667","laneoccupy":"12.13333333"&#125;,# &#123;"vdid":"nfbVD-5N-9.013","volume":11,"date_year":2015,"date_month":1,"date_day":1,"time_hour":0,"time_minute":1,# "weekday":5,"holiday":true,"speed":"87.36363636","laneoccupy":"4.00000000"&#125;,# &#123;"vdid":"nfbVD-5S-9.063","volume":18,"date_year":2015,"date_month":1,"date_day":1,"time_hour":0,"time_minute":2,# 放到hdfs上hdfs dfs -mkdir /drillhdfs dfs -put tw5_df.csv /drill/tw5_df.csvhdfs dfs -put tw5_df.csv /drill/tw5_df_hive.csvhdfs dfs -put tw5_df_hbase.csv /drill/tw5_df_hbase.csvhdfs dfs -put tw5_df.json /drill/tw5_df.json HBase使用hbase shell，然後用下面的script去input資料以及做query： hbase shell &gt;是打開hbase shell跑的意思，不然請在console跑 12345678910111213141516171819## hbase shell &gt; create 'vddata','vdid','vd_info','datetime'hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns="vdid,vd_info:volume,datetime:year,datetime:month,datetime:day,datetime:hour,datetime:minute,datetime:weekday,datetime:holiday,vd_info:speed,vd_info:laneoccupy,HBASE_ROW_KEY" '-Dimporttsv.separator=,' vddata /drill/tw5_df_hbase.csv## hbase shell &gt; scan 'vddata', &#123;LIMIT =&gt; 1&#125;# ROW COLUMN+CELL# row0000001 column=datetime:day, timestamp=1474478950775, value=2015# row0000001 column=datetime:holiday, timestamp=1474478950775, value=0# row0000001 column=datetime:hour, timestamp=1474478950775, value=1# row0000001 column=datetime:minute, timestamp=1474478950775, value=1# row0000001 column=datetime:month, timestamp=1474478950775, value=30# row0000001 column=datetime:weekday, timestamp=1474478950775, value=0# row0000001 column=datetime:year, timestamp=1474478950775, value=1.21333333333333E1# row0000001 column=vd_info:laneoccupy, timestamp=1474478950775, value=TRUE# row0000001 column=vd_info:speed, timestamp=1474478950775, value=5# row0000001 column=vd_info:volume, timestamp=1474478950775, value=8.42666666666667E1# row0000001 column=vdid:, timestamp=1474478950775, value=nfbVD-5S-9.063# 1 row(s) in 0.0260 seconds# 結果發現group by要用JAVA API去寫...就只好放棄了，留給Drill用 Hive則用下面的SQL script： 1234567891011121314151617181920212223242526272829303132333435CREATE TABLE vddata (vdid STRING, speed DOUBLE, laneoccupy DOUBLE, volume INT, date_year INT, date_month INT, date_day INT, time_hour INT, time_minute INT, weekday INT, holiday BOOLEAN)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILE;LOAD DATA INPATH '/drill/tw5_df_hive.csv'OVERWRITE INTO TABLE vddata;select * from vddata limit 5;# OK# nfbVD-5S-9.063 30.0 2015 1 1 0 0 5 true# nfbVD-5N-9.013 11.0 2015 1 1 0 0 5 true# nfbVD-5S-9.063 30.0 2015 1 1 0 1 5 true# nfbVD-5N-9.013 11.0 2015 1 1 0 1 5 true# nfbVD-5S-9.063 18.0 2015 1 1 0 2 5 true# Time taken: 0.86 seconds, Fetched: 5 row(s)select count(vdid) from vddata;# Total MapReduce CPU Time Spent: 4 seconds 590 msec# OK# Time taken: 27.214 seconds, Fetched: 1 row(s)select date_month,count(vdid) from vddata group by date_month;# Total MapReduce CPU Time Spent: 4 seconds 330 msec# Time taken: 23.09 seconds, Fetched: 12 row(s)select date_month,avg(speed),avg(laneoccupy),avg(volume) from vddata group by date_month;# Total MapReduce CPU Time Spent: 4 seconds 100 msec# Time taken: 19.655 seconds, Fetched: 12 row(s)select date_month,date_day,avg(speed),avg(laneoccupy),avg(volume) from vddata group by date_month,date_day;# Total MapReduce CPU Time Spent: 5 seconds 230 msec# Time taken: 19.033 seconds, Fetched: 364 row(s) 再來是Spark SQL，要用Spark SQL就只能用 123456789101112131415161718192021222324252627import org.apache.spark.sql.SparkSessionimport java.util.Calendar._import java.sql.Timestampval spark = SparkSession.builder().appName("spark on hive") .config("spark.sql.warehouse.dir", "hdfs://hc1/spark") .enableHiveSupport().getOrCreate() val st = getInstance().getTime()val sql_res_1 = spark.sql("select count(vdid) from vddata").collect()println(getInstance().getTime().getTime() - st.getTime())// 7724 msval st = getInstance().getTime()val sql_res_2 = spark.sql("select date_month,count(vdid) from vddata group by date_month").collect()println(getInstance().getTime().getTime() - st.getTime())// 3729 msval st = getInstance().getTime()val sql_res_3 = spark.sql("select date_month,avg(speed),avg(laneoccupy),avg(volume) from vddata group by date_month").collect()println(getInstance().getTime().getTime() - st.getTime())// 4520 msval st = getInstance().getTime()val sql_res_4 = spark.sql("select date_month,date_day,avg(speed),avg(laneoccupy),avg(volume) from vddata group by date_month,date_day").collect()println(getInstance().getTime().getTime() - st.getTime())// 4353 ms 最後是Drill： 123456789101112131415161718192021222324252627282930313233343536373839404142# on HBaseselect count(vdid) from hbase.vddata;# 1.319sselect vddata.datetime.`month`,count(vddata.vdid) from hbase.vddata group by vddata.datetime.`month`;# 11.125s select vddata.datetime.`month`,avg(vddata.vd_info.volume) from hbase.vddata group by vddata.datetime.`month`;# datatype errorselect datetime.month,date_day,avg(vd_info.speed),avg(vd_info.laneoccupy),avg(vd_info.volume) from hbase.vddata group by datetime.month,datetime.day;# datatype error# on Hiveselect count(vdid) from hive_cassSpark1.vddata;# in timeselect date_month,count(vdid) from hive_cassSpark1.vddata group by date_month;# 2.203s select date_month,avg(speed),avg(laneoccupy),avg(volume) from hive_cassSpark1.vddata group by date_month;# 2.632sselect date_month,date_day,avg(speed),avg(laneoccupy),avg(volume) from hive_cassSpark1.vddata group by date_month,date_day;# 3.167s# on csv in hdfsselect count(columns[0]) from dfs.`/drill/tw5_df.csv`;# 1.078sselect columns[5],count(columns[0]) from dfs.`/drill/tw5_df.csv` group by columns[5];# 2.049sselect columns[5],avg(columns[1]),avg(columns[2]),avg(columns[3]) from dfs.`/drill/tw5_df.csv` group by columns[5];# datatype error select columns[5],columns[6],avg(columns[1]),avg(columns[2]),avg(columns[3]) from dfs.`/drill/tw5_df.csv` group by columns[5],columns[6];# datatype error # on json in hdfsselect count(vdid) from dfs.`/drill/tw5_df.json`;# 2.834sselect date_month,count(vdid) from dfs.`/drill/tw5_df.json` group by date_month;# 5.778s select date_month,avg(speed),avg(laneoccupy),avg(volume) from dfs.`/drill/tw5_df.json` group by date_month;# datatype error select date_month,date_day,avg(speed),avg(laneoccupy),avg(volume) from dfs.`/drill/tw5_df.json` group by date_month,date_day;# datatype error datatype error是因為裡面有int, double混在同一列，Drill現在還無法有效處理這種問題 因此，透過Hive去做storage會是比較好的選擇 小結一下，在這樣的資料量(1M x 11)下，其實用Spark SQL沒有輸Drill太多 接下來測試看看大一點的資料量 我從UCI下載了兩個資料下來：SUSY跟HIGGS 解壓縮後的大小分別是G跟G，然後直接用hive把資料放上去，分別用Spark SQL跟Drill去測試看看 先看一下資料長相 12345678910111213head SUSY.csv# 0.000000000000000000e+00,9.728614687919616699e-01,6.538545489311218262e-01,1.176224589347839355e+00,1.157156467437744141e+00,-1.739873170852661133e+00,-8.743090629577636719e-01,5.677649974822998047e-01,-1.750000417232513428e-01,8.100607395172119141e-01,-2.525521218776702881e-01,1.921887040138244629e+00,8.896374106407165527e-01,4.107718467712402344e-01,1.145620822906494141e+00,1.932632088661193848e+00,9.944640994071960449e-01,1.367815494537353516e+00,4.071449860930442810e-02# 1.000000000000000000e+00,1.667973041534423828e+00,6.419061869382858276e-02,-1.225171446800231934e+00,5.061022043228149414e-01,-3.389389812946319580e-01,1.672542810440063477e+00,3.475464344024658203e+00,-1.219136357307434082e+00,1.295456290245056152e-02,3.775173664093017578e+00,1.045977115631103516e+00,5.680512785911560059e-01,4.819284379482269287e-01,0.000000000000000000e+00,4.484102725982666016e-01,2.053557634353637695e-01,1.321893453598022461e+00,3.775840103626251221e-01 # 1.000000000000000000e+00,4.448399245738983154e-01,-1.342980116605758667e-01,-7.099716067314147949e-01,4.517189264297485352e-01,-1.613871216773986816e+00,-7.686609029769897461e-01,1.219918131828308105e+00,5.040258169174194336e-01,1.831247568130493164e+00,-4.313853085041046143e-01,5.262832045555114746e-01,9.415140151977539062e-01,1.587535023689270020e+00,2.024308204650878906e+00,6.034975647926330566e-01,1.562373995780944824e+00,1.135454416275024414e+00,1.809100061655044556e-01# 1.000000000000000000e+00,3.812560737133026123e-01,-9.761453866958618164e-01,6.931523084640502930e-01,4.489588439464569092e-01,8.917528986930847168e-01,-6.773284673690795898e-01,2.033060073852539062e+00,1.533040523529052734e+00,3.046259880065917969e+00,-1.005284786224365234e+00,5.693860650062561035e-01,1.015211343765258789e+00,1.582216739654541016e+00,1.551914215087890625e+00,7.612152099609375000e-01,1.715463757514953613e+00,1.492256760597229004e+00,9.071890264749526978e-02 # 1.000000000000000000e+00,1.309996485710144043e+00,-6.900894641876220703e-01,-6.762592792510986328e-01,1.589282631874084473e+00,-6.933256387710571289e-01,6.229069828987121582e-01,1.087561845779418945e+00,-3.817416727542877197e-01,5.892043709754943848e-01,1.365478992462158203e+00,1.179295063018798828e+00,9.682182073593139648e-01,7.285631299018859863e-01,0.000000000000000000e+00,1.083157896995544434e+00,4.342924803495407104e-02,1.154853701591491699e+00,9.485860168933868408e-02head HIGGS.csv# 1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01"# 1.000000000000000000e+00,9.075421094894409180e-01,3.291472792625427246e-01,3.594118654727935791e-01,1.497969865798950195e+00,-3.130095303058624268e-01,1.095530629158020020e+00,-5.575249195098876953e-01,-1.588229775428771973e+00,2.173076152801513672e+00,8.125811815261840820e-01,-2.136419266462326050e-01,1.271014571189880371e+00,2.214872121810913086e+00,4.999939501285552979e-01,-1.261431813240051270e+00,7.321561574935913086e-01,0.000000000000000000e+00,3.987008929252624512e-01,-1.138930082321166992e+00,-8.191101951524615288e-04,0.000000000000000000e+00,3.022198975086212158e-01,8.330481648445129395e-01,9.856996536254882812e-01,9.780983924865722656e-01,7.797321677207946777e-01,9.923557639122009277e-01,7.983425855636596680e-01" # 1.000000000000000000e+00,7.988347411155700684e-01,1.470638751983642578e+00,-1.635974764823913574e+00,4.537731707096099854e-01,4.256291687488555908e-01,1.104874610900878906e+00,1.282322287559509277e+00,1.381664276123046875e+00,0.000000000000000000e+00,8.517372012138366699e-01,1.540658950805664062e+00,-8.196895122528076172e-01,2.214872121810913086e+00,9.934899210929870605e-01,3.560801148414611816e-01,-2.087775468826293945e-01,2.548224449157714844e+00,1.256954550743103027e+00,1.128847599029541016e+00,9.004608392715454102e-01,0.000000000000000000e+00,9.097532629966735840e-01,1.108330488204956055e+00,9.856922030448913574e-01,9.513312578201293945e-01,8.032515048980712891e-01,8.659244179725646973e-01,7.801175713539123535e-01" # 0.000000000000000000e+00,1.344384789466857910e+00,-8.766260147094726562e-01,9.359127283096313477e-01,1.992050051689147949e+00,8.824543952941894531e-01,1.786065936088562012e+00,-1.646777749061584473e+00,-9.423825144767761230e-01,0.000000000000000000e+00,2.423264741897583008e+00,-6.760157942771911621e-01,7.361586689949035645e-01,2.214872121810913086e+00,1.298719763755798340e+00,-1.430738091468811035e+00,-3.646581768989562988e-01,0.000000000000000000e+00,7.453126907348632812e-01,-6.783788204193115234e-01,-1.360356330871582031e+00,0.000000000000000000e+00,9.466524720191955566e-01,1.028703689575195312e+00,9.986560940742492676e-01,7.282806038856506348e-01,8.692002296447753906e-01,1.026736497879028320e+00,9.579039812088012695e-01" # 1.000000000000000000e+00,1.105008959770202637e+00,3.213555514812469482e-01,1.522401213645935059e+00,8.828076124191284180e-01,-1.205349326133728027e+00,6.814661026000976562e-01,-1.070463895797729492e+00,-9.218706488609313965e-01,0.000000000000000000e+00,8.008721470832824707e-01,1.020974040031433105e+00,9.714065194129943848e-01,2.214872121810913086e+00,5.967612862586975098e-01,-3.502728641033172607e-01,6.311942934989929199e-01,0.000000000000000000e+00,4.799988865852355957e-01,-3.735655248165130615e-01,1.130406111478805542e-01,0.000000000000000000e+00,7.558564543724060059e-01,1.361057043075561523e+00,9.866096973419189453e-01,8.380846381187438965e-01,1.133295178413391113e+00,8.722448945045471191e-01,8.084865212440490723e-01 12hdfs dfs -put SUSY.csv.gz /drill/SUSY.csv.gzhdfs dfs -put HIGGS.csv.gz /drill/HIGGS.csv.gz Hive上傳local file 12345678910111213141516171819202122232425CREATE TABLE susy_df (lepton1pt DOUBLE, lepton1eta DOUBLE, lepton1phi DOUBLE, lepton2pt DOUBLE, lepton2eta DOUBLE, lepton2phi DOUBLE, mem DOUBLE, mep DOUBLE, met_rel DOUBLE, axialmet DOUBLE, m_r DOUBLE, m_tr_2 DOUBLE, r DOUBLE, mt2 DOUBLE,s_r DOUBLE, m_delta_r DOUBLE, dphi_r_b DOUBLE, cos_theta_r1 DOUBLE)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '/home/tester/SUSY.csv.gz'OVERWRITE INTO TABLE susy_df;-- Time taken: 19.125 secondsCREATE TABLE higgs_df (lepton_pt DOUBLE, lepton_eta DOUBLE, lepton_phi DOUBLE,mem DOUBLE, mep DOUBLE, jet1pt DOUBLE, jet1eta DOUBLE, jet1phi DOUBLE, jet1b_tag DOUBLE, jet2pt DOUBLE, jet2eta DOUBLE, jet2phi DOUBLE, jet2b_tag DOUBLE,jet3pt DOUBLE, jet3eta DOUBLE, jet3phi DOUBLE, jet3b_tag DOUBLE, jet4pt DOUBLE,jet4eta DOUBLE, jet4phi DOUBLE, jet4b_tag DOUBLE, m_jj DOUBLE, m_jjj DOUBLE,m_lv DOUBLE, m_jlv DOUBLE, m_bb DOUBLE, m_wbb DOUBLE, m_wwbb DOUBLE)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '/home/tester/HIGGS.csv.gz'OVERWRITE INTO TABLE higgs_df;-- Time taken: 50.145 seconds 複雜一點的運算，或是filter，我的VM都撐不住，就只能等到有好電腦才有機會測試了 這裡就簡單測試aggregation select count(lepton1phi) from susy_df group by lepton1ptselect count(lepton_phi) from higgs_df group by lepton_pt Hive Spark Drill SUSY 44.374s 28.794s 29.262s HIGGS 105.887s 58.748s 74s Spark script: 12345678910111213import org.apache.spark.sql.SparkSessionimport java.util.Calendar._import java.sql.Timestampval spark = SparkSession.builder().appName("spark on hive").config("spark.sql.warehouse.dir", "hdfs://hc1/spark").enableHiveSupport().getOrCreate()val st = getInstance().getTime()val susy_out_3 = spark.sql("select count(lepton1phi) from susy_df group by lepton1pt").collect()println(getInstance().getTime().getTime() - st.getTime())val st = getInstance().getTime()val tmp = spark.sql("select count(lepton_phi) from higgs_df group by lepton_pt").collect()println(getInstance().getTime().getTime() - st.getTime()) Drill運行SQL script 12select count(lepton1phi) from hive_cassSpark1.susy_df group by lepton1ptselect count(lepton_phi) from hive_cassSpark1.higgs_df group by lepton_pt 在這麼大量的資料中，Spark用了三個核心，Drill只用了一個核心 這樣的差距都還在可以接受的範圍，Drill稍微微調後，應該可以在相同的電腦設備下 擁有比Spark SQL還強大的運算能力 結論，Drill在我看來會是比較長久的發展，只是它只適合用來做為讀取介面]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>Hive</tag>
        <tag>Drill</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark on Apache Hive]]></title>
    <url>%2Fposts%2F201609%2F2016-09-22-spark-on-hive.html</url>
    <content type="text"><![CDATA[這篇主要是用Spark去連接現存的Hive 可能有人會先好奇說為什麼不用Spark本身的Thrift Server 我稍微看了一下，Spark的Thrift Server只能跑Local 也就是說你的資料只能在一台電腦上跑，因此，這樣是有風險的 只是現在Hive的設定只在一台Mysql上，也是很有風險 但是Hive可以把Metastore移到Mysql Cluster上，這樣就可以避開這個風險了 不過這不是本篇的重點，本篇會專注在怎麼用Spark去連接現有的Hive 配置很簡單，只需要使用下面四個指令，以及修改一下Spark的spark-default.conf即可 1234cp $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/cp $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf/cp $HIVE_HOME/lib/mysql-connector-java-5.1.39-bin.jar $SPARK_HOME/extraClass/ spark-default.conf增加下面的東西： 12spark.driver.extraClassPath /usr/local/bigdata/spark/extraClass/mysql-connector-java-5.1.39-bin.jarspark.executor.extraClassPath /usr/local/bigdata/spark/extraClass/mysql-connector-java-5.1.39-bin.jar 如果已經設定了，就用,去append即可 接下來就可以直接執行spark-shell了 執行spark-shell mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos script如下： 123456789101112131415161718192021222324252627282930import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("spark on hive"). config("spark.sql.warehouse.dir", "hdfs://hc1/spark"). enableHiveSupport().getOrCreate()// 之前丟上去hive的test_df，沒有資料的話可以往前翻兩篇...spark.sql("select count(*) from vddata").show()/*+--------+|count(1)|+--------+| 41|+--------+*/spark.sql("select v1,v2,sum(v3) from test_df group by v1,v2").show()/*+---+---+-------------------+| v1| v2| sum(v3)|+---+---+-------------------+| b| e| 1.1515786935289156|| a| c| 2.1048099797195103|| b| d| 0.5861563687201317|| a| d|-0.4549045928741139|| b| c| 2.0411431978258983|| a| e| -4.942821584114654|| v1| v2| null|+---+---+-------------------+*/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hive</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Hive and ACID]]></title>
    <url>%2Fposts%2F201609%2F2016-09-21-apache-hive-ACID.html</url>
    <content type="text"><![CDATA[Hive支援ACID，可以讓資料庫做transactions 其具備以下四種性質 原子性(atomicity)：做就要做完，不做就全部都不做，不會做一半 一致性(consistency)：資料庫的資訊是完整的 隔離性(isolation)：可以同時讀寫多個transactions，讓讀寫不會互相影響 持久性(durability)：資料的修改是永久的，不會丟失 那麼Hive支援這個有什麼好處？讓Hive能夠如同RMDB去做資料的UPDATE, DELETE 重新下載一個新的Hive做部署，MySQL那裏則跳過 123456789101112131415161718192021# 下載hive並部署curl -v -j -k -L http://apache.stu.edu.tw/hive/stable/apache-hive-1.2.1-bin.tar.gz -o apache-hive-1.2.1-bin.tar.gztar zxvf apache-hive-1.2.1-bin.tar.gzmv apache-hive-1.2.1-bin /usr/local/bigdata/hive# 增加pathsudo tee -a /etc/bashrc &lt;&lt; "EOF"export HIVE_HOME=/usr/local/bigdata/hiveexport PATH=$PATH:$HIVE_HOME/binEOFsource /etc/bashrc# 解壓縮mysql jdbc connectortar zxvf mysql-connector-java-5.1.39.tar.gzcp mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $HIVE_HOME/lib# 複製設定cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml# 新增hive所需的目錄sudo mkdir /tmp/testersudo mkdir /tmp/tester/hive_resourcesudo chown -R tester.tester /tmp/tester# 初始化hive的schemaschematool -initSchema -dbType mysql 用vi $HIVE_HOME/conf/hive-site.xml去配置設定 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.0.121:3306/hive?autoReconnect=true&amp;amp;useSSL=true&amp;amp;createDatabaseIfNotExist=true&amp;amp;characterEncoding=utf8&lt;/value&gt; &lt;description&gt;JDBC connection string used by Hive Metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;JDBC Driver class&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Metastore database user name&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;Qscf123%^&lt;/value&gt; &lt;description&gt;Metastore database password&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/tester&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/tester/hive_resource&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; Whether to run the initiator and cleaner threads on this metastore instance or not. Set this to true on one instance of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; How many compactor worker threads to run on this metastore instance. Set this to a positive number on one or more instances of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager. Worker threads spawn MapReduce jobs to do compactions. They do not do the compactions themselves. Increasing the number of worker threads will decrease the time it takes tables or partitions to be compacted once they are determined to need compaction. It will also increase the background load on the Hadoop cluster as more MapReduce jobs will be running in the background. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; Whether Hive supports concurrency control or not. A ZooKeeper instance must be up and running when using zookeeper Hive lock manager &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.enforce.bucketing&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether bucketing is enforced. If true, while inserting into the table, bucketing is enforced.&lt;/description&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether or not to allow dynamic partitions in DML/DDL.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;nonstrict&lt;/value&gt; &lt;description&gt; In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions. In nonstrict mode all partitions are allowed to be dynamic. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.txn.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager&lt;/value&gt; &lt;description&gt; Set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager as part of turning on Hive transactions, which also requires appropriate settings for hive.compactor.initiator.on, hive.compactor.worker.threads, hive.support.concurrency (true), hive.enforce.bucketing (true), and hive.exec.dynamic.partition.mode (nonstrict). The default DummyTxnManager replicates pre-Hive-0.13 behavior and provides no transactions. &lt;/description&gt; &lt;/property&gt; 測試： 1234567891011# 新增資料tee ~/test_df2.csv &lt;&lt; "EOF"id1,a,0.01991423id2,b,0.73282957id3,c,0.00552144id4,d,0.83103357id5,a,-0.79378789id6,b,-0.36969293id7,c,-1.66246829id8,d,-0.73893442EOF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869-- 創一張表，先放資料CREATE TABLE test_df2 (v1 STRING, v2 STRING, v3 DOUBLE)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILE;-- 讀入資料LOAD DATA LOCAL INPATH '/home/tester/test_df2.csv'OVERWRITE INTO TABLE test_df2;-- 創一張transaction表create table test_df2_transaction(v1 STRING, v2 STRING, v3 DOUBLE) clustered by (v1) into 3 buckets stored as orc TBLPROPERTIES ('transactional'='true');-- 把資料加入transaction表insert into test_df2_transaction select * from test_df2;-- 看資料select * from test_df2_transaction;/*id8 d -0.73893442id5 a -0.79378789id2 b 0.73282957id6 b -0.36969293id3 c 0.00552144id7 c -1.66246829id4 d 0.83103357id1 a 0.01991423*/-- 試試看updateupdate test_df2_transaction set v2='c' where v1='id1';select * from test_df2_transaction;/*id8 d -0.73893442id5 a -0.79378789id2 b 0.73282957id11 a 2.001id6 b -0.36969293id3 c 0.00552144id9 b 2.001id7 c -1.66246829id4 d 0.83103357id1 c 2.001*/-- 試試看deletedelete from test_df2_transaction where v1='id1';select * from test_df2_transaction;/*id8 d -0.73893442id5 a -0.79378789id2 b 0.73282957id6 b -0.36969293id3 c 0.00552144id7 c -1.66246829id4 d 0.83103357*/-- 試試看insertinsert into table test_df2_transaction values ('id9', 'b', 2.001),('id1', 'b', 2.001),('id11', 'a', 2.001);select * from test_df2_transaction;/*id8 d -0.73893442id5 a -0.79378789id2 b 0.73282957id11 a 2.001id6 b -0.36969293id3 c 0.00552144id9 b 2.001id7 c -1.66246829id4 d 0.83103357id1 b 2.001*/ 接下來用spark去測試 用spark-shell mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos開啟 12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("spark on hive"). config("spark.sql.warehouse.dir", "hdfs://hc1/spark"). enableHiveSupport().getOrCreate()spark.sql("update test_df2_transaction set v2='c' where v1='id1'").show()# org.apache.spark.sql.catalyst.parser.ParseExceptionspark.sql("delete from test_df2_transaction where v1='id1'").show()# org.apache.spark.sql.catalyst.parser.ParseException Spark SQL無法使用update或是delete的動作]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hive</tag>
        <tag>ACID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Hive with Apache Drill]]></title>
    <url>%2Fposts%2F201609%2F2016-09-21-apache-hive-with-apache-drill.html</url>
    <content type="text"><![CDATA[前一篇的Apache Drill效能方面極佳 唯一可惜的點是不能直接存寫hive, hbase 但是如果只需要用到讀取資料 不做insert, update的話，Drill無疑是最佳的方案 這篇主要介紹怎麼建立Hive，Hive的建立相當麻煩 以前建立過一次，就不想在建立它，只是沒想到還是得用它 Hive有三種建立模式： localhost derby localhost mysql remote mysql 以前是直接走localhost mysql，這次則採用remote mysql的方式建立 這樣的好處是，全部的cluster都共用一個metastore，不需要再各台都建立mysql mysql也可以用其他資料庫代替，如Oracle, PostgreSQL以及MS SQL Server 不過在centos中最簡單取得的就是Mysql，而我這使用的是Oracle MySQL community server 請到下面網址去下載Red Hat Enterprise Linux 7 / Oracle Linux 7 (x86, 64-bit), RPM Bundle： http://dev.mysql.com/downloads/mysql/ 並在 http://dev.mysql.com/downloads/connector/j/ 下載mysql的jdbc連線用的jar檔 安裝 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 開始部署tar xvf mysql-5.7.15-1.el7.x86_64.rpm-bundle.tarsudo yum remove mariadb-libssudo yum install -y mysql-community-client-5.7.15-1.el7.x86_64.rpm mysql-community-common-5.7.15-1.el7.x86_64.rpm mysql-community-devel-5.7.15-1.el7.x86_64.rpm mysql-community-libs-5.7.15-1.el7.x86_64.rpm mysql-community-server-5.7.15-1.el7.x86_64.rpm mysql-community-test-5.7.15-1.el7.x86_64.rpm# 配置mysql檔案tee -a /etc/my.cnf &lt;&lt; "EOF"bind-address=192.168.0.121skip_sslEOF# 啟動mysqlsudo systemctl start mysqld# 抓取密碼sudo grep 'temporary password' /var/log/mysqld.log # find the password# 登入mysqlmysql -u root -p# 修改密碼跟創新的user for hive# mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'Qscf123%^';# mysql&gt; create user 'hive'@'%' identified by 'Qscf123%^';# mysql&gt; create database hive DEFAULT CHARACTER SET utf8;# mysql&gt; grant all PRIVILEGES on *.* TO 'hive'@'%' IDENTIFIED BY 'Qscf123%^' WITH GRANT OPTION;# 重啟mysqlsudo systemctl restart mysqld# 下載hive並部署curl -v -j -k -L http://apache.stu.edu.tw/hive/stable/apache-hive-1.2.1-bin.tar.gz -o apache-hive-1.2.1-bin.tar.gztar zxvf apache-hive-1.2.1-bin.tar.gzmv apache-hive-1.2.1-bin /usr/local/bigdata/hive# 增加pathsudo tee -a /etc/bashrc &lt;&lt; "EOF"export HIVE_HOME=/usr/local/bigdata/hiveexport PATH=$PATH:$HIVE_HOME/binEOFsource /etc/bashrc# 解壓縮mysql jdbc connectortar zxvf mysql-connector-java-5.1.39.tar.gzcp mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $HIVE_HOME/lib# 配置hivecp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xmlvi $HIVE_HOME/conf/hive-site.xml hive的配置項目： 123456789101112131415161718192021222324252627282930&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.0.121:3306/hive?autoReconnect=true&amp;amp;useSSL=true&amp;amp;createDatabaseIfNotExist=true&amp;amp;characterEncoding=utf8&lt;/value&gt; &lt;description&gt;JDBC connection string used by Hive Metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;JDBC Driver class&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Metastore database user name&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;Qscf123%^&lt;/value&gt; &lt;description&gt;Metastore database password&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/tester&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/tester/hive_resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt; 配置完hive之後，就可以開始啟動了 123456789101112131415161718192021222324252627282930# 新增hive所需的目錄sudo mkdir /tmp/testersudo mkdir /tmp/tester/hive_resourcesudo chown -R tester.tester /tmp/tester# 初始化hive的schemaschematool -initSchema -dbType mysql# 啟動hivehive # 成功進去之後就可以用exit;離開了# 配置hive的supervisord啟動方案sudo vi /etc/supervisor/supervisord.conf# environmet# HIVE_HOME=/usr/local/bigdata/hivesudo tee -a /etc/supervisor/supervisord.conf &lt;&lt; "EOF"[program:hive]command=/bin/bash -c "$HIVE_HOME/bin/hive --service metastore"stdout_logfile=/var/log/supervisor/hive-stdout.outstderr_logfile=/var/log/supervisor/hive-stderr.outautostart=truestartsecs=5priority=90EOF# 重啟supervisordsudo systemctl restart supervisor# 複製到各台，其他台都連這台的mysql即可，複製過去配置PATH跟supervisord即可使用scp -r $HIVE_HOME tester@cassSpark2:/usr/local/bigdatascp -r $HIVE_HOME tester@cassSpark3:/usr/local/bigdata 測試： 123456789101112131415161718192021222324252627282930313233343536373839404142tee ~/test_df.csv &lt;&lt; "EOF"b,d,0.01991423b,e,0.73282957b,c,0.00552144b,d,0.83103357a,d,-0.79378789a,e,-0.36969293a,c,-1.66246829b,c,-0.73893442a,c,-0.49832169b,e,-0.83277815a,e,-0.10230033a,c,0.14617246a,d,-0.50122879a,d,-1.01900482b,e,-0.08073591a,d,-1.02780559b,d,-1.28585578a,e,2.19115715b,c,0.22095974a,d,0.98055576b,d,0.92121262a,c,0.43742128a,d,0.1683678a,e,-1.31987619b,c,1.0675091b,e,0.49024668a,e,-1.65978632b,c,-0.1294416b,c,1.00880932a,d,0.27295147b,d,-0.1935518a,d,0.92765404b,d,-0.49652849b,c,0.11603917a,d,1.00496088a,e,0.5742589a,c,-0.07431593a,e,1.91539019a,c,0.07681478b,c,0.69678599EOF 12345678910111213141516CREATE TABLE test_df (v1 STRING, v2 STRING, v3 DOUBLE)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '/home/tester/test_df.csv'OVERWRITE INTO TABLE test_df;select count(*) from test_df; # 40select v1,v2,sum(v3) from test_df group by v1,v2; # a c -1.57469739# a d 0.012662860000000276# a e 1.22915047# b c 2.24724874# b d -0.2037756499999998# b e 0.30956219000000007 附上成功執行畫面： 可以看到它是透過hadoop的mapreduce去做的，執行時間是23.925秒 與Apache Drill共舞 先修改Storage的hive，改成下方這樣： 123456789&#123; "type": "hive", "enabled": true, "configProps": &#123; "hive.metastore.uris": "thrift://192.168.0.121:9083", "hive.metastore.sasl.enabled": "false", "fs.default.name": "hdfs://hc1/" &#125;&#125; 到query去執行下方的查詢： 1select v1,v2,avg(v3) from test_df group by v1,v2; 可以在Profile那裏看到執行的細節： 全部執行時間只有0.530s，整整比hive的mapreduce快上45倍 最後，也測試看看R直接用Drill的REST方式去query到hive的結果 12345678910111213141516171819202122232425262728library(httr)library(jsonlite)post_data &lt;- POST(&quot;http://192.168.0.121:8047/query.json&quot;, body = list(queryType = &quot;SQL&quot;, query = &quot;SELECT v1,v2,avg(v3) v3_avg FROM hive_cassSpark1.test_df group by v1,v2&quot;), encode = &quot;json&quot;)content(post_data, type = &quot;text&quot;) %&gt;&gt;% fromJSON %&gt;&gt;% `[[`(2)# No encoding supplied: defaulting to UTF-8.# v3_avg v1 v2# 1 -0.262449565 a c# 2 0.0014069844444444442 a d# 3 0.17559292428571424 a e# 4 0.28090609250000004 b c# 5 -0.03396260833333329 b d# 6 0.0773905475 b e# 這裡v3_avg會是字串，需要自己parse，但是如果很熟R的話，這一點應該不成問題# 用下面指令即可library(data.table)content(post_data, type = &quot;text&quot;) %&gt;&gt;% fromJSON %&gt;&gt;% `[[`(2) %&gt;&gt;% data.table %&gt;&gt;% `[`( , lapply(.SD, type.convert, as.is = TRUE)) %&gt;&gt;% str# Classes ‘data.table’ and &apos;data.frame&apos;: 6 obs. of 3 variables:# $ v3_avg: num -0.26245 0.00141 0.17559 0.28091 -0.03396 ...# $ v1 : chr &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; ...# $ v2 : chr &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;c&quot; ...# - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; 將mysql納入supervisord 執行下面的指令即可 1234567891011121314sudo systemctl stop mysqldsudo systemctl disable mysqldsudo tee -a /etc/supervisor/supervisord.conf &lt;&lt; "EOF"[program:mysql]command=/usr/bin/pidproxy /var/run/mysqld/mysqld.pid /usr/sbin/mysqldstdout_logfile=/var/log/supervisor/mysql-stdout.outstderr_logfile=/var/log/supervisor/mysql-stderr.outautostart=truestartsecs=5priority=80user=mysqlEOFsudo systemctl restart supervisor]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hive</tag>
        <tag>Drill</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Drill]]></title>
    <url>%2Fposts%2F201609%2F2016-09-20-apache-drill.html</url>
    <content type="text"><![CDATA[SQL on Hadoop不外乎Apache Drill, Hive, Hive on Tez, Phoenix, Cloudera Impala (正在孵化為Apache專案), Presto, Pivotal HAWQ, IBM BigSQL, Apache Tajo, Apache Kylin等 在這麼多選擇中，我選擇用Drill，以下闡述我的原因 Drill是一套SQL on Hadoop的解決方案 一個Schema-free的data model，這意思代表說不在受限於data model限制而無法查詢 但是他不能用index，所以在有些查詢上會比有schema的data model來的慢 儘管如此，Drill在query的表現仍然相當優秀，勝過Hive with MapReduce, Hive on Tez, Spark SQL, Presto，跟Impala互有上下 (reference 1, reference 2) 不過問題來了，Drill要base什麼去建立？ hdfs? hbase? hive? 還是其他nosql的架構？ 這個就depends on每個人的需求了，因為我需要一個Loader去同步在Oracle的資料 而這loader最簡單的方式就是使用Spark SQL去做(因為Drill沒辦法直接讀寫hive, hbase等資料庫) 所以這裡我採用hive當做表格儲存的位置，能讓Spark SQL直接做table的存寫 而且Drill直接搜尋hive的速度相當快，不需要透過hive下的engine(mapredure, tez or spark)去處理 我原本傾向使用有schema的hbase跟hive，可以直接透過Spark SQL去存讀資料 但是又想到我需要一個Loader去同步在Oracle的資料，不過Drill無法提供直接使用DataFrame存寫的功能 做了一點功課之後，決定直接使用HDFS存JSON檔案 在Spark SQL可以透過JDBC接口直接使用Spark SQL去處理資料再存新的json回去 下載檔案並部署 123curl -v -j -k -L http://apache.stu.edu.tw/drill/drill-1.8.0/apache-drill-1.8.0.tar.gz -o apache-drill-1.8.0.tar.gztar -xzvf apache-drill-1.8.0.tar.gzmv apache-drill-1.8.0 /usr/local/bigdata/drill 配置 使用vi /usr/local/bigdata/drill/conf/drill-override.conf去configure Drill 1234drill.exec: &#123; cluster-id: "drillcluster", zk.connect: "192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181"&#125; 如果記憶體不多的話建議下面配置： 1234567tee -a /usr/local/bigdata/drill/conf/drill-env.sh &lt;&lt; "EOF"# 這一行一定要留著DRILL_MAX_DIRECT_MEMORY="2G"DRILL_MAX_HEAP="1G"export DRILL_JAVA_OPTS="-Xms1G -Xmx$DRILL_MAX_HEAP -XX:MaxDirectMemorySize=$DRILL_MAX_DIRECT_MEMORY -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=1G -ea"EOF 複製到各台node 12scp -r /usr/local/bigdata/drill tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/drill tester@cassSpark3:/usr/local/bigdata 使用/usr/local/bigdata/drill/bin/drillbit.sh start去啟動Drill 就可以用http://192.168.0.121:8047/連到Drill的web UI了 可以再Storage Page看到dfs的Option，按下Update即可更新(更新下面的值即可)： 12345678910111213"connection": "hdfs://hc1","workspaces": &#123; "root": &#123; "location": "/drill", "writable": true, "defaultInputFormat": null &#125;, "tmp": &#123; "location": "/drill/tmp", "writable": true, "defaultInputFormat": null &#125;&#125; 按下Update，然後Close 再來是上傳一些資料吧 12345678910111213141516171819202122232425tee ~/test_df.json &lt;&lt; "EOF"[&#123;"v1":"b","v2":"d","v3":0.01991423&#125;,&#123;"v1":"b","v2":"e","v3":0.73282957&#125;,&#123;"v1":"b","v2":"c","v3":0.00552144&#125;,&#123;"v1":"b","v2":"d","v3":0.83103357&#125;,&#123;"v1":"a","v2":"d","v3":-0.79378789&#125;,&#123;"v1":"a","v2":"e","v3":-0.36969293&#125;,&#123;"v1":"a","v2":"c","v3":-1.66246829&#125;,&#123;"v1":"b","v2":"c","v3":-0.73893442&#125;,&#123;"v1":"a","v2":"c","v3":-0.49832169&#125;,&#123;"v1":"b","v2":"e","v3":-0.83277815&#125;,&#123;"v1":"a","v2":"e","v3":-0.10230033&#125;,&#123;"v1":"a","v2":"c","v3":0.14617246&#125;,&#123;"v1":"a","v2":"d","v3":-0.50122879&#125;,&#123;"v1":"a","v2":"d","v3":-1.01900482&#125;,&#123;"v1":"b","v2":"e","v3":-0.08073591&#125;,&#123;"v1":"a","v2":"d","v3":-1.02780559&#125;,&#123;"v1":"b","v2":"d","v3":-1.28585578&#125;,&#123;"v1":"a","v2":"e","v3":2.19115715&#125;,&#123;"v1":"b","v2":"c","v3":0.22095974&#125;,&#123;"v1":"a","v2":"d","v3":0.98055576&#125;,&#123;"v1":"b","v2":"d","v3":0.92121262&#125;,&#123;"v1":"a","v2":"c","v3":0.43742128&#125;,&#123;"v1":"a","v2":"d","v3":0.1683678&#125;,&#123;"v1":"a","v2":"e","v3":-1.31987619&#125;,&#123;"v1":"b","v2":"c","v3":1.0675091&#125;,&#123;"v1":"b","v2":"e","v3":0.49024668&#125;,&#123;"v1":"a","v2":"e","v3":-1.65978632&#125;,&#123;"v1":"b","v2":"c","v3":-0.1294416&#125;,&#123;"v1":"b","v2":"c","v3":1.00880932&#125;,&#123;"v1":"a","v2":"d","v3":0.27295147&#125;,&#123;"v1":"b","v2":"d","v3":-0.1935518&#125;,&#123;"v1":"a","v2":"d","v3":0.92765404&#125;,&#123;"v1":"b","v2":"d","v3":-0.49652849&#125;,&#123;"v1":"b","v2":"c","v3":0.11603917&#125;,&#123;"v1":"a","v2":"d","v3":1.00496088&#125;,&#123;"v1":"a","v2":"e","v3":0.5742589&#125;,&#123;"v1":"a","v2":"c","v3":-0.07431593&#125;,&#123;"v1":"a","v2":"e","v3":1.91539019&#125;,&#123;"v1":"a","v2":"c","v3":0.07681478&#125;,&#123;"v1":"b","v2":"c","v3":0.69678599&#125;]EOFtee ~/test_widecol.json &lt;&lt; "EOF"[&#123;"v1":"b","v2":"d","v3":[0.01991423,0.83103357,-1.28585578,0.92121262,-0.1935518,-0.49652849]&#125;,&#123;"v1":"b","v2":"e","v3":[0.73282957,-0.83277815,-0.08073591,0.49024668]&#125;,&#123;"v1":"b","v2":"c","v3":[0.00552144,-0.73893442,0.22095974,1.0675091,-0.1294416,1.00880932,0.11603917,0.69678599]&#125;,&#123;"v1":"a","v2":"d","v3":[-0.79378789,-0.50122879,-1.01900482,-1.02780559,0.98055576,0.1683678,0.27295147,0.92765404,1.00496088]&#125;,&#123;"v1":"a","v2":"e","v3":[-0.36969293,-0.10230033,2.19115715,-1.31987619,-1.65978632,0.5742589,1.91539019]&#125;,&#123;"v1":"a","v2":"c","v3":[-1.66246829,-0.49832169,0.14617246,0.43742128,-0.07431593,0.07681478]&#125;]EOF 上傳到hdfs去： 12345678910hdfs dfs -mkdir /drillhdfs dfs -mkdir /drill/tmphdfs dfs -put ~/test_df.json /drillhdfs dfs -put ~/test_widecol.json /drill# 確定資料有上去hdfs dfs -ls /drill# Found 3 items# -rw-r--r-- 3 tester supergroup 1456 2016-09-19 23:49 /drill/test_df.json# -rw-r--r-- 3 tester supergroup 618 2016-09-19 23:49 /drill/test_widecol.json# drwxr-xr-x - tester supergroup 0 2016-09-19 23:46 /drill/tmp 執行下面的SQL可以得到下圖的結果 1SELECT * FROM dfs.`/drill/test_df.json`; 執行下面的SQL可以得到下圖的結果 1SELECT * FROM dfs.`/drill/test_widecol.json`; 使用SQuirreL SQL Client透過JDBC去操作Drill 先點擊左邊的Driver按鈕，然後按下左上角藍色的+去新增Driver 其中Example URL填jdbc:drill:zk=cassSpark1:2181 Extra Class Path新增Drill binary tar裡面jars/jdbc-driver/drill-jdbc-all-1.8.0.jar Driver就可以選org.apache.drill.jdbc.Driver了，如同下圖的設定 接著點擊左邊的Alias，一樣按下藍色的+去新增，其中URL的填法如下： 1jdbc:drill:zk=&lt;zookeeper_quorum&gt;/&lt;drill_directory_in_zookeeper&gt;/&lt;cluster_ID&gt;;schema=&lt;schema_to_use_as_default&gt; 像是我的是jdbc:drill:zk=cassSpark1:2181/drill/drillcluster;schema=dfs cassSpark1:2181是我的zookeeper地址，drillcluster是我在conf裡面設定的cluster name 使用的預設schema就dfs (請看Drill網頁UI裡面的Storage那頁)，設定會如下圖： 然後就可以連線了！如下圖： 再來是Drill的REST接口 這裡分別用linux的curl指令跟R httr套件的POST去做： 123456library(httr)POST("http://192.168.0.121:8047/query.json", body = list(queryType = "SQL", query = "SELECT v1,v2,v3 FROM dfs.`/drill/test_df.json`"), encode = "json") 另外，也可以用jdbc去連Oracle，先從Oracle官方網站下載到ojdbc7.jar 將ojdbc7.jar放到/usr/local/bigdata/drill/lib/3party裡面，然後重開drill (記得是每一台都要放) (如果有用我之前Spark在Oracle的配置，可以直接下cp $SPARK_HOME/extraClass/ojdbc7.jar /usr/local/bigdata/drill/jars/3rdparty/) 然後在web UI的Storage增加一個New Storage Plugin，叫做oracle： 123456&#123; type: "jdbc", enabled: true, driver: "oracle.jdbc.OracleDriver", url:"jdbc:oracle:thin:system/qscf12356@192.168.0.120:1521/orcl"&#125; 就可以使用select * from oracle.&lt;user_name&gt;.&lt;table_name&gt;去做查詢了 最後附上supervisor的config 12345678910sudo tee -a /etc/supervisor/supervisord.conf &lt;&lt; &quot;EOF&quot;[program:drill]command=/bin/bash -c &quot;/usr/local/bigdata/drill/bin/drillbit.sh run&quot;stdout_logfile=/var/log/supervisor/drill-stdout.outstderr_logfile=/var/log/supervisor/drill-stderr.outautostart=truestartsecs=5priority=95EOFsudo systemctl restart supervisor]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Drill</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重返Hadoop Architecture (設定HA群集)]]></title>
    <url>%2Fposts%2F201609%2F2016-09-19-return-hadoop-architecture.html</url>
    <content type="text"><![CDATA[用完Cassandra的primary key跟secondary index之後 覺得Cassandra適合表格有固定query pattern做使用才方便 primary key中的partition key在query時都要用到 clustering key要照順序使用，secondary index不能做複合查詢 更不用提ALLOW FILTERING的功能帶來的崩潰效能 我的用途真的很難用Cassandra去滿足，也不願意透過Spark SQL去使用 (畢竟Spark SQL要吃的資源也不少) 因此，我就survey了SQL on Hadoop的solutions… 而這一篇會先forcus在Hadoop的群集建立 SQL on Hadoop將會在下一篇介紹 這一篇會非常的長，因為Hadoop之前沒有完全的建好 (讓它能夠開機自動啟動，並且有HA的能力) 這一篇斷斷續續寫了三週，一直提不太起勁一口氣完成他 廢話不多說，下面是本文 配置架構： Hadoop的HA只有兩台電腦、YARN也是同理，並且配置hdfs HA的電腦要啟動zkfc的服務 每一台都要配置Journal node去讓資料同步、並且在Hadoop Master當機時能夠做自動切換 Zookeeper要設定成奇數台 重新佈署： 部署之前的環境設定 ssh部分要實現全部都能互相連接，不能只有master能夠連到slaves 因此，這裡給一個簡單的script去做key的傳遞 1234567891011121314151617# 每一台都執行完下面兩個指令後ssh-keygen -t rsa -P ""cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# 在master上跑tee ~/all_hosts &lt;&lt; "EOF"cassSpark1cassSpark2cassSpark3EOFscp ~/all_hosts tester@cassSpark2:~/scp ~/all_hosts tester@cassSpark2:~/# 然後在每一台都跑下面這個指令(要打很多次密碼)for hostname in `cat all_hosts`; do ssh-copy-id -i ~/.ssh/id_rsa.pub $hostnamedone 下載檔案 1234567891011121314151617181920212223242526272829# 建立放置資料夾sudo mkdir /usr/local/bigdatasudo chown -R tester /usr/local/bigdata# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u101-b13/jdk-8u101-linux-x64.rpm -o jdk-8u101-linux-x64.rpmsudo yum install -y jdk-8u101-linux-x64.rpm# 下載並部署Hadoopcurl -v -j -k -L http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz -o hadoop-2.7.3.tar.gztar -zxvf hadoop-2.7.3.tar.gzmv hadoop-2.7.3 /usr/local/bigdata/hadoop# 下載並部署zookeepercurl -v -j -k -L http://apache.stu.edu.tw/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz -o zookeeper-3.4.8.tar.gztar -zxvf zookeeper-3.4.8.tar.gzmv zookeeper-3.4.8 /usr/local/bigdata/zookeeper# 下載並部署HBasecurl -v -j -k -L http://apache.stu.edu.tw/hbase/stable/hbase-1.2.2-bin.tar.gz -o hbase-1.2.2-bin.tar.gztar -zxvf hbase-1.2.2-bin.tar.gzmv hbase-1.2.2 /usr/local/bigdata/hbase# 下載並部署mesoscurl -v -j -k -L http://repos.mesosphere.com/el/7/x86_64/RPMS/mesos-1.0.0-2.0.89.centos701406.x86_64.rpm -o mesos-1.0.0-2.0.89.centos701406.x86_64.rpmsudo yum install mesos-1.0.0-2.0.89.centos701406.x86_64.rpm# 下載並部署scalacurl -v -j -k -L http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz -o scala-2.11.8.tgztar -zxvf scala-2.11.8.tgzmv scala-2.11.8 /usr/local/bigdata/scala# 下載並部署sparkcurl -v -j -k -L http://apache.stu.edu.tw/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz -o spark-2.0.0-bin-hadoop2.7.tgztar -zxvf spark-2.0.0-bin-hadoop2.7.tgzmv spark-2.0.0-bin-hadoop2.7 /usr/local/bigdata/spark 環境變數設置： 123456789101112131415161718192021222324sudo tee -a /etc/bashrc &lt;&lt; "EOF"# JAVAexport JAVA_HOME=/usr/java/jdk1.8.0_101# HADOOPexport HADOOP_HOME=/usr/local/bigdata/hadoopexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"# ZOOKEEPERexport ZOOKEEPER_HOME=/usr/local/bigdata/zookeeper# HBASEexport HBASE_HOME=/usr/local/bigdata/hbaseexport HBASE_MANAGES_ZK=falseexport HBASE_CLASSPATH=$HBASE_CLASSPATH:$HADOOP_CONF_DIRexport HBASE_CONF_DIR=$HBASE_HOME/conf# SCALAexport SCALA_HOME=/usr/local/bigdata/scala# SPARKexport SPARK_HOME=/usr/local/bigdata/spark# PATHexport PATH=$PATH:$JAVA_HOME:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/binEOFsource /etc/bashrc 開始設定 a. Zookeeper, Mesos設定 123456789101112131415161718192021222324252627282930313233## Zookeeper# 複製zoo.cfgcp $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg# 傳入設定tee $ZOOKEEPER_HOME/conf/zoo.cfg &lt;&lt; "EOF"dataDir=/usr/local/bigdata/zookeeper/dataserver.1=cassSpark1:2888:3888server.2=cassSpark2:2888:3888server.3=cassSpark3:2888:3888EOF# 接著創立需要的資料夾，並新增檔案mkdir $ZOOKEEPER_HOME/datatee $ZOOKEEPER_HOME/data/myid &lt;&lt; "EOF"1EOF# 佈置到其他台scp -r /usr/local/bigdata/zookeeper tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/zookeeper tester@cassSpark3:/usr/local/bigdatassh tester@cassSpark2 "sed -i -e 's/1/2/g' $ZOOKEEPER_HOME/data/myid"ssh tester@cassSpark2 "sed -i -e 's/1/3/g' $ZOOKEEPER_HOME/data/myid"## Mesos# 修改zookeepersudo tee /etc/mesos/zk &lt;&lt; "EOF"zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesosEOF# 配置quorumsudo tee /etc/mesos-master/quorum &lt;&lt; "EOF"2EOF b. Hadoop設定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184# slavestee $HADOOP_CONF_DIR/slaves &lt;&lt; "EOF"cassSpark1cassSpark2cassSpark3EOF# core-site.xmlsed -i -e 's/&lt;\/configuration&gt;//g' $HADOOP_CONF_DIR/core-site.xmltee -a $HADOOP_CONF_DIR/core-site.xml &lt;&lt; "EOF" &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hc1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/bigdata/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOFmkdir -p $HADOOP_HOME/tmp# hdfs-site.xmlsed -i -e 's/&lt;\/configuration&gt;//g' $HADOOP_CONF_DIR/hdfs-site.xmltee -a $HADOOP_CONF_DIR/hdfs-site.xml &lt;&lt; "EOF" &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/bigdata/hadoop/tmp/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/bigdata/hadoop/tmp/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/bigdata/hadoop/tmp/name/chkpt&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;hc1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://192.168.0.121:8485;192.168.0.122:8485;192.168.0.123:8485/hc1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/bigdata/hadoop/tmp/journal&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.hc1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hc1.nn1&lt;/name&gt; &lt;value&gt;192.168.0.121:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hc1.nn2&lt;/name&gt; &lt;value&gt;192.168.0.122:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hc1.nn1&lt;/name&gt; &lt;value&gt;192.168.0.121:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hc1.nn2&lt;/name&gt; &lt;value&gt;192.168.0.122:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/bigdata/hadoop/tmp/ha-name-dir-shared&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.hc1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence(tester)&lt;/value&gt; &lt;-- 如果不能用root權限登入ssh，記得加上能登入ssh的username --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/tester/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOFmkdir -p $HADOOP_HOME/tmp/datamkdir -p $HADOOP_HOME/tmp/namemkdir -p $HADOOP_HOME/tmp/journalmkdir -p $HADOOP_HOME/tmp/name/chkptmkdir -p $HADOOP_HOME/tmp/ha-name-dir-shared# mapred-site.xmlcp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-site.xmlsed -i -e 's/&lt;\/configuration&gt;//g' $HADOOP_CONF_DIR/mapred-site.xmltee -a $HADOOP_CONF_DIR/mapred-site.xml &lt;&lt; "EOF" &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# yarn-site.xmlsed -i -e 's/&lt;\/configuration&gt;//g' $HADOOP_CONF_DIR/yarn-site.xmltee -a $HADOOP_CONF_DIR/yarn-site.xml &lt;&lt; "EOF" &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-ha&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;cassSpark1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;cassSpark2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 複製到各台scp -r /usr/local/bigdata/hadoop tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/hadoop tester@cassSpark3:/usr/local/bigdata c. HBase設定 123456789101112131415161718192021222324252627282930313233343536373839sed -i -e 's/&lt;\/configuration&gt;//g' $HBASE_HOME/conf/hbase-site.xmltee -a $HBASE_HOME/conf/hbase-site.xml &lt;&lt; "EOF" &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hc1/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.0.121,192.168.0.122,192.168.0.123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;file:///usr/local/bigdata/zookeeper/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;EOF# 複製slavescp $HADOOP_CONF_DIR/slaves $HBASE_HOME/conf/regionservers# 複製到各台scp -r /usr/local/bigdata/hbase tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/hbase tester@cassSpark3:/usr/local/bigdata 啟動 1234567891011121314151617181920212223242526272829303132333435363738# 啟動zookeeper server (設定自動啟動可以跳過)zkServer.sh startssh tester@cassSpark2 "zkServer.sh start"ssh tester@cassSpark3 "zkServer.sh start"# 格式化zkfchdfs zkfc -formatZKssh tester@cassSpark2 "hdfs zkfc -formatZK"# 開啟journalnode$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnodessh tester@cassSpark2 "$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode"ssh tester@cassSpark3 "$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode"# 格式化namenode，並啟動namenode (nn1)hadoop namenode -format hc1$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode# 備用namenode啟動 (nn2)ssh tester@cassSpark2 "hadoop namenode -format hc1"ssh tester@cassSpark2 "hdfs namenode -bootstrapStandby"ssh tester@cassSpark2 "$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode"# 啟動zkfc$HADOOP_HOME/sbin/hadoop-daemon.sh start zkfcssh tester@cassSpark2 "$HADOOP_HOME/sbin/hadoop-daemon.sh start zkfc"# 啟動datanode$HADOOP_HOME/sbin/hadoop-daemon.sh start datanodessh tester@cassSpark2 "$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode"ssh tester@cassSpark3 "$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode"# 啟動yarn$HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanagerssh tester@cassSpark2 "$HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager"ssh tester@cassSpark2 "$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager"ssh tester@cassSpark3 "$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager"# 啟動HBasehbase-daemon.sh start masterhbase-daemon.sh start regionserverssh tester@cassSpark2 "hbase-daemon.sh start master"ssh tester@cassSpark2 "hbase-daemon.sh start regionserver"ssh tester@cassSpark3 "hbase-daemon.sh start master"ssh tester@cassSpark3 "hbase-daemon.sh start regionserver" 開啟之後就可以用jps去看各台開啟狀況，如果確定都沒問題之後 接下來就可以往下去設定自動啟動的部分了，也可以先往下跳去測試部分 這裡採用python的supervisord去協助監控service的進程，並做自動啟動的動作 先安裝supervisor: 123456sudo yum install python-setuptoolssudo easy_install pipsudo pip install supervisor# echo default configsudo mkdir /etc/supervisorsudo bash -c 'echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf' 使用sudo vi /etc/supervisor/supervisord.conf編輯，更動下面的設定： 123456789101112131415161718192021222324252627[inet_http_server] ; inet (TCP) server disabled by defaultport=192.168.0.121:10088 ; (ip_address:port specifier, *:port for all iface)username=tester ; (default is no username (open server))password=qscf12356 ; (default is no password (open server))[supervisorctl]serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socketserverurl=http://192.168.0.121:10088 ; use an http:// url to specify an inet socketusername=tester ; should be same as http_username if setpassword=qscf12356 ; should be same as http_password if set[supervisord]environment= JAVA_HOME=/usr/java/jdk1.8.0_101, SCALA_HOME=/usr/local/scala/scala-2.11, SPARK_HOME=/usr/local/bigdata/spark, CASSANDRA_HOME=/usr/local/bigdata/cassandra, ZOOKEEPER_HOME=/usr/local/bigdata/zookeeper, HADOOP_HOME=/usr/local/bigdata/hadoop, HADOOP_COMMON_HOME=/usr/local/bigdata/hadoop, HADOOP_CONF_DIR=/usr/local/bigdata/hadoop/etc/hadoop, HADOOP_COMMON_LIB_NATIVE_DIR=/usr/local/bigdata/hadoop/lib/native, HADOOP_OPTS=&quot;-Djava.library.path=/usr/local/bigdata/hadoop/lib/native&quot;, HBASE_HOME=/usr/local/bigdata/hbase, HBASE_MANAGES_ZK=&quot;false&quot;, HBASE_CLASSPATH=/usr/local/bigdata/hadoop/etc/hadoop, HBASE_CONF_DIR=/usr/local/bigdata/hbase/conf 然後用下面的指令在設定後面追加下面的設定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106sudo tee -a /etc/supervisor/supervisord.conf &lt;&lt; &quot;EOF&quot;; 全部都要配置的服務[program:hadoop-hdfs-journalnode]command=/bin/bash -c &quot;$HADOOP_HOME/bin/hdfs journalnode&quot;stdout_logfile=/var/log/supervisor/hadoop-hdfs-journalnode-stdout.outstderr_logfile=/var/log/supervisor/hadoop-hdfs-journalnode-stderr.outautostart=truestartsecs=5priority=60[program:mesos-slave]command=/bin/bash -c &quot;/usr/bin/mesos-init-wrapper slave&quot;stdout_logfile=/var/log/supervisor/mesos-slave-stdout.outstderr_logfile=/var/log/supervisor/mesos-slave-stderr.outautostart=truestartsecs=5priority=80[program:hadoop-hdfs-datanode]command=/bin/bash -c &quot;$HADOOP_HOME/bin/hdfs datanode&quot;stdout_logfile=/var/log/supervisor/hadoop-hdfs-datanode-stdout.outstderr_logfile=/var/log/supervisor/hadoop-hdfs-datanode-stderr.outautostart=truestartsecs=5priority=80[program:hadoop-yarn-nodemanager]command=/bin/bash -c &quot;$HADOOP_HOME/bin/yarn nodemanager&quot;stdout_logfile=/var/log/supervisor/hadoop-yarn-nodemanager-stdout.outstderr_logfile=/var/log/supervisor/hadoop-yarn-nodemanager-stderr.outautostart=truestartsecs=5priority=85[program:spark-mesos-shuffle]command=/bin/bash -c &quot;$SPARK_HOME/bin/spark-submit --class org.apache.spark.deploy.mesos.MesosExternalShuffleService 1&quot;stdout_logfile=/var/log/supervisor/spark-mesos-shuffle-stdout.outstderr_logfile=/var/log/supervisor/spark-mesos-shuffle-stderr.outautostart=truestartsecs=5priority=90[program:hbase-regionserver]command=/bin/bash -c &quot;$HBASE_HOME/bin/hbase regionserver start&quot;stdout_logfile=/var/log/supervisor/hbase-regionserver-stdout.outstderr_logfile=/var/log/supervisor/hbase-regionserver-stderr.outautostart=truestartsecs=5priority=95; 下面只有需要的node才要配置; zookeeper只需要配置在三台有zookeeper的電腦上; namenode, zkfc跟hadoop-yarn-resourcemanager只需要配置在要hdfs, yarn做HA的那兩台上面; mesos-master跟hbase-master只需要配置在有zookeeper的電腦上[program:zookeeper]command=/bin/bash -c &quot;$ZOOKEEPER_HOME/bin/zkServer.sh start-foreground&quot;stdout_logfile=/var/log/supervisor/zookeeper-stdout.outstderr_logfile=/var/log/supervisor/zookeeper-stderr.outautostart=truestartsecs=10priority=50[program:mesos-master]command=/bin/bash -c &quot;/usr/bin/mesos-init-wrapper master&quot;stdout_logfile=/var/log/supervisor/mesos-master-stdout.outstderr_logfile=/var/log/supervisor/mesos-master-stderr.outautostart=truestartsecs=5priority=60[program:hadoop-hdfs-namenode]command=/bin/bash -c &quot;$HADOOP_HOME/bin/hdfs namenode&quot;stdout_logfile=/var/log/supervisor/hadoop-hdfs-namenode-stdout.outstderr_logfile=/var/log/supervisor/hadoop-hdfs-namenode-stderr.outautostart=truestartsecs=5priority=70[program:hadoop-hdfs-zkfc]command=/bin/bash -c &quot;$HADOOP_HOME/bin/hdfs zkfc&quot;stdout_logfile=/var/log/supervisor/hadoop-hdfs-zkfc-stdout.outstderr_logfile=/var/log/supervisor/hadoop-hdfs-zkfc-stderr.outautostart=truestartsecs=5priority=75[program:hadoop-yarn-resourcemanager]command=/bin/bash -c &quot;$HADOOP_HOME/bin/yarn resourcemanager&quot;stdout_logfile=/var/log/supervisor/hadoop-yarn-resourcemanager-stdout.outstderr_logfile=/var/log/supervisor/hadoop-yarn-resourcemanager-stderr.outautostart=truestartsecs=5priority=80[program:hbase-master]command=/bin/bash -c &quot;$HBASE_HOME/bin/hbase master start&quot;stdout_logfile=/var/log/supervisor/hbase-master-stdout.outstderr_logfile=/var/log/supervisor/hbase-master-stderr.outautostart=truestartsecs=5priority=90EOFsudo systemctl restart supervisor.service 測試 用網頁連到cassSpark1:50070跟cassSpark2:50070 cassSpark1:50070應該會顯示是active node (或是用 hdfs haadmin -getServiceState nn1) cassSpark2:50070則會顯示是standby node (或是用 hdfs haadmin -getServiceState nn2) (根據啟動順序不同，active的node不一定就是cassSpark1) 在active node上，輸入sudo service hadoop-hdfs-namenode stop停掉Hadoop namenode試試看 (如果安裝了自動啟動，就直接在supervisor的web UI直接操作即可) 等一下下，就可以看到cassSpark2:50070會變成active node，這樣hadoop的HA就完成了 至於YARN的HA就連到8081去看就好(或是用 yarn rmadmin -getServiceState rm1, yarn rmadmin -getServiceState rm2) HBase則是16010，用一樣方式都可以測試到HA是否有成功 至於zookeeper, hadoop其他測試就看我之前發的那篇文章即可點這就好 備註：版本記得改成這裡的2.7.3即可，一定要測試，不然後面出問題很難抓 而且記得要重開，看看是否全部服務都如同預期一樣啟動了 Reference: http://debugo.com/yarn-rm-ha/ http://www.cnblogs.com/junrong624/p/3580477.html http://www.cnblogs.com/captainlucky/p/4710642.html https://phoenix.apache.org/server.html]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark assembly]]></title>
    <url>%2Fposts%2F201609%2F2016-09-15-spark-assembly.html</url>
    <content type="text"><![CDATA[之前一直配置失敗的Spark assembly 今天花了點時間GOOGLE，終於可以成功assembly了 也可以擺脫在Spark設定時候那些Jars檔了 在Intellij開啟專案後，在build.sbt新增如下： 12345678910111213141516171819202122// Spark相關的記得都加上 % "provided" 因為cluster上已經會有對應的jar檔了libraryDependencies ++= Seq( "com.datastax.spark" %% "spark-cassandra-connector" % "2.0.0-M1", "org.apache.spark" %% "spark-core" % "2.0.0" % "provided", "org.apache.spark" %% "spark-sql" % "2.0.0" % "provided", "org.apache.spark" %% "spark-mllib" % "2.0.0" % "provided", "com.databricks" %% "spark-csv" % "1.5.0", "com.github.nscala-time" % "nscala-time_2.11" % "2.14.0")// 避免重複納入，只merge第一個出現的檔案assemblyMergeStrategy in assembly := &#123; case PathList(ps @ _*) if ps.last endsWith ".properties" =&gt; MergeStrategy.first case x =&gt; val oldStrategy = (assemblyMergeStrategy in assembly).value oldStrategy(x)&#125;// 避免出現com.google.guava版本低於1.6.1的錯誤assemblyShadeRules in assembly := Seq( ShadeRule.rename("com.google.**" -&gt; "shadeio.@1").inAll) 並在project下方新增一個檔案assembly.sbt，其內容是： 1addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.3") 接著在src/main/scala-2.11裡面新增檔案cassSpark.scala 程式內容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.net.InetAddress._import com.datastax.spark.connector._import org.apache.spark.sql.&#123;SparkSession, SaveMode&#125;import com.datastax.spark.connector.cql.&#123;CassandraConnector, CassandraConnectorConf&#125;import org.apache.spark.sql.functions._object cassSpark &#123; def main(args: Array[String]) &#123; val cassIps = Set("192.168.0.121", "192.168.0.122", "192.168.0.123") val spark = SparkSession.builder().appName("test") .config("spark.cassandra.connection.host", cassIps.mkString(",")) .config("spark.sql.warehouse.dir", "file:///home/tester").getOrCreate() import spark.implicits._ val cassConf = new CassandraConnectorConf(cassIps.map(getByName(_))) val exeRes = new CassandraConnector(cassConf).withSessionDo &#123; session =&gt; session.execute("CREATE KEYSPACE IF NOT EXISTS test " + "WITH REPLICATION = &#123;'class': 'SimpleStrategy', 'replication_factor': 3 &#125;") session.execute("DROP TABLE IF EXISTS test.testtbl") session.execute("CREATE TABLE test.testtbl (var0 text, var1 text, var2 double, PRIMARY KEY(var0, var1))") session.execute("INSERT INTO test.testtbl(var0, var1, var2) VALUES ('T', 'A', 23.1)") session.execute("INSERT INTO test.testtbl(var0, var1, var2) VALUES ('T', 'B', 17.5)") session.execute("INSERT INTO test.testtbl(var0, var1, var2) VALUES ('U', 'B', 11.9)") session.execute("INSERT INTO test.testtbl(var0, var1, var2) VALUES ('U', 'A', 25.3)") &#125; val collection = spark.sparkContext.parallelize(Seq(("T", "C", 17.0), ("U", "C", 5.0))) collection.saveToCassandra("test", "testtbl", SomeColumns("var0", "var1", "var2")) val cass_tbl = spark.read.format("org.apache.spark.sql.cassandra") .option("keyspace", "test").option("table", "testtbl").load() cass_tbl.write.format("com.databricks.spark.csv").option("header", "true").save("file:///home/tester/test.csv") val concatMap = udf((maps: Seq[Map[String, Double]]) =&gt; maps.flatten.toMap) val cass_tbl_agg = cass_tbl.withColumn("var2_map", map($"var1", $"var2")).groupBy($"var0").agg(concatMap(collect_list($"var2_map")).alias("var2")) try &#123; cass_tbl_agg.createCassandraTable("test", "testtbl_trans") &#125; catch &#123; case e1: com.datastax.driver.core.exceptions.AlreadyExistsException =&gt; None case e2: Exception =&gt; throw e2 &#125; cass_tbl_agg.write.format("org.apache.spark.sql.cassandra").options(Map("table" -&gt; "testtbl_trans", "keyspace" -&gt; "test")).mode(SaveMode.Append).save() &#125;&#125; 最後，新增一個SBT Task，打assembly，然後執行就可以在target/scala-2.11下看到assembly的檔案了 再來是$SPARK_HOME/conf/spark-default.conf的設定 原本設定了spark.jars、spark.executor.extraClassPath、spark.executor.extraLibraryPath spark.driver.extraClassPath以及spark.driver.extraLibraryPath就可以通通先拿掉 然後上傳test_cassSpark-assembly-1.0.jar，在console輸入下面的指令就可以執行了 1spark-submit --master mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos --class cassSpark test_cassSpark-assembly-1.0.jar 最後就可以在/home/tester/test.csv看到產生的csv檔案了]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Spark</tag>
        <tag>assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python的套件supervisor監控程式 - 以Cassandra, Spark, Mesos為例]]></title>
    <url>%2Fposts%2F201609%2F2016-09-14-python-supervisor-monitor.html</url>
    <content type="text"><![CDATA[Python的supervisor是一套簡單、輕量的監控系統服務之工具 透過簡單的安裝跟些許的設定即可以達到想要的效果 先安裝supervisor: 1234567sudo yum install python-setuptoolssudo easy_install pipsudo pip install supervisor# echo default configsudo mkdir /etc/supervisorsudo mkdir /var/log/supervisorsudo bash -c 'echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf' 使用sudo vi /etc/supervisor/supervisord.conf編輯，更動下面的設定(記得針對每一台的IP要做更改)： 1234567891011121314151617[inet_http_server] ; inet (TCP) server disabled by defaultport=192.168.0.121:10088 ; (ip_address:port specifier, *:port for all iface)username=tester ; (default is no username (open server))password=qscf12356 ; (default is no password (open server))[supervisorctl]serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socketserverurl=http://192.168.0.121:10088 ; use an http:// url to specify an inet socketusername=tester ; should be same as http_username if setpassword=qscf12356 ; should be same as http_password if set[supervisord]environment= JAVA_HOME=/usr/java/jdk1.8.0_101, SPARK_HOME=/usr/local/bigdata/spark, ZOOKEEPER_HOME=/usr/local/bigdata/zookeeper, CASSANDRA_HOME=/usr/local/bigdata/cassandra 如果之前有設定過自動啟動，先關掉： 12345678910111213141516sudo systemctl stop mesos-mastersudo systemctl stop mesos-slavesudo systemctl disable mesos-mastersudo systemctl disable mesos-slavesudo service cassandra stopsudo service zookeeper stopsudo service spark-shuffle stopsudo chkconfig --del cassandrasudo chkconfig --del zookeepersudo chkconfig --del spark-shuffle sudo rm /etc/init.d/cassandrasudo rm /etc/init.d/spark-shuffle sudo rm /etc/init.d/zookeeper 最後是使用下面的script去配置supervisord： 12345678910111213141516171819202122232425262728293031323334353637383940414243sudo tee -a /etc/supervisor/supervisord.conf &lt;&lt; "EOF"[program:cassandra]command=/bin/bash -c "$CASSANDRA_HOME/bin/cassandra -f"stdout_logfile=/var/log/supervisor/cassandra-stdout.outstderr_logfile=/var/log/supervisor/cassandra-stderr.outautostart=truestartsecs=5priority=50[program:mesos-slave]command=/bin/bash -c "/usr/bin/mesos-init-wrapper slave"stdout_logfile=/var/log/supervisor/mesos-slave-stdout.outstderr_logfile=/var/log/supervisor/mesos-slave-stderr.outautostart=truestartsecs=5priority=80[program:spark-mesos-shuffle]command=/bin/bash -c "$SPARK_HOME/bin/spark-submit --class org.apache.spark.deploy.mesos.MesosExternalShuffleService 1"stdout_logfile=/var/log/supervisor/spark-mesos-shuffle-stdout.outstderr_logfile=/var/log/supervisor/spark-mesos-shuffle-stderr.outautostart=truestartsecs=5priority=90; 下面只有需要的node才要配置[program:zookeeper]command=/bin/bash -c "$ZOOKEEPER_HOME/bin/zkServer.sh start-foreground"stdout_logfile=/var/log/supervisor/zookeeper-stdout.outstderr_logfile=/var/log/supervisor/zookeeper-stderr.outautostart=truestartsecs=10priority=50[program:mesos-master]command=/bin/bash -c "/usr/bin/mesos-init-wrapper master"stdout_logfile=/var/log/supervisor/mesos-master-stdout.outstderr_logfile=/var/log/supervisor/mesos-master-stderr.outautostart=truestartsecs=5priority=60EOF 再來是配置supervisor的自動啟動服務 123456789101112131415161718192021sudo tee -a /usr/lib/systemd/system/supervisor.service &lt;&lt; "EOF"[Unit]Description=supervisorAfter=network.target[Service]Type=forkingExecStart=/usr/bin/supervisord -c /etc/supervisor/supervisord.confExecStop=/usr/bin/supervisorctl $OPTIONS shutdownExecReload=/usr/bin/supervisorctl $OPTIONS reloadKillMode=processRestart=on-failureRestartSec=42s[Install]WantedBy=multi-user.targetEOFsudo systemctl daemon-reloadsudo systemctl start supervisor.servicesudo systemctl enable supervisor.service 最後使用設定的IP跟PORT，連上該網址，像是我的設定是連http://192.168.0.121:10088 就可以看到可以手動操作的部分了]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Spark</tag>
        <tag>Mesos</tag>
        <tag>supervisor</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparklyr extensions]]></title>
    <url>%2Fposts%2F201609%2F2016-09-10-sparkrlyr-extensions.html</url>
    <content type="text"><![CDATA[稍微介紹一下sparklyr的extension寫法 但是細節都還在研究，只是環境的配置跟使用官方的extension套件而已 環境只有一點要注意，sparklyr只會找下面三個地方有沒有scala-2.10或是scala-2.11 /opt/local/scala /usr/local/scala /opt/scala 所以根據我之前的配置就需要重新做調整 使用下面的指令做調整的動作即可 12sudo mkdir /usr/local/scalasudo mv /usr/local/bigdata/scala /usr/local/scala/scala-2.11 接著run下面的script 123456789101112131415161718192021222324252627282930313233343536library(sparklyr)# 下載sparkhello套件if (!dir.exists("sparkhello")) git2r::clone("https://github.com/jjallaire/sparkhello.git", "sparkhello")# 移除prebuild的jar檔file.remove("sparkhello/inst/java/sparkhello-1.6-2.10.jar")file.remove("sparkhello/inst/java/sparkhello-2.0-2.11.jar")# 工作目錄設定在套件rootsetwd("sparkhello")# 指定jar名稱jar_name &lt;- sparklyr:::infer_active_package_name() %&gt;&gt;% paste0("-2.0-2.11.jar")# 編譯jarcompile_package_jars(spark_version = "2.0.0", spark_home = Sys.getenv("SPARK_HOME"), scalac_path = find_scalac("2.11"), jar_name = jar_name)# build套件devtools::build()# install套件devtools::install()# 工作目錄回到上一層setwd("..")# 切斷之前的連線，這點很重要，不然用之前的連線的話# 不會把套件中的jar檔load進去spark_disconnect_all()# library套件library(sparkhello)# create spark connectionspark_master &lt;- "mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos"sc &lt;- spark_connect(master = spark_master, config = spark_config("config.yml", FALSE))# 執行spark_hello(sc)# [1] "Hello, world! - From Scala" 以上就是sparklyr的extension部分 很可惜的是這個範例沒有講到怎麼傳參數到scala裡面 也不知道能不能設定一些函數，output到R裡面使用]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>BigData</tag>
        <tag>Spark</tag>
        <tag>sparkly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkR初探]]></title>
    <url>%2Fposts%2F201609%2F2016-09-09-sparkr.html</url>
    <content type="text"><![CDATA[雖然順序有點反了，先介紹了sparklyr 這裡就簡單show一下怎麼用SparkR，並且去拉Cassandra的table R跟rstudio server就不贅述了 1234567891011121314151617181920212223242526# 讓R能夠找到SparkR套件Sys.setenv(SPARK_HOME = "/usr/local/bigdata/spark").libPaths(c(.libPaths(), paste0(Sys.getenv("SPARK_HOME"), "/R/lib")))library(SparkR)library(pipeR)library(stringr)# 讀取spark設定spark_settings &lt;- readLines(paste0(Sys.getenv("SPARK_HOME"), "/conf/spark-defaults.conf")) %&gt;&gt;% `[`(!str_detect(., "^#")) %&gt;&gt;% `[`(nchar(.) &gt; 0) %&gt;&gt;% str_split("\\s") %&gt;&gt;% &#123; `names&lt;-`(lapply(., `[`, 2), sapply(., `[`, 1)) &#125; %&gt;&gt;% c(list(spark.cassandra.connection.host = "192.168.0.121,192.168.0.122,192.168.0.123"))# 開spark sessionsc &lt;- sparkR.session(master = spark_master, sparkJars = spark_settings$spark.jars %&gt;&gt;% str_split(",") %&gt;&gt;% `[[`(1), sparkConfig = spark_settings)# 讀入Cassandra表cass_tbl &lt;- read.df(NULL, source = "org.apache.spark.sql.cassandra", keyspace = "test", table = "kv2") # 拉回localcass_tbl_r &lt;- as.data.frame(cass_tbl)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>BigData</tag>
        <tag>Spark</tag>
        <tag>SparkR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparklyr初探，並連接Cassandra使用]]></title>
    <url>%2Fposts%2F201609%2F2016-09-08-sparklyr-and-use-it-with-cassandra.html</url>
    <content type="text"><![CDATA[RStudio推出了一個感覺很厲害的套件sparklyr 可以讓dplyr變得lazy，然後去即時的操作Spark中的dataFrame 先從安裝R, RStudio Server開始： 123456789101112131415161718192021222324252627282930313233curl -v -j -k -L https://mran.microsoft.com/install/mro/3.3.1/microsoft-r-open-3.3.1.tar.gz -o microsoft-r-open-3.3.1.tar.gztar -zxvf microsoft-r-open-3.3.1.tar.gzcd microsoft-r-open/rpmsu -c 'rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm'# for offline installationsudo yum install R R-devel R-java --downloadonly --downloaddir=r_depssudo yum install openssl-devel libcurl-devel --downloadonly --downloaddir=r_depssudo yum install *.rpm --downloadonly --downloaddir=r_deps# install R, R-devel and R-java and depssudo yum install -y r_deps/*.rpm# remove Rsudo rm -rf /usr/lib64/Rsudo rm -rf /usr/bin/Rsudo rm -rf /usr/bin/Rscript# install MRO and MKLsudo yum install -y *.rpmsudo chown -R tester:tester /usr/lib64/microsoft-r/3.3/lib64/Rsudo chmod -R 775 /usr/lib64/microsoft-r/3.3/lib64/R# for offline installationsudo chown -R tester:tester r_depsmv r_deps ~/# 這裡下載preview版本，是為了可以直接有UI操作sparkcurl -v -j -k -L https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-1.0.8-x86_64.rpm -o rstudio-server-rhel-1.0.8-x86_64.rpmsudo yum install --nogpgcheck rstudio-server-rhel-1.0.8-x86_64.rpm## start the rstudio-serversudo rstudio-server start## start rstudio-server on bootsudo cp /usr/lib/rstudio-server/extras/init.d/redhat/rstudio-server /etc/init.d/sudo chmod 755 /etc/init.d/rstudio-serversudo chkconfig --add rstudio-server 為了測試，我先用scala在spark-shell上塞了一些資料進去 12345678910111213141516171819202122232425262728293031323334353637383940414243444546spark.stop()import java.net.InetAddress._import com.datastax.spark.connector._import org.apache.spark.sql.&#123;SparkSession, SaveMode&#125;import com.datastax.spark.connector.cql.&#123;CassandraConnector, CassandraConnectorConf&#125;import org.apache.spark.sql.functions._val cassIps = Set("192.168.0.121", "192.168.0.122", "192.168.0.123")val spark = SparkSession.builder().appName("test") .config("spark.cassandra.connection.host", cassIps.mkString(",")) .config("spark.sql.warehouse.dir", "file:///home/tester").getOrCreate() import spark.implicits._val cassConf = new CassandraConnectorConf(cassIps.map(getByName(_)))val exeRes = new CassandraConnector(cassConf).withSessionDo &#123; session =&gt; session.execute("CREATE KEYSPACE IF NOT EXISTS test " + "WITH REPLICATION = &#123;'class': 'SimpleStrategy', 'replication_factor': 3 &#125;") session.execute("DROP TABLE IF EXISTS test.kv2") session.execute("CREATE TABLE test.kv2 (var0 text, var1 text, var2 double, PRIMARY KEY(var0, var1))") session.execute("INSERT INTO test.kv2(var0, var1, var2) VALUES ('T', 'A', 23.1)") session.execute("INSERT INTO test.kv2(var0, var1, var2) VALUES ('T', 'B', 17.5)") session.execute("INSERT INTO test.kv2(var0, var1, var2) VALUES ('U', 'B', 11.9)") session.execute("INSERT INTO test.kv2(var0, var1, var2) VALUES ('U', 'A', 25.3)")&#125;val collection = spark.sparkContext.parallelize(Seq(("T", "C", 17.0), ("U", "C", 5.0)))collection.saveToCassandra("test", "kv2", SomeColumns("var0", "var1", "var2"))val cass_tbl = spark.read.format("org.apache.spark.sql.cassandra") .option("keyspace", "test").option("table", "kv2").load()val concatMap = udf((maps: Seq[Map[String, Double]]) =&gt; maps.flatten.toMap)val cass_tbl_agg = cass_tbl.withColumn("var2_map", map($"var1", $"var2")).groupBy($"var0").agg(concatMap(collect_list($"var2_map")).alias("var2"))try &#123; cass_tbl_agg.createCassandraTable("test", "kv2_trans")&#125; catch &#123; case e1: com.datastax.driver.core.exceptions.AlreadyExistsException =&gt; None case e2: Exception =&gt; throw e2&#125;cass_tbl_agg.write.format("org.apache.spark.sql.cassandra").options(Map("table" -&gt; "kv2_trans", "keyspace" -&gt; "test")).mode(SaveMode.Append).save() 裡面的test.kv2會長下面這樣： var0 var1 var2 T A 23.1 T B 17.5 T C 17 U A 25.3 U B 11.9 U C 5 裡面的test.kv2_trans會長下面這樣： | var0 | var2 ||——+———————————|| T | {‘A’: 23.1, ‘B’: 17.5, ‘C’: 17} || U | {‘A’: 25.3, ‘B’: 11.9, ‘C’: 5} | 可以看到test.kv2是一般長相的table test.kv2_trans則是包含了Map這個資料格式 接下來安裝sparklyr 12install.packages("devtools")devtools::install_github("rstudio/sparklyr") 如果照我之前文章跟我設定一樣Spark的路徑的話 下面的R script就可以直接執行看到結果了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160# 設定spark home的位置Sys.setenv(SPARK_HOME = "/usr/local/bigdata/spark")# library sparklyr跟dplyrlibrary(sparklyr)library(dplyr)# library pipeR跟stringrlibrary(pipeR)library(stringr)# 讀取spark的設定spark_settings &lt;- readLines(paste0(Sys.getenv("SPARK_HOME"), "/conf/spark-defaults.conf")) %&gt;&gt;% `[`(!str_detect(., "^#")) %&gt;&gt;% `[`(nchar(.) &gt; 0) %&gt;&gt;% str_split("\\s")# 複製sparklyr的預設config檔案file.copy(system.file(file.path("conf", "config-template.yml"), package = "sparklyr"), "config.yml", TRUE)# 把spark的設定寫進config去sink("config.yml", TRUE)sapply(spark_settings %&gt;&gt;% sapply(paste, collapse = ": "), function(x) cat(" ", x, "\n")) %&gt;&gt;% invisiblesink()# 開啟spark connectionspark_master &lt;- "mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos"sc &lt;- spark_connect(master = spark_master, config = spark_config("config.yml", FALSE))# 開啟spark sessionsparkSession &lt;- invoke_static(sc2, "org.apache.spark.sql.SparkSession", "builder") %&gt;&gt;% invoke("config", "spark.cassandra.connection.host", "192.168.0.121,192.168.0.122,192.168.0.123") %&gt;&gt;% invoke("getOrCreate")# 讀取Cassandra table，可以看到讀進來是Dataset，還是一個java的物件cass_df &lt;- invoke(sparkSession, "read") %&gt;&gt;% invoke("format", "org.apache.spark.sql.cassandra") %&gt;&gt;% invoke("option", "keyspace", "test") %&gt;&gt;% invoke("option", "table", "kv2") %&gt;&gt;% invoke("load") %&gt;&gt;% (~ print(cass_df))# &lt;jobj[32]&gt;# class org.apache.spark.sql.Dataset# [var0: string, var1: string ... 1 more field] # register Spark dataframe，註冊後，就可以在R裡面看到部分資料，並且可以使用dplyrcass_df_tbl &lt;- sparklyr:::spark_partition_register_df(sc2, cass_df, "test_kv2", 0, TRUE)print(cass_df_tbl)# Source: query [?? x 3]# Database: spark connection master=mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos app=sparklyr local=FALSE# # var0 var1 var2# &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;# 1 T A 23.1# 2 T B 17.5# 3 T C 17.0# 4 U A 25.3# 5 U B 11.9# 6 U C 5.0# 測試一下filter，可以發現結果還 ?? x 3，因為dplyr這裡還是lazy，還沒做回收的動作cass_df_tbl %&gt;&gt;% filter(var2 &gt; 5)# Source: query [?? x 3]# Database: spark connection master=mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos app=sparklyr local=FALSE# # var0 var1 var2# &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;# 1 T A 23.1# 2 T B 17.5# 3 T C 17.0# 4 U A 25.3# 5 U B 11.9# 使用as.data.frame就會變成local了cass_df_tbl %&gt;&gt;% filter(var2 &gt; 5) %&gt;&gt;% as.data.frame# var0 var1 var2# 1 T A 23.1# 2 T B 17.5# 3 T C 17.0# 4 U A 25.3# 5 U B 11.9# 這裡也可以使用很多org.apache.spark.sql.functions中的函數# 最簡單的如abs, min, max, sin, cos, sqrt等# 到比較少見的如map, collect_list, unix_timestamp等# 下面示範把column在spark裡面bind成mapcass_df_tbl_agg &lt;- cass_df_tbl %&gt;&gt;% mutate(var2_map = map(var1, var2)) %&gt;&gt;% group_by(var0) %&gt;&gt;% summarise(var2 = collect_list(var2_map)) %&gt;&gt;% as.data.frame # print data.frame，看不到map的keyprint(cass_df_tbl_agg)# var0 var2# 1 T 23.1, 17.5, 17.0# 2 U 25.3, 11.9, 5.0# 用str看，可以看到key還是有被留下來# 只是要在R裡面再做一點處理而已，這部分就越過str(cass_df_tbl_agg)'data.frame': 2 obs. of 2 variables: $ var0: chr "T" "U" $ var2:List of 2 ..$ :List of 3 .. ..$ :List of 1 .. .. ..$ A: num 23.1 .. ..$ :List of 1 .. .. ..$ B: num 17.5 .. ..$ :List of 1 .. .. ..$ C: num 17 ..$ :List of 3 .. ..$ :List of 1 .. .. ..$ A: num 25.3 .. ..$ :List of 1 .. .. ..$ B: num 11.9 .. ..$ :List of 1 .. .. ..$ C: num 5 # 再來是直接從Cassandra讀入那張含有map欄位的表，一樣是Datasetcass_df_2 &lt;- invoke(sparkSession, "read") %&gt;&gt;% invoke("format", "org.apache.spark.sql.cassandra") %&gt;&gt;% invoke("option", "keyspace", "test") %&gt;&gt;% invoke("option", "table", "kv2_trans") %&gt;&gt;% invoke("load") %&gt;&gt;% (~print(.))# &lt;jobj[51]&gt;# class org.apache.spark.sql.Dataset# [var0: string, var2: map&lt;string,double&gt;]# 一樣做register的動作cass_df_tbl_2 &lt;- sparklyr:::spark_partition_register_df(sc2, cass_df_2, "test_kv2_trans", 0, TRUE)# print出來看看，是變成list了，只是不知道有沒有抓到keyprint(cass_df_tbl_2)# Source: query [?? x 2]# Database: spark connection master=mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos app=sparklyr local=FALSE# # var0 var2# &lt;chr&gt; &lt;list&gt;# 1 T &lt;list [3]&gt;# 2 U &lt;list [3]&gt;# 拉回local瞧瞧，可以看到變成很漂亮的formatas.data.frame(cass_df_tbl_2) %&gt;&gt;% str# 'data.frame': 2 obs. of 2 variables:# $ var0: chr "T" "U"# $ var2:List of 2# ..$ :List of 3# .. ..$ A: num 23.1# .. ..$ B: num 17.5# .. ..$ C: num 17# ..$ :List of 3# .. ..$ A: num 25.3# .. ..$ B: num 11.9# .. ..$ C: num 5# 這裡拉回local，用data.table稍微整一下就可以回到test.kv2的樣子了cass_df_tbl_2_r &lt;- as.data.frame(cass_df_tbl_2) %&gt;&gt;% data.table %&gt;&gt;% `[`( , `:=`(var2_v = lapply(var2, unlist), var2_k = lapply(var2, names))) %&gt;&gt;% `[`( , .(var2_value = unlist(var2_v), var2_key = unlist(var2_k)), by = "var0") %&gt;&gt;% (~ print(.))# var0 var2_value var2_key# 1: T 23.1 A# 2: T 17.5 B# 3: T 17.0 C# 4: U 25.3 A# 5: U 11.9 B# 6: U 5.0 C 以上是sparklyr的初探，並嘗試Cassandra的結果 一些小心得： 沒有辦法直接udf，無法直接套用R函數是致命傷 幾乎都要去翻spark.sql.functions的document 能用的操作少之又少，又懶得去寫extension 希望這部分能趕快有enhancement，滿期待這個出現Github issue]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Spark</tag>
        <tag>sparkly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[simple file server in centos]]></title>
    <url>%2Fposts%2F201609%2F2016-09-07-simple-file-server-centos.html</url>
    <content type="text"><![CDATA[這篇主要是講在centos做一個簡單的file server 123456789101112# 安裝httpdsudo yum install httpd# create sharing foldersudo mkdir /var/www/html/sharesudo chown tester:tester /var/www/html/share# link to ~/shareln -s /var/www/html/share /home/tester/share# 啟動sudo service httpd start 接下來只要把檔案放進/home/tester/html/share 然後連到那台電腦的IP下面的share就可以看到file server了 像我電腦就是連到192.168.0.121:80/share]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>File Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on Mesos: dynamic resource allocation]]></title>
    <url>%2Fposts%2F201608%2F2016-08-26-spark-on-mesos-dynamic-resource-allocation.html</url>
    <content type="text"><![CDATA[上篇部署了Spark on Mesos的環境 而這篇主要是想要使用Spark on Mesos的dynamic resource allocation跟external shuffle service Dynamic resource allocation是為了能夠讓Spark能夠更有效的使用系統資源的系統 能夠動態的去增加worker以利application的運行，並能realease不在使用中的executor 而這個功能原本只有在Spark on Yarn的配置上才有，2.0.0的Spark也在Mesos上實現支援了 在external shuffle Service啟動下，它會把executors寫入的shuffle檔案安全的移除 這個服務的啟動是為了去讓Spark能夠使用dynamic resource allocation 同時，他也會協助Executor去分配shuffle的資料，以增加shuffle的效率，減少executor的loading 準備工作 基本上就是有一組Spark On Mesos的cluster，並有Cassandra 開始配置 這裡只要重新配置Spark即可 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 移除舊的 (有的話)rm -r $SPARK_HOME# 下載新的binary包curl -v -j -k -L http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.6.tgz -o spark-2.0.0-bin-hadoop2.6.tgztar -zxvf spark-2.0.0-bin-hadoop2.6.tgzmv spark-2.0.0-bin-hadoop2.6 /usr/local/bigdata/spark# 複製檔案cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.shcp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.confcp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.properties# additional jars (這兩個的取得在前幾篇都有說，有用到再放)mkdir $SPARK_HOME/extraClasscp spark-cassandra-connector-assembly-2.0.0-M1.jar $SPARK_HOME/extraClass/cp ojdbc7.jar $SPARK_HOME/extraClass/# 傳入設定tee -a $SPARK_HOME/conf/spark-env.sh &lt;&lt; "EOF"SPARK_LOCAL_DIRS=/usr/local/bigdata/sparkSPARK_SCALA_VERSION=2.11MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.soEOFtee -a $SPARK_HOME/conf/spark-defaults.conf &lt;&lt; "EOF"spark.driver.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar:/usr/local/bigdata/spark/extraClass/ojdbc7.jarspark.driver.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.executor.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar:/usr/local/bigdata/spark/extraClass/ojdbc7.jarspark.executor.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.jars /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar,/usr/local/bigdata/spark/extraClass/ojdbc7.jarspark.scheduler.mode fairspark.driver.cores 1spark.driver.memory 2gspark.cores.max 6spark.executor.cores 1spark.executor.memory 2gspark.dynamicAllocation.enabled truespark.dynamicAllocation.initialExecutors 6spark.executor.instances 6spark.dynamicAllocation.executorIdleTimeout 15spark.shuffle.service.enabled trueEOF# 複製到其他台ssh tester@cassSpar2 "rm -r $SPARK_HOME"ssh tester@cassSpar3 "rm -r $SPARK_HOME"scp -r /usr/local/bigdata/spark tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/spark tester@cassSpark3:/usr/local/bigdata 設定自動啟動shuffle service 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081sudo tee /etc/init.d/spark-shuffle &lt;&lt; "EOF"#!/bin/bash## spark-shuffle# # chkconfig: 2345 89 9 # description: spark-shufflesource /etc/rc.d/init.d/functions# Spark install path (where you extracted the tarball)SPARK_HOME=/usr/local/bigdata/sparkRETVAL=0PIDFILE=/var/run/spark-shuffle.piddesc="spark-shuffle daemon"start() &#123; echo -n $"Starting $desc (spark-shuffle): " daemon $SPARK_HOME/sbin/start-mesos-shuffle-service.sh RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/spark-shuffle return $RETVAL&#125;stop() &#123; echo -n $"Stopping $desc (spark-shuffle): " daemon $SPARK_HOME/sbin/stop-mesos-shuffle-service.sh RETVAL=$? sleep 5 echo [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/spark-shuffle $PIDFILE&#125;restart() &#123; stop start&#125;get_pid() &#123; cat "$PIDFILE"&#125;checkstatus()&#123; status -p $PIDFILE $&#123;JAVA_HOME&#125;/bin/java RETVAL=$?&#125;condrestart()&#123; [ -e /var/lock/subsys/spark-shuffle ] &amp;&amp; restart || :&#125;case "$1" in start) start ;; stop) stop ;; status) checkstatus ;; restart) restart ;; condrestart) condrestart ;; *) echo $"Usage: $0 &#123;start|stop|status|restart|condrestart&#125;" exit 1esacexit $RETVALEOF# 然後使用下面指令讓這個script能夠自動跑：sudo chmod +x /etc/init.d/spark-shufflesudo chkconfig --add spark-shufflesudo service spark-shuffle start 試試看用下面指令submit任務 1spark-submit --master mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos --class cassSpark test_cassspark_2.11-1.0.jar 下方的指令可以用來找出現在的Mesos master 1mesos-resolve `cat /etc/mesos/zk` 執行完後會出現你配置zookeeper的其中一個IP with port 5050 例如我的Mesos master ip是192.168.0.121:5050，那麼我連上這個網址就會看到現在任務狀況 用spark-submit後，在Frameworks那個分頁會看到一個framework被啟動 接著spark會開始動態的規畫所需的executor進行MapReduce的job 運行的時候會在Mesos的主頁看到Task被起起來做運行的動作 最後是關於刪掉framework的方式，可以在unix系統上使用下方指令去刪除framework： 12# framework id可以在Framework那頁找到，按copy id就可以複製下來，然後貼到命令裡面來發出刪除的動作curl -XPOST http://192.168.0.121:5050/master/teardown -d 'frameworkId=2444f6a3-1bfb-47d6-8b11-ab9c8f56e3c9-0000' 我這裡還是覺得為什麼mesos的webUI會沒有提供直接kill的命令感到疑惑]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Spark</tag>
        <tag>Mesos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署Spark on Mesos and Cassandra環境]]></title>
    <url>%2Fposts%2F201608%2F2016-08-23-deployment-of-mesos-spark.html</url>
    <content type="text"><![CDATA[本篇主要在部署Spark on Mesos的環境 目標是Spark跟Mesos的master配上兩台Mesos standby(同時為Zookeeper) cassSpark1為Mesos master跟Spark master，cassSpark2以及cassSpark3為mesos standby 這三台同時也是Mesos slaves跟Spark slaves (實際用途中，會是其他電腦，這裡用VM就都放一起了) 準備工作 這裡基本上跟前篇一樣，就不贅述了 開始部署 i. 下載檔案並移到適當位置 123456789101112131415161718192021222324252627# 建立放置資料夾sudo mkdir /usr/local/bigdatasudo chown -R tester /usr/local/bigdata# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u101-b13/jdk-8u101-linux-x64.rpm -o jdk-8u101-linux-x64.rpmsudo yum install -y jdk-8u101-linux-x64.rpm# 下載並部署zookeepercurl -v -j -k -L http://apache.stu.edu.tw/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz -o zookeeper-3.4.8.tar.gztar -zxvf zookeeper-3.4.8.tar.gzsudo mv zookeeper-3.4.8 /usr/local/bigdata/zookeepersudo chown -R tester /usr/local/bigdata/zookeeper# 下載並部署mesoscurl -v -j -k -L http://repos.mesosphere.com/el/7/x86_64/RPMS/mesos-1.0.0-2.0.89.centos701406.x86_64.rpm -o mesos-1.0.0-2.0.89.centos701406.x86_64.rpmsudo yum install mesos-1.0.0-2.0.89.centos701406.x86_64.rpm# 下載並部署scalacurl -v -j -k -L http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz -o scala-2.11.8.tgztar -zxvf scala-2.11.8.tgzmv scala-2.11.8 /usr/local/bigdata/scala# 下載並部署sparkcurl -v -j -k -L http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.6.tgz -o spark-2.0.0-bin-hadoop2.6.tgztar -zxvf spark-2.0.0-bin-hadoop2.6.tgzmv spark-2.0.0-bin-hadoop2.6 /usr/local/bigdata/spark# 下載並部署cassandracurl -v -j -k -L http://apache.stu.edu.tw/cassandra/2.2.7/apache-cassandra-2.2.7-bin.tar.gz -o apache-cassandra-2.2.7-bin.tar.gztar -zxvf apache-cassandra-2.2.7-bin.tar.gzmv apache-cassandra-2.2.7 /usr/local/bigdata/cassandra ii. 環境變數設置 12345678910111213sudo tee -a /etc/bashrc &lt;&lt; "EOF"# JAVAexport JAVA_HOME=/usr/java/jdk1.8.0_101# ZOOKEEPERexport ZOOKEEPER_HOME=/usr/local/bigdata/zookeeper# SCALAexport SCALA_HOME=/usr/local/bigdata/scala# SPARKexport SPARK_HOME=/usr/local/bigdata/spark# CASSANDRAexport CASSANDRA_HOME=/usr/local/bigdata/cassandra# PATHexport PATH=$PATH:$JAVA_HOME:$ZOOKEEPER_HOME/bin:$SPARK_HOME/bin:$CASSANDRA_HOME/bin iv. 配置Zookeeper 123456789101112131415# 複製zoo.cfgcp $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg# 傳入設定tee $ZOOKEEPER_HOME/conf/zoo.cfg &lt;&lt; "EOF"dataDir=/usr/local/bigdata/zookeeper/dataserver.1=cassSpark1:2888:3888server.2=cassSpark2:2888:3888server.3=cassSpark3:2888:3888EOF# 接著創立需要的資料夾，並新增檔案mkdir $ZOOKEEPER_HOME/datatee $ZOOKEEPER_HOME/data/myid &lt;&lt; "EOF"1EOF 在cassSpark2跟cassSpark3分別設定為2跟3。 啟動zookeeper: 1234# 啟動zookeeper serverzkServer.sh startssh tester@cassSpark2 "zkServer.sh start"ssh tester@cassSpark3 "zkServer.sh start" 再來是測試看看是否有部署成功，先輸入zkCli.sh -server cassSpark1:2181,cassSpark2:2181,cassSpark3:2181可以登錄到zookeeper的server上，如果是正常運作會看到下面的訊息： 1[zk: cassSpark1:2181,cassSpark2:2181,cassSpark3:2181(CONNECTED) 0] 此時試著輸入看看create /test01 abcd，然後輸入ls /看看是否會出現[test01, zookeeper] 如果是，zookeeper就是設定成功，如果中間有出現任何錯誤，則否 最後用delete /test01做刪除即可，然後用quit離開。 最後是設定開機自動啟動zookeeper server: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182sudo tee /etc/init.d/zookeeper &lt;&lt; "EOF"#!/bin/bash## ZooKeeper# # chkconfig: - 80 99# description: zookeeper# ZooKeeper install path (where you extracted the tarball)ZOOKEEPER=/usr/local/bigdata/zookeepersource /etc/rc.d/init.d/functionssource $ZOOKEEPER/bin/zkEnv.shRETVAL=0PIDFILE=/var/run/zookeeper_server.piddesc="ZooKeeper daemon"start() &#123; echo -n $"Starting $desc (zookeeper): " daemon $ZOOKEEPER/bin/zkServer.sh start RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/zookeeper return $RETVAL&#125;stop() &#123; echo -n $"Stopping $desc (zookeeper): " daemon $ZOOKEEPER/bin/zkServer.sh stop RETVAL=$? sleep 5 echo [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/zookeeper $PIDFILE&#125;restart() &#123; stop start&#125;get_pid() &#123; cat "$PIDFILE"&#125;checkstatus()&#123; status -p $PIDFILE $&#123;JAVA_HOME&#125;/bin/java RETVAL=$?&#125;condrestart()&#123; [ -e /var/lock/subsys/zookeeper ] &amp;&amp; restart || :&#125;case "$1" in start) start ;; stop) stop ;; status) checkstatus ;; restart) restart ;; condrestart) condrestart ;; *) echo $"Usage: $0 &#123;start|stop|status|restart|condrestart&#125;" exit 1esacexit $RETVALEOF# 然後使用下面指令讓這個script能夠自動跑：sudo chmod +x /etc/init.d/zookeepersudo chkconfig --add zookeepersudo service zookeeper start v. 配置mesos 下面的動作在cassSpark1, cassSpark2, cassSpark3這三台都要配置 123456789# 修改zookeepersudo tee /etc/mesos/zk &lt;&lt; "EOF"zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesosEOF# 配置quorumsudo tee /etc/mesos-master/quorum &lt;&lt; "EOF"2EOF ssh-copy-id -i ~/.ssh/id_rsa.pub cassSpark3 再來就是啟動了 12345678910# 記得先啟動zookeeper server，再啟動mesos# master要記得關掉slave (這裡的配置下不用關，指令供人參考用)sudo systemctl disable mesos-mastersudo systemctl stop mesos-mastersudo service mesos-master restart# 在cassSpark2,cassSpark3上啟動slave# slave要記得關掉master (這裡的配置下不用關，指令供人參考用)sudo systemctl disable mesos-slavesudo systemctl stop mesos-slavesudo service mesos-slave restart iii. 配置scala and spark 123456789101112131415161718192021222324252627282930313233343536# 複製檔案cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.shcp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.propertiescp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf# 傳入設定tee -a $SPARK_HOME/conf/spark-env.sh &lt;&lt; "EOF"SPARK_LOCAL_DIRS=/usr/local/bigdata/sparkSPARK_SCALA_VERSION=2.11EOF# install sbt and git sudo yum install sbt git-core# clone spark-cassandra-connectorgit clone git@github.com:datastax/spark-cassandra-connector.git# compile assembly jarcd spark-cassandra-connectorrm -r spark-cassandra-connector-perfsbt -Dscala-2.11=true assembly# copy jar to sparkmkdir $SPARK_HOME/extraClasscp spark-cassandra-connector/target/scala-2.11/spark-cassandra-connector-assembly-2.0.0-M1-2-g70018a6.jar $SPARK_HOME/extraClasstee -a $SPARK_HOME/conf/spark-defaults.conf &lt;&lt; "EOF"spark.driver.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarspark.driver.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.executor.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarspark.executor.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.jars /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarspark.cores.max 3spark.driver.memory 4gspark.executor.memory 4gEOF Spark的master, slave就不用啟動了，直接用Mesos即可 如果之前用我的方式配置過自動啟動的話，請用下面的指令移除： 12345678# 移除master的服務sudo systemctl stop spark-master.servicesudo rm /etc/systemd/system/multi-user.target.wants/spark-master.service# 移除slave的服務sudo systemctl stop spark-slave.servicesudo rm /etc/systemd/system/multi-user.target.wants/spark-slave.service# 重新讀取開機的服務sudo systemctl daemon-reload 至於cassandra的設置就都一樣，此處就不贅述，直接進測試 執行下面兩行，成功執行就是Mesos有裝成功 12MASTER=$(mesos-resolve `cat /etc/mesos/zk`)mesos-execute --master=$MASTER --name="cluster-test" --command="sleep 5" Mesos也可以透過去連接5050 port到目前的master上去，有出現網頁就是正常 再來測試Spark，用spark-shell --master mesos://zk://192.168.0.121:2181,192.168.0.122:2181,192.168.0.123:2181/mesos開啟spark-shell確定功能正常 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 單純測試sparkval NUM_SAMPLES = 1000000val count = sc.parallelize(1 to NUM_SAMPLES).map&#123;i =&gt; val x = Math.random() val y = Math.random() if (x*x + y*y &lt; 1) 1 else 0&#125;.reduce(_ + _)println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)// 測試與cassandra的connector// 重啟scsc.stop()// imprt需要套件import com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConf, com.datastax.spark.connector.cql.CassandraConnector// 設定cassandra hostval conf = new SparkConf(true).set("spark.cassandra.connection.host", "192.168.0.121")val sc = new SparkContext(conf)// 創立keyspace, table，然後塞值CassandraConnector(conf).withSessionDo &#123; session =&gt; session.execute("CREATE KEYSPACE IF NOT EXISTS test WITH REPLICATION = &#123;'class': 'SimpleStrategy', 'replication_factor': 2 &#125;") session.execute("DROP TABLE IF EXISTS test.kv") session.execute("CREATE TABLE test.kv (key text PRIMARY KEY, value DOUBLE)") session.execute("INSERT INTO test.kv(key, value) VALUES ('key1', 1.0)") session.execute("INSERT INTO test.kv(key, value) VALUES ('key2', 2.5)")&#125;// 印出cassandra的元素val rdd = sc.cassandraTable("test", "kv")println(rdd.first)// 會出現CassandraRow&#123;key: key1, value: 1.0&#125;val collection = sc.parallelize(Seq(("key3", 1.7), ("key4", 3.5)))// save new data into cassandracollection.saveToCassandra("test", "kv", SomeColumns("key", "value"))// 印出塞完的結果rdd.collect().foreach(row =&gt; println(s"Existing Data: $row"))// Existing Data: CassandraRow&#123;key: key3, value: 1.7&#125;// Existing Data: CassandraRow&#123;key: key1, value: 1.0&#125;// Existing Data: CassandraRow&#123;key: key4, value: 3.5&#125;// Existing Data: CassandraRow&#123;key: key2, value: 2.5&#125;// 停止spark-shell，然後離開sc.stop():quit 備註： 如果cluster不能對外連線的話，curl可以取得的檔案都先經由能夠連線的電腦取得 至於Mesos的dependencies，先在能夠對外連線的centos電腦上下 sudo yum install --downloadonly --downloaddir=pkgs mesos-1.0.0-2.0.89.centos701406.x86_64.rpm 這樣就會把要下載rpm檔案全部都載下來到pkgs的資料夾，這些在打包傳到cluster上 然後用sudo yum install *.rpm安裝，在安裝Mesos即可]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Spark</tag>
        <tag>Mesos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用scala透過sparkSQL去搬移Oracle DB的資料到Cassandra上]]></title>
    <url>%2Fposts%2F201608%2F2016-08-13-use-scala-migrate-oracle-data-to-cassandra-through-sparksql.html</url>
    <content type="text"><![CDATA[這篇主要有兩個目的： 幫ROracle澄清其實沒那麼難用，只是要把table name跟column name都轉成大寫，就不會有double quote了 在scala用sparkSQL連ojdbc7，把Oracle資料拉出來，再透過spark-cassandra-connector把資料倒進Cassandra ROracle安裝部分請參考這篇 在server使用$ORACLE_HOME/bin/sqlplus system/password@oracleServer:1521/orcl登入 透過sqlplus在Oracle server上創新的user，其SQL如下： 12345678910111213141516171819CREATE TABLESPACE testuser DATAFILE 'testuser.dat' SIZE 40M REUSE AUTOEXTEND ON;CREATE TEMPORARY TABLESPACE testuser_tmp TEMPFILE 'testuser_tmp.dbf' SIZE 10M REUSE AUTOEXTEND ON; CREATE USER C##testuserIDENTIFIED BY testuserDEFAULT TABLESPACE testuserTEMPORARY TABLESPACE testuser_tmpquota unlimited on testuser;GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW, CREATE PROCEDURETO C##testuser; 我們再透過R去塞一個夠大的資料到Oracle上去，其R code如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647library(rpace) # my package (there is a big data.frame)library(fasttime)library(ROracle)library(pipeR)library(data.table)Sys.setenv(TZ = "GMT", ORA_SDTZ = "GMT")tmpData &lt;- TaiwanFreeway5vdData %&gt;&gt;% data.table %&gt;&gt;% `[`( , time := fastPOSIXct(sprintf("%i-%02i-%02i %02i:%02i:00", year, month, day, hour, minute))) %&gt;&gt;% setnames(toupper(names(.)))print(c(nrow(tmpData), ncol(tmpData)))# [1] 998177 12# 連線資訊host &lt;- "192.168.0.120"port &lt;- 1521sid &lt;- "orcl"connectString &lt;- paste( "(DESCRIPTION=", "(ADDRESS=(PROTOCOL=TCP)(HOST=", host, ")(PORT=", port, "))", "(CONNECT_DATA=(SID=", sid, ")))", sep = "")# 先用system權限登入查看usercon &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString)# query all_usersuserList &lt;- dbSendQuery(con, "select * from ALL_USERS") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.tableprint(userList)# 印出表格，user按照創造時間排列# 可以看到已經創造了C##TESTUSER這個user# system帳號斷線dbDisconnect(con)# 用C##TESTUSER去上傳，這樣才會傳到C##TESTUSER的schema下con &lt;- dbConnect(dbDriver("Oracle"), username = "c##testuser", password = "testuser", dbname = connectString)# 把上傳的tabledbWriteTable(con, "VDDATA", as.data.frame(tmpData), row.names = FALSE)# 帳號斷線dbDisconnect(con) 在server上query看看結果： 接下來就是用scala寫一個程式去拉資料 build.sbt的部分： 1234567891011name := "oracle2cassandra_sparksql"version := "1.0"scalaVersion := "2.11.8"libraryDependencies += "org.apache.spark" %% "spark-core" % "2.0.0"libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.0.0"libraryDependencies += "com.datastax.spark" %% "spark-cassandra-connector" % "2.0.0-M1" scala code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.net.InetAddressimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.cassandra._import com.datastax.spark.connector._import com.datastax.spark.connector.cql.&#123;CassandraConnector, CassandraConnectorConf&#125;object ora2cass &#123; def main(args: Array[String]) &#123; /* test args val args = List("C##TESTUSER", "VDDATA", "testuser", "vddata") */ // connection information of Oracle DB val oraConnInfo = "jdbc:oracle:thin:system/qscf12356@//192.168.0.120:1521/orcl" val oraSchema = args(0).toUpperCase() val oraTable = args(1).toUpperCase() val cassKeyspace = args(2).toLowerCase() val cassTable = args(3).toLowerCase() // create SQL to grap data from Oracle DB val oraSQL = s"(SELECT ROWIDTOCHAR(t.ROWID) ID,t.ORA_ROWSCN,t.* FROM $oraSchema.$oraTable t) tmp" // connection information of Cassandra val cassIps = List("192.168.0.121", "192.168.0.122", "192.168.0.123") .map(InetAddress.getByName(_)).toSet // spark session val spark = SparkSession.builder().master("local") .appName("oracle to cassandra") .config("spark.cassandra.connection.host", "192.168.0.121") .config("spark.sql.warehouse.dir", "file:///d://") .getOrCreate() // read data from Oracle DB val jdbcDF = spark.read.format("jdbc") .options(Map("url" -&gt; oraConnInfo, "dbtable" -&gt; oraSQL)) .load() // create connection to Cassandra val cassConf = new CassandraConnectorConf(cassIps) // create keyspace and table // get all ks: val allcassKS = session.getCluster().getMetadata().getKeyspaces() val cassCreate = new CassandraConnector(cassConf).withSessionDo &#123; session =&gt; session.execute(s"CREATE KEYSPACE IF NOT EXISTS $cassKeyspace " + "WITH REPLICATION = &#123;'class': 'SimpleStrategy', 'replication_factor': 2 &#125;") session.execute(s"DROP TABLE IF EXISTS $cassKeyspace.$cassTable") &#125; // create table in Cassandra jdbcDF.createCassandraTable(cassKeyspace, cassTable) // save the RDD in cassandra jdbcDF.write.cassandraFormat(cassTable, cassKeyspace).save() &#125;&#125; 然後在Intellij用Application執行，記得Program arguments要給參數，本地端就可以運行成功了 如果要在遠端伺服器跑的話，master跟spark.sql.warehouse.dir記得修改成相對應的位置 然後把ojdbc7.jar放進server，用 12345678910cp ~/ojdbc7.jar $SPARK_HOME/extraClasscp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conftee -a $SPARK_HOME/conf/spark-defaults.conf &lt;&lt; "EOF"spark.driver.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar:/usr/local/bigdata/spark/extraClass/ojdbc7.jarspark.driver.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.executor.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar:/usr/local/bigdata/spark/extraClass/ojdbc7.jarspark.executor.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.jars /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jar,/usr/local/bigdata/spark/extraClass/ojdbc7.jarEOF 重開spark，再用spark-submit： 1spark-submit --class ora2cass oracle2cassandra_sparksql_2.11-1.0.jar C##TESTUSER VDDATA testuser vddata]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Oracle</tag>
        <tag>Spark</tag>
        <tag>Mesos</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基於cassandra的spark 2.0.0環境部署 (scala 2.11)]]></title>
    <url>%2Fposts%2F201608%2F2016-08-12-deployment-of-spark-2_0_0-based-on-cassandra.html</url>
    <content type="text"><![CDATA[spark升級到2.0.0，等了幾天 spark-cassandra-connector終於升級到2.0.0-M1 就直接來嘗試重新裝一次 準備工作 這裡基本上跟前篇一樣，就不贅述了 開始部署 i. 下載檔案並移到適當位置 1234567891011121314151617181920212223sudo mkdir /usr/local/bigdatasudo chown -R tester /usr/local/bigdata# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u101-b13/jdk-8u101-linux-x64.rpm -o jdk-8u101-linux-x64.rpmsudo yum install -y jdk-8u101-linux-x64.rpm# 下載並部署scalacurl -v -j -k -L http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz -o scala-2.11.8.tgztar -zxvf scala-2.11.8.tgzsudo mkdir /usr/local/scalasudo mv scala-2.11.8 /usr/local/scala/scala-2.11# 下載並部署sparkcurl -v -j -k -L http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.6.tgz -o spark-2.0.0-bin-hadoop2.6.tgztar -zxvf spark-2.0.0-bin-hadoop2.6.tgzmv spark-2.0.0-bin-hadoop2.6 /usr/local/bigdata/spark# 下載並部署cassandracurl -v -j -k -L http://apache.stu.edu.tw/cassandra/2.2.7/apache-cassandra-2.2.7-bin.tar.gz -o apache-cassandra-2.2.7-bin.tar.gztar -zxvf apache-cassandra-2.2.7-bin.tar.gzmv apache-cassandra-2.2.7 /usr/local/bigdata/cassandra# 如果不是VM，實體可以一台下載之後用scp# scp -r /usr/local/bigdata/* tester@cassSpark2:/usr/local/bigdata# scp -r /usr/local/bigdata/* tester@cassSpark3:/usr/local/bigdata ii. 環境變數設置 12345678910111213sudo tee -a /etc/bashrc &lt;&lt; "EOF"# JAVAexport JAVA_HOME=/usr/java/jdk1.8.0_101# SCALAexport SCALA_HOME=/usr/local/scala/scala-2.11# SPARKexport SPARK_HOME=/usr/local/bigdata/spark# CASSANDRAexport CASSANDRA_HOME=/usr/local/bigdata/cassandra# PATHexport PATH=$PATH:$JAVA_HOME:$SPARK_HOME/bin:$SPARK_HOME/sbin:$CASSANDRA_HOME/binEOFsource /etc/bashrc iii. 配置scala and spark 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647tee $SPARK_HOME/conf/slaves &lt;&lt; "EOF"cassSpark1cassSpark2cassSpark3EOF# 複製檔案cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.shcp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.propertiescp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf# 傳入設定tee -a $SPARK_HOME/conf/spark-env.sh &lt;&lt; "EOF"SPARK_MASTER_IP=cassSpark1SPARK_LOCAL_DIRS=/usr/local/bigdata/sparkSPARK_SCALA_VERSION=2.11SPARK_DRIVER_MEMORY=3GSPARK_WORKER_CORES=2SPARK_WORKER_MEMORY=3gEOF# install sbt and git sudo yum install sbt git-core# clone spark-cassandra-connectorgit clone git@github.com:datastax/spark-cassandra-connector.git# compile assembly jarcd spark-cassandra-connectorrm -r spark-cassandra-connector-perfsbt -Dscala-2.11=true assembly# copy jar to sparkmkdir $SPARK_HOME/extraClasscp spark-cassandra-connector/target/scala-2.11/spark-cassandra-connector-assembly-2.0.0-M1-2-g70018a6.jar $SPARK_HOME/extraClasstee -a $SPARK_HOME/conf/spark-defaults.conf &lt;&lt; "EOF"spark.driver.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarspark.driver.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.executor.extraClassPath /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarspark.executor.extraLibraryPath /usr/local/bigdata/spark/extraClassspark.jars /usr/local/bigdata/spark/extraClass/spark-cassandra-connector-assembly-2.0.0-M1.jarEOF# 複製到各台scp -r /usr/local/bigdata/spark tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/spark tester@cassSpark3:/usr/local/bigdata slaves的部署、cassandra的設置跟自動啟動部分就都一樣，此處也跳過，直接進測試 測試 用cqlsh cassSpark1登入CQL介面 用下面指令去創測試資料： 123456CREATE KEYSPACE test WITH replication = &#123;'class': 'SimpleStrategy', 'replication_factor': 3 &#125;;CREATE TABLE test.kv(key text PRIMARY KEY, value int);INSERT INTO test.kv(key, value) VALUES ('key1', 1);INSERT INTO test.kv(key, value) VALUES ('key2', 2);INSERT INTO test.kv(key, value) VALUES ('key3', 11);INSERT INTO test.kv(key, value) VALUES ('key4', 25); 然後用spark-shell打開spark的shell 12345678910111213sc.stop()import com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConfval conf = new SparkConf(true).set("spark.cassandra.connection.host", "192.168.0.121")val sc = new SparkContext(conf)val rdd = sc.cassandraTable("test", "kv")println(rdd.first)// CassandraRow&#123;key: key4, value: 25&#125;println(rdd.map(_.getInt("value")).sum) // 39.0val collection = sc.parallelize(Seq(("key5", 38), ("key6", 5)))// save new data into cassandracollection.saveToCassandra("test", "kv", SomeColumns("key", "value"))sc.stop():quit 用cqlsh cassSpark1登入CQL，去看剛剛塞進去的資料 12345678910USE test;select * from kv;# key | value# -------+-------# key5 | 38# key6 | 5# key1 | 1# key4 | 25# key3 | 11# key2 | 2 根據我自己的測試，要用spark-submit才能在remote cluster上run Intellij的application不能直接跑，所以先用intellij的SBT Task: package (scala SDK用2.11.8) 會在專案目錄/target/scala-2.11下面產生一個jar檔，我這裡是test_cassspark_2.11-1.0.jar 把這個jar上傳到cluster上，然後用下面指令就可以成功運行： 1spark-submit --class cassSpark test_cassspark_2.11-1.0.jar Ref https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md https://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Spark</tag>
        <tag>Mesos</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Nexus建立本地maven倉庫]]></title>
    <url>%2Fposts%2F201608%2F2016-08-10-nexus-local-maven-repository.html</url>
    <content type="text"><![CDATA[sbt每次撈maven跟sbt相關的套件時 都會花很多時間，如果能夠透過本地proxy去降低時間就好了 或是在公司內部網路無法access到外部網路時 就能夠透過proxy去處理這類問題 此時，簡單易用的Nexus就提供很好的協助 安裝很簡單，只需要到官網下載 然後電腦裝有Java就可以跑了 我自己是下載2.13的版本，下載之後，解壓縮放到C:\下面 此處路徑假設是C:\nexus-2.13.0-01，按開始搜尋cmd，對cmd.exe點右鍵 以系統管理員身分開啟，鍵入cd C:\nexus-2.13.0-01\bin 接著，打nexus.bat install進行安裝，再用nexus.bat start運行Nexus 在瀏覽器上打http:\\localhost:8081\nexus就可以成功進入到configuration的網頁了 右上角log in，帳號密碼為admin/admin123 至於設置方式，這部分也很簡單 在左邊Views/Repositories下面的Repositories裡面 按下Add...新增Proxy Repository Repository ID/Repository Name都隨便你打 Remote Storage Location就輸入你要的倉庫 這裡列舉一些我用到的： http://repo2.maven.org/maven2/ http://repository.jboss.com/maven2/ http://dl.bintray.com/sbt/sbt-plugin-releases/ http://repo.typesafe.com/typesafe/ivy-releases/ 建好之後，要為ivy另外建一個Repository Group 一樣是Add..然後Repository Group，Key上ID跟Name 把typesafe跟sbt那兩個放進去 至於repo2跟maven2放到public跟maven的central repo一起即可 最後，要在本地端使用，就要到~/.sbt下，去新增/修改repositories 內容是： 1234567[repositories] local my-ivy-proxy-releases: http://localhost:8081/nexus/content/groups/ivy/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext] my-maven-proxy-releases: http://localhost:8081/nexus/content/groups/public Reference: http://www.coderblog.cn/server/building-maven-and-sbt-repository-using-nexus-3.0.0/ http://lab.howie.tw/2012/06/prepare-for-build-automation-install.html http://shzhangji.com/blog/2014/11/07/sbt-offline/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Maven</tag>
        <tag>sbt</tag>
        <tag>Spark</tag>
        <tag>proxy</tag>
        <tag>Nexus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基於Cassandra的spark環境部署]]></title>
    <url>%2Fposts%2F201608%2F2016-08-05-deployment-of-spark-based-on-cassandra.html</url>
    <content type="text"><![CDATA[之前的部屬是base on hadoop所建立的spark環境 這一篇會從頭建立基於cassandra的spark環境 一樣是透過VM安裝minimal system of centos 再部署好其中一台做為spark的master，將該台的映像檔做clone，變成slaves Cassandra則部署到全部的節點上 準備工作 安裝好VMware，然後新增一台VM (網路連接方式使用bridged即可)，引進centos 7.2安裝映像檔 安裝centos的時候先將一些設定configure好，之後就不需要用小黑框慢慢config了 先設置網路，第一個分頁的自動連線打勾、第四頁的IPv4選Manual，填上IP、Net mask、Gateway跟DNS，接著點進硬碟確定配置，然後設定時區(你可以不設定，就用美國紐約) 按下一步安裝，此時可以新增使用者跟設置root密碼，我這裡新增一個使用者tester 安裝後重開機後，先幫你自己的使用者帳號開權限： 1234su # 切換到rootvisudo # 打開設定檔# 打/root\tALL找到這行 root ALL=(ALL) ALL# 在下面新增 tester ALL=(ALL) ALL 而ssh已經有了，就不用安裝了，直接產生key： 1234567891011# 產生SSH Keyssh-keygen -t rsa -P ""# 授權SSH Keycat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 設定.ssh的權限sudo chmod 700 /home/testersudo chmod 700 /home/tester/.sshsudo chmod 644 /home/tester/.ssh/authorized_keyssudo chmod 600 /home/tester/.ssh/id_rsasudo service sshd restart 再來是讓電腦認得自己，並讓其他電腦認得自己 echo &quot;cassSpark1&quot; | sudo tee /etc/hostname (每台電腦用不同名字) 12345sudo tee -a /etc/hosts &lt;&lt; "EOF"192.168.0.121 cassSpark1192.168.0.122 cassSpark2192.168.0.123 cassSpark3EOF 最後是斷掉防火牆 12sudo systemctl stop firewalldsudo systemctl disable firewalld 然後用sudo reboot重開機 開始部署 Note: spark-cassandra-connector 1.6是用spark 1.6.1, scala 2.10.5編譯出來的 所以我這版本就用一樣的，避免版本不同，沒有align情況下出現methodNotFound的exception i. 下載檔案並移到適當位置 1234567891011121314151617181920212223sudo mkdir /usr/local/bigdatasudo chown -R tester /usr/local/bigdata# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u101-b13/jdk-8u101-linux-x64.rpm -o jdk-8u101-linux-x64.rpmsudo yum install -y jdk-8u101-linux-x64.rpm# 下載並部署scalacurl -v -j -k -L http://downloads.lightbend.com/scala/2.10.5/scala-2.10.5.tgz -o scala-2.10.5.tgztar -zxvf scala-2.10.5.tgzsudo mkdir /usr/local/scalasudo mv scala-2.10.5 /usr/local/scala/scala-2.10# 下載並部署sparkcurl -v -j -k -L http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz -o spark-1.6.1-bin-hadoop2.6.tgztar -zxvf spark-1.6.1-bin-hadoop2.6.tgzmv spark-1.6.1-bin-hadoop2.6 /usr/local/bigdata/spark# 下載並部署cassandracurl -v -j -k -L http://apache.stu.edu.tw/cassandra/2.2.7/apache-cassandra-2.2.7-bin.tar.gz -o apache-cassandra-2.2.7-bin.tar.gztar -zxvf apache-cassandra-2.2.7-bin.tar.gzmv apache-cassandra-2.2.7 /usr/local/bigdata/cassandra# 如果不是VM，實體可以一台下載之後用scp# scp -r /usr/local/bigdata/* tester@cassSpark2:/usr/local/bigdata# scp -r /usr/local/bigdata/* tester@cassSpark3:/usr/local/bigdata ii. 環境變數設置 12345678910111213sudo tee -a /etc/bashrc &lt;&lt; "EOF"# JAVAexport JAVA_HOME=/usr/java/jdk1.8.0_101# SCALAexport SCALA_HOME=/usr/local/scala/scala-2.10# SPARKexport SPARK_HOME=/usr/local/bigdata/spark# CASSANDRAexport CASSANDRA_HOME=/usr/local/bigdata/cassandra# PATHexport PATH=$PATH:$JAVA_HOME:$SPARK_HOME/bin:$SPARK_HOME/sbin:$CASSANDRA_HOME/binEOFsource /etc/bashrc iii. 配置scala and spark 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 複製hadoop的slavestee $SPARK_HOME/conf/slaves &lt;&lt; "EOF"cassSpark1cassSpark2cassSpark3EOF# 複製檔案cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.shcp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.propertiescp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf# 傳入設定tee -a $SPARK_HOME/conf/spark-env.sh &lt;&lt; "EOF"SPARK_MASTER_IP=cassSpark1SPARK_LOCAL_DIRS=/usr/local/bigdata/sparkSPARK_DRIVER_MEMORY=3GSPARK_WORKER_CORES=2SPARK_WORKER_MEMORY=3gEOF# download dependencies of spark-cassandra-connectormkdir $SPARK_HOME/extraClasscurl -v -j -k -L http://search.maven.org/remotecontent?filepath=org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar -o ivy-2.4.0.jarcurl -v -j -k -L http://search.maven.org/remotecontent?filepath=org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar -o ivy-2.4.0.jarcurl -v -j -k -L http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector_2.10/1.6.0/spark-cassandra-connector_2.10-1.6.0.jar -o spark-cassandra-connector_2.10-1.6.0.jar# remove spark-related jarrm spark-catalyst_2.10-1.6.1.jarrm spark-core_2.10-1.6.1.jarrm spark-hive_2.10-1.6.1.jarrm spark-launcher_2.10-1.6.1.jarrm spark-network-common_2.10-1.6.1.jarrm spark-network-shuffle_2.10-1.6.1.jarrm spark-sql_2.10-1.6.1.jarrm spark-streaming_2.10-1.6.1.jarrm spark-unsafe_2.10-1.6.1.jarrm datanucleus-*java -jar ivy-2.3.0.jar -dependency com.datastax.cassandra spark-cassandra-connector_2.10 1.6.0 -retrieve "[artifact]-[revision](-[classifier]).[ext]"rm -f *-&#123;sources,javadoc&#125;.jar# add jars to spark-defaults.confjarsToImport=$(echo $SPARK_HOME/extraClass/*.jar | sed 's/ /:/g')echo "spark.executor.extraClassPath $jarsToImport" &gt;&gt; $SPARK_HOME/conf/spark-defaults.confecho "spark.driver.extraClassPath $jarsToImport" &gt;&gt; $SPARK_HOME/conf/spark-defaults.confecho "spark.driver.extraLibraryPath $SPARK_HOME/extraClass" &gt;&gt; $SPARK_HOME/conf/spark-defaults.confecho "spark.executor.extraLibraryPath $SPARK_HOME/extraClass" &gt;&gt; $SPARK_HOME/conf/spark-defaults.confjarsToImport2=$(echo $SPARK_HOME/extraClass/*.jar | sed 's/ /,/g')echo "spark.jars $jarsToImport2" &gt;&gt; $SPARK_HOME/conf/spark-defaults.conf# 複製到各台scp -r /usr/local/bigdata/spark tester@cassSpark2:/usr/local/bigdatascp -r /usr/local/bigdata/spark tester@cassSpark3:/usr/local/bigdata iv. slaves的部署 因為是VM，所以剩下的就是把映像檔clone複製成各個nodes，然後針對需要個別配置的地方做配置： 123456789101112131415# 改hostnamesudo vi /etc/hostname# 改網路設定sudo vi /etc/sysconfig/network-scripts/ifcfg-eno16777736# 配置玩各台電腦，並透過`sudo service network restart`重啟網路服務後# 生成新的SSH keyssh-keygen -t rsa -P ""# 在sparkServer0，把他的ssh key傳到各台電腦去tee run.sh &lt;&lt; "EOF"#!/bin/bashfor hostname in `cat $SPARK_HOME/conf/slaves`; do ssh-copy-id -i ~/.ssh/id_rsa.pub $hostnamedoneEOFbash ./run.sh v. 設置Cassandra 使用vi $CASSANDRA_HOME/conf/cassandra.yaml去改設定檔，改的部分如下： 1234567891011121314151617# first place:cluster_name: 'cassSparkServer'# second place:seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: "192.168.0.121,192.168.0.122,192.168.0.123" # third place:listen_address: 192.168.0.121# fourth place:rpc_address: 192.168.0.121# fifth place:endpoint_snitch: GossipingPropertyFileSnitch 設置多台的做法： 1234scp -rp $CASSANDRA_HOME/* tester@cassSpark2:/usr/local/bigdata/cassandrascp -rp $CASSANDRA_HOME/* tester@cassSpark3:/usr/local/bigdata/cassandrassh tester@cassSpark2 &quot;sed -i -e &apos;s/: 192.168.0.121/: 192.168.0.122/g&apos; /usr/local/bigdata/cassandra/conf/cassandra.yaml&quot;ssh tester@cassSpark3 &quot;sed -i -e &apos;s/: 192.168.0.121/: 192.168.0.123/g&apos; /usr/local/bigdata/cassandra/conf/cassandra.yaml&quot; vi. 設置Cassandra自動開機啟動 開機自動啟動Cassandra的script(用sudo vi /etc/init.d/cassandra去create)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/bin/bash# chkconfig: - 79 01# description: Cassandra. /etc/rc.d/init.d/functionsCASSANDRA_HOME=/usr/local/bigdata/cassandraCASSANDRA_BIN=$CASSANDRA_HOME/bin/cassandraCASSANDRA_NODETOOL=$CASSANDRA_HOME/bin/nodetoolCASSANDRA_LOG=$CASSANDRA_HOME/logs/cassandra.logCASSANDRA_PID=/var/run/cassandra.pidCASSANDRA_LOCK=/var/lock/subsys/cassandraPROGRAM="cassandra"if [ ! -f $CASSANDRA_BIN ]; then echo "File not found: $CASSANDRA_BIN" exit 1fiRETVAL=0start() &#123; if [ -f $CASSANDRA_PID ] &amp;&amp; checkpid `cat $CASSANDRA_PID`; then echo "Cassandra is already running." exit 0 fi echo -n $"Starting $PROGRAM: " daemon $CASSANDRA_BIN -p $CASSANDRA_PID &gt;&gt; $CASSANDRA_LOG 2&gt;&amp;1 usleep 500000 RETVAL=$? if [ $RETVAL -eq 0 ]; then touch $CASSANDRA_LOCK echo_success else echo_failure fi echo return $RETVAL&#125;stop() &#123; if [ ! -f $CASSANDRA_PID ]; then echo "Cassandra is already stopped." exit 0 fi echo -n $"Stopping $PROGRAM: " $CASSANDRA_NODETOOL -h 127.0.0.1 decommission if kill `cat $CASSANDRA_PID`; then RETVAL=0 rm -f $CASSANDRA_LOCK echo_success else RETVAL=1 echo_failure fi echo [ $RETVAL = 0 ]&#125;status_fn() &#123; if [ -f $CASSANDRA_PID ] &amp;&amp; checkpid `cat $CASSANDRA_PID`; then echo "Cassandra is running." exit 0 else echo "Cassandra is stopped." exit 1 fi&#125;case "$1" in start) start ;; stop) stop ;; status) status_fn ;; restart) stop start ;; *) echo $"Usage: $PROGRAM &#123;start|stop|restart|status&#125;" RETVAL=3esacexit $RETVAL 然後使用下面指令讓這個script能夠自動跑： 123sudo chmod +x /etc/init.d/cassandrasudo chkconfig --add cassandrasudo service cassandra start 接著用nodetool status可以確定一下是不是都有跑起來，顯示資訊如下： 123456789nodetool status# Datacenter: dc1# ===============# Status=Up/Down# |/ State=Normal/Leaving/Joining/Moving# -- Address Load Tokens Owns (effective) Host ID Rack# UN 192.168.0.121 202.17 KB 256 66.7% 265daa7e-8039-46f4-842e-4255ec18be19 rack1# UN 192.168.0.122 185.65 KB 256 66.4% 0fed9978-0097-417c-b0a8-dcd7be2b2c10 rack1# UN 192.168.0.123 207.79 KB 256 66.9% cf7b47b3-5d50-444e-99b4-843f9a5dadb2 rack1 vii. 設置spark自動開機 在spark的master (你的User跟Group那裏記得要修改): 1234567891011121314151617181920sudo tee -a /etc/systemd/system/multi-user.target.wants/spark-master.service &lt;&lt; &quot;EOF&quot;[Unit]Description=Spark MasterAfter=network.target [Service]Type=forkingUser=testerGroup=testerExecStart=/usr/local/bigdata/spark/sbin/start-master.shStandardOutput=journalStandardError=journalLimitNOFILE=infinityLimitMEMLOCK=infinityLimitNPROC=infinityLimitAS=infinity [Install]WantedBy=multi-user.targetEOF 在spark的master跟slaves: 12345678910111213141516171819202122sudo tee -a /etc/systemd/system/multi-user.target.wants/spark-slave.service &lt;&lt; &quot;EOF&quot;[Unit]Description=Spark SlaveAfter=network.target [Service]Type=forkingUser=testerGroup=testerExecStart=/usr/local/bigdata/spark/sbin/start-slave.sh spark://cassSpark1:7077StandardOutput=journalStandardError=journalLimitNOFILE=infinityLimitMEMLOCK=infinityLimitNPROC=infinityLimitAS=infinityCPUAccounting=trueCPUShares=100[Install]WantedBy=multi-user.targetEOF 然後全部node執行sudo systemctl daemon-reload master執行sudo systemctl start spark-master.service master跟slave都執行sudo systemctl start spark-slave.service 測試 用cqlsh cassSpark1登入CQL介面 用下面指令去創測試資料： 123456CREATE KEYSPACE test WITH replication = &#123;'class': 'SimpleStrategy', 'replication_factor': 3 &#125;;CREATE TABLE test.kv(key text PRIMARY KEY, value int);INSERT INTO test.kv(key, value) VALUES ('key1', 1);INSERT INTO test.kv(key, value) VALUES ('key2', 2);INSERT INTO test.kv(key, value) VALUES ('key3', 11);INSERT INTO test.kv(key, value) VALUES ('key4', 25); 然後用spark-shell打開spark的shell 12345678910111213sc.stop()import com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConfval conf = new SparkConf(true).set("spark.cassandra.connection.host", "192.168.0.121")val sc = new SparkContext(conf)val rdd = sc.cassandraTable("test", "kv")println(rdd.first)// CassandraRow&#123;key: key4, value: 25&#125;println(rdd.map(_.getInt("value")).sum) // 39.0val collection = sc.parallelize(Seq(("key5", 38), ("key6", 5)))// save new data into cassandracollection.saveToCassandra("test", "kv", SomeColumns("key", "value"))sc.stop():quit 用cqlsh cassSpark1登入CQL，去看剛剛塞進去的資料 12345678910USE test;select * from kv;# key | value# -------+-------# key5 | 38# key6 | 5# key1 | 1# key4 | 25# key3 | 11# key2 | 2 根據我自己的測試，要用spark-submit才能在remote cluster上run Intellij的application不能直接跑，所以先用intellij的SBT Task: package 會在專案目錄/target/scala-2.10下面產生一個jar檔，我這裡是test_cassspark_2.10-1.0.jar 把這個jar上傳到cluster上，然後用下面指令就可以成功運行： 1spark-submit --class cassSpark test_cassspark_2.10-1.0.jar --master spark://192.168.0.121:7077 Ref https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md https://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python將Oracle DB的資料匯出到Cassandra]]></title>
    <url>%2Fposts%2F201608%2F2016-08-01-export-oracle-data-to-cassandra-using-python.html</url>
    <content type="text"><![CDATA[基本上的工具在前幾篇都安裝完 這一篇主要目的是把Oracle資料庫的資料匯出到Cassandra中 原本這部分要用sqoop做，結果發現沒辦法使用 只好用Python自己做輪子，於是這篇就誕生了 準備工作 要有Oracle DB, Cassandra cluster跟安裝好Python本機 我的架構是： 5台VM，每一台都是4G RAM，兩核心 1台是Oracle DB，其他四台是Cassandra Cluster 本機是四核心八執行緒，32GB的電腦，所以全部電腦在跑，本機會有點喘XDD Python程式 上傳、刪除Oracle DB的部分就參考cx_Oracle那篇 這篇程式主要是做了一些type mapping、cassandra上傳加速的測試 最後發現使用execute_concurrent_with_args可以比正常用for + execute快上7倍 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#!/usr/bin/pythonimport sys, cx_Oracle, datetime, timeimport cassandra.clusterimport cassandra.concurrentfrom itertools import isliceclass TooManyArgsError(Exception): """Err type for too many arguments.""" pass# import sys, getoptdef main(argv): # information for connecting to oracle oracleLoginInfo = u'system/qscf12356@192.168.0.120:1521/orcl' # define the number of fetching once = the number of rows uploaded to cassandra numFetch = 5000 # cassandra cluster ips cassandraIpAddrs = [u'192.168.0.161', u'192.168.0.162', \ u'192.168.0.163', u'192.168.0.164']; # create a dict to map the data type of python to the one in Cassandra pythonCassTypeMap = dict([["str", "text"], ["int", "int"], \ ["float", "double"], ["datetime", "timestamp"]]) #%% input check if len(argv) is 0: tableListFN = 'dataTable.csv' print """The default filename of the table list is '%s'.""" % tableListFN elif len(argv) &gt; 1: raise TooManyArgsError('Too many input arguments!') else: tableListFN = argv[0] #%% read input file # print log print 'The filename you input is %s.' % tableListFN # read the title row of csv fieldnames = None with open(tableListFN) as csvfile: firstRow = csvfile.readlines(1) fieldnames = tuple(firstRow[0].strip('\n').split(",")) # read the rows of csv tableList = list() with open(tableListFN) as csvfile: for row in islice(csvfile, 1, None): values = [elem.upper() for elem in row.strip('\n').split(",")] tableList.append(dict(zip(fieldnames, values))) if len(tableList) is 0: print 'There is no data in %s.' % tableListFN for row in tableList: print 'Now is upload the dataset %s.%s...' % \ (row['oracleSchema'].lower(), row['oracleTblName'].lower()) #%% import data from Oracle database # create connection oracleConn = cx_Oracle.connect(oracleLoginInfo) # activate cursor oracleCursor = oracleConn.cursor() # make sql oracleSql = "select rowid,t.* from %s.%s t" % \ (row['oracleSchema'], row['oracleTblName']) # execute sql query oracleCursor.execute(oracleSql) #%% start to fetch rows and upload to cassandra # connect to cassandra casCon = cassandra.cluster.Cluster(cassandraIpAddrs) casSession = casCon.connect() # create keyspace if it does not exist casSession.execute("""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = &#123;'class': 'SimpleStrategy', 'replication_factor': '2'&#125; """ % row['cassandraKeyspace']) # set to keyspace casSession.set_keyspace(row['cassandraKeyspace'].lower()) #%% get the oracle data and check the data type oracleRows = oracleCursor.fetchmany(numFetch) # get data type colDataType = [type(r).__name__ for r in oracleRows[0]] # find the mapping data type in cassandra cassType = [pythonCassTypeMap[dataType] for dataType in colDataType] # acquire the column names in Oracle DB oracleColNames = [x[0].lower() for x in oracleCursor.description] # create table if it does not exist casSession.execute('''CREATE TABLE IF NOT EXISTS &#123;&#125;.&#123;&#125; (&#123;&#125;, PRIMARY KEY (rowid))'''.format(\ row['cassandraKeyspace'].lower(), row['cassandraTblName'].lower(),\ ','.join([x + ' ' + y for (x,y) in zip(oracleColNames, cassType)]))) #%% upload data to cassandra insertCQL = """INSERT INTO &#123;&#125;.&#123;&#125; (&#123;&#125;) VALUES (&#123;&#125;)""".format(\ row['cassandraKeyspace'].lower(), row['cassandraTblName'].lower(), \ ','.join(oracleColNames), \ ','.join(['%s' for x in range(len(oracleColNames))])) preparedCQL = casSession.prepare(insertCQL.replace('%s', '?')) print 'start upload at %s...' % datetime.datetime.now() st = time.clock() numRowsOracle = len(oracleRows) while len(oracleRows) is not 0: ## first 750,000 cells ~ 55 secs # for dataRow in oracleRows: # casSession.execute(insertCQL, dataRow) ## second 750,000 cells ~ 50 secs # for dataRow in oracleRows: # casSession.execute(preparedCQL, dataRow) ## third 750,000 cells ~ 15 secs # for dataRow in oracleRows: # casSession.execute_async(preparedCQL, dataRow) ## fourth 750,000 cells ~ 8 secs oracleRows = oracleCursor.fetchmany(numFetch) cassandra.concurrent.execute_concurrent_with_args(casSession, preparedCQL, oracleRows, concurrency = 50) numRowsOracle += len(oracleRows) print 'End upload at %s...' % datetime.datetime.now() print 'The total upload time for %i cells is %s seconds...\n' % \ (numRowsOracle * len(oracleColNames), time.clock() - st) # shutdown the connection to cassandra casCon.shutdown() # clost the connection to Oracle oracleCursor.close() oracleConn.close()if __name__ == "__main__": main(sys.argv[1:]) 之後又寫了一版同時用pp去平行的處理各張表格 手上表格不多(六個)，反正處理時間大概就是最大那張表格的時間： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/usr/bin/pythonfrom itertools import isliceimport sys, ppclass TooManyArgsError(Exception): """Err type for too many arguments.""" passdef implementUploadTbl(row): import cx_Oracle import cassandra.cluster import cassandra.concurrent # information for connecting to oracle oracleLoginInfo = u'system/qscf12356@192.168.0.120:1521/orcl' # define the number of fetching once = the number of rows uploaded to cassandra numFetch = 5000 # cassandra cluster ips cassandraIpAddrs = [u'192.168.0.161', u'192.168.0.162', \ u'192.168.0.163', u'192.168.0.164']; # create a dict to map the data type of python to the one in Cassandra pythonCassTypeMap = dict([["str", "text"], ["int", "int"], \ ["float", "double"], ["datetime", "timestamp"]]) #%% import data from Oracle database # create connection oracleConn = cx_Oracle.connect(oracleLoginInfo) # activate cursor oracleCursor = oracleConn.cursor() # make sql oracleSql = "select rowid,t.* from %s.%s t" % \ (row['oracleSchema'], row['oracleTblName']) # execute sql query oracleCursor.execute(oracleSql) #%% start to fetch rows and upload to cassandra # connect to cassandra casCon = cassandra.cluster.Cluster(cassandraIpAddrs) casSession = casCon.connect() # create keyspace if it does not exist casSession.execute("""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = &#123;'class': 'SimpleStrategy', 'replication_factor': '2'&#125; """ % row['cassandraKeyspace']) # set to keyspace casSession.set_keyspace(row['cassandraKeyspace'].lower()) #%% get the oracle data and check the data type oracleRows = oracleCursor.fetchmany(numFetch) # get data type colDataType = [type(r).__name__ for r in oracleRows[0]] # find the mapping data type in cassandra cassType = [pythonCassTypeMap[dataType] for dataType in colDataType] # acquire the column names in Oracle DB oracleColNames = [x[0].lower() for x in oracleCursor.description] # create table if it does not exist casSession.execute('''CREATE TABLE IF NOT EXISTS &#123;&#125;.&#123;&#125; (&#123;&#125;, PRIMARY KEY (rowid))'''.format(\ row['cassandraKeyspace'].lower(), row['cassandraTblName'].lower(),\ ','.join([x + ' ' + y for (x,y) in zip(oracleColNames, cassType)]))) #%% upload data to cassandra insertCQL = """INSERT INTO &#123;&#125;.&#123;&#125; (&#123;&#125;) VALUES (&#123;&#125;)""".format(\ row['cassandraKeyspace'].lower(), row['cassandraTblName'].lower(), \ ','.join(oracleColNames), \ ','.join(['%s' for x in range(len(oracleColNames))])) preparedCQL = casSession.prepare(insertCQL.replace('%s', '?')) numRowsOracle = len(oracleRows) while len(oracleRows) is not 0: oracleRows = oracleCursor.fetchmany(numFetch) cassandra.concurrent.execute_concurrent_with_args(casSession, preparedCQL, oracleRows, concurrency = 50) numRowsOracle += len(oracleRows) # shutdown the connection to cassandra casCon.shutdown() # clost the connection to Oracle oracleCursor.close() oracleConn.close()def main(argv): #%% input check if len(argv) is 0: tableListFN = 'dataTable.csv' print """The default filename of the table list is '%s'.""" % tableListFN elif len(argv) &gt; 1: raise TooManyArgsError('Too many input arguments!') else: tableListFN = argv[0] #%% read input file # print log print 'The filename you input is %s.' % tableListFN # read the title row of csv fieldnames = None with open(tableListFN) as csvfile: firstRow = csvfile.readlines(1) fieldnames = tuple(firstRow[0].strip('\n').split(",")) # read the rows of csv tableList = list() with open(tableListFN) as csvfile: for row in islice(csvfile, 1, None): values = [elem.upper() for elem in row.strip('\n').split(",")] tableList.append(dict(zip(fieldnames, values))) if len(tableList) is 0: print 'There is no data in %s.' % tableListFN jobs = [] job_server = pp.Server() job_server.set_ncpus() for row in tableList: jobs.append(job_server.submit(implementUploadTbl, (row,))) runJob = [job() for job in jobs] if __name__ == "__main__": import time st = time.clock() main(sys.argv[1:]) print 'The total upload time is %s seconds...\n' % (time.clock() - st)]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BigData</tag>
        <tag>Cassandra</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在centos下部署cassandra]]></title>
    <url>%2Fposts%2F201607%2F2016-07-30-deployment-of-cassandra-in-centos.html</url>
    <content type="text"><![CDATA[這篇是我在centos部署cassandra的紀錄 準備工作 基本上同Hadoop那篇，這裡不贅述 部署Cassandra 123456789curl -v -j -k -L http://apache.stu.edu.tw/cassandra/2.2.7/apache-cassandra-2.2.7-bin.tar.gz -o apache-cassandra-2.2.7-bin.tar.gztar -zxvf apache-cassandra-2.2.7-bin.tar.gzsudo mv apache-cassandra-2.2.7 /usr/local/cassandrasudo chown -R tester /usr/local/cassandrasudo tee -a /etc/bashrc &lt;&lt; "EOF"export CASSANDRA_HOME=/usr/local/cassandraexport PATH=$PATH:$CASSANDRA_HOME/binEOF 修改配置 使用vi $CASSANDRA_HOME/conf/cassandra.yaml去改設定檔，改的部分如下： 123456789101112131415161718192021# first place:cluster_name: 'sparkSever'# second place:seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: "192.168.0.161,192.168.0.162,192.168.0.163,192.168.0.164" # third place:listen_address: 192.168.0.161# fourth place:rpc_address: 192.168.0.161# fifth place:endpoint_snitch: GossipingPropertyFileSnitch# sixth place:# seventh place: 一台裝完之後，可以用下面指令做複製的動作，然後修改需要設定的地方(listen_address跟rpc_address)： 1234567scp -rp /usr/local/cassandra tester@sparkServer1:/usr/localscp -rp /usr/local/cassandra tester@sparkServer2:/usr/localscp -rp /usr/local/cassandra tester@sparkServer3:/usr/localssh tester@sparkServer1 "sed -i -e 's/: 192.168.0.161/: 192.168.0.162/g' /usr/local/cassandra/conf/cassandra.yaml"ssh tester@sparkServer2 "sed -i -e 's/: 192.168.0.161/: 192.168.0.163/g' /usr/local/cassandra/conf/cassandra.yaml"ssh tester@sparkServer3 "sed -i -e 's/: 192.168.0.161/: 192.168.0.164/g' /usr/local/cassandra/conf/cassandra.yaml" 啟動Cassandra 在sparkServer0上輸入下面的指令，就可以成功開啟四台Cassandra的node： 1234ssh tester@sparkServer1 "cassandra"ssh tester@sparkServer2 "cassandra"ssh tester@sparkServer3 "cassandra"cassandra 用nodetool status可以確定一下是不是都有跑起來，顯示資訊如下： 12345678910nodetool status# Datacenter: dc1# ===============# Status=Up/Down# |/ State=Normal/Leaving/Joining/Moving# -- Address Load Tokens Owns (effective) Host ID Rack# UN 192.168.0.161 150.93 KB 256 45.5% 5fbf33ca-a88c-4ca4-86c3-6fee0679f2ac rack1# UN 192.168.0.162 98.96 KB 256 49.4% 752cd742-8169-415a-b6e5-b0d60fdf3fc0 rack1# UN 192.168.0.163 68.49 KB 256 52.8% b6b7f4b9-9843-430f-a2a4-7ad7a56e4361 rack1# UN 192.168.0.164 90.3 KB 256 52.2% 2ee0ec24-b542-4bd6-86d3-fc284a12bf5e rack1 自動啟動Cassandra 開機自動啟動Cassandra的script(用sudo vi /etc/init.d/cassandra去create)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/bin/bash# chkconfig: 2345 99 01# description: Cassandra. /etc/rc.d/init.d/functionsCASSANDRA_HOME=/usr/local/cassandraCASSANDRA_BIN=$CASSANDRA_HOME/bin/cassandraCASSANDRA_NODETOOL=$CASSANDRA_HOME/bin/nodetoolCASSANDRA_LOG=$CASSANDRA_HOME/logs/cassandra.logCASSANDRA_PID=/var/run/cassandra.pidCASSANDRA_LOCK=/var/lock/subsys/cassandraPROGRAM="cassandra"if [ ! -f $CASSANDRA_BIN ]; then echo "File not found: $CASSANDRA_BIN" exit 1fiRETVAL=0start() &#123; if [ -f $CASSANDRA_PID ] &amp;&amp; checkpid `cat $CASSANDRA_PID`; then echo "Cassandra is already running." exit 0 fi echo -n $"Starting $PROGRAM: " daemon $CASSANDRA_BIN -p $CASSANDRA_PID &gt;&gt; $CASSANDRA_LOG 2&gt;&amp;1 usleep 500000 RETVAL=$? if [ $RETVAL -eq 0 ]; then touch $CASSANDRA_LOCK echo_success else echo_failure fi echo return $RETVAL&#125;stop() &#123; if [ ! -f $CASSANDRA_PID ]; then echo "Cassandra is already stopped." exit 0 fi echo -n $"Stopping $PROGRAM: " $CASSANDRA_NODETOOL -h 127.0.0.1 decommission if kill `cat $CASSANDRA_PID`; then RETVAL=0 rm -f $CASSANDRA_LOCK echo_success else RETVAL=1 echo_failure fi echo [ $RETVAL = 0 ]&#125;status_fn() &#123; if [ -f $CASSANDRA_PID ] &amp;&amp; checkpid `cat $CASSANDRA_PID`; then echo "Cassandra is running." exit 0 else echo "Cassandra is stopped." exit 1 fi&#125;case "$1" in start) start ;; stop) stop ;; status) status_fn ;; restart) stop start ;; *) echo $"Usage: $PROGRAM &#123;start|stop|restart|status&#125;" RETVAL=3esacexit $RETVAL 然後使用下面指令讓這個script能夠自動跑： 123sudo chmod +x /etc/init.d/cassandrasudo chkconfig --add cassandrasudo service cassandra start 測試 打開Terminal，輸入cqlsh 192.168.0.161 (任意一台有cassandra在運行的電腦IP)就可以開始用Cassandra的cql了，簡單測試如下： 1234567891011121314151617181920212223242526272829303132333435363738394041-- Create KEYSPACECREATE KEYSPACE mariadbtest2 WITH replication = &#123;'class': 'SimpleStrategy', 'replication_factor': '3'&#125;;-- Use the KEYSPACEUSE mariadbtest2;-- create tableCREATE TABLE t1 (rowid text, data1 text, data2 int, PRIMARY KEY (rowid));-- insert dataINSERT INTO t1 (rowid, data1, data2) VALUES ('rowid001', 'g1', 123456);INSERT INTO t1 (rowid, data1, data2) VALUES ('rowid002', 'g2', 34543);INSERT INTO t1 (rowid, data1, data2) VALUES ('rowid003', 'g1', 97548);INSERT INTO t1 (rowid, data1, data2) VALUES ('rowid004', 'g1', 62145);INSERT INTO t1 (rowid, data1, data2) VALUES ('rowid005', 'g2', 140578);-- query whole dataSELECT * FROM t1;/* rowid | data1 | data2----------+-------+-------- rowid003 | g1 | 97548 rowid002 | g2 | 34543 rowid001 | g1 | 123456 rowid005 | g2 | 140578 rowid004 | g1 | 62145(5 rows)*/-- simple calculationSELECT sum(data2) FROM t1;/* system.sum(data2)------------------- 458270(1 rows)*/ 現行的Cassandra CQL還沒支援使用GROUP BY的功能 看了一下網路討論，主要是Cassandra最原本的開發目的是為了快速讀取、儲存的地方，而非計算之用 因此，CQL這一塊還在發展，我有看到issue已經要準備在3.X更新GROUP BY的部分了，敬請期待 Reference http://blog.fens.me/nosql-r-cassandra/ https://twgame.wordpress.com/2015/02/16/real-machine-cassandra-cluster/ http://www.planetcassandra.org/blog/installing-the-cassandra-spark-oss-stack/ http://datastax.github.io/python-driver/getting_started.html https://docs.datastax.com/en/developer/python-driver/1.0/python-driver/quick_start/qsSimpleClientAddSession_t.html https://mariadb.com/kb/en/mariadb/cassandra-storage-engine-use-example/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>CentOS</tag>
        <tag>Cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test on apache sqoop]]></title>
    <url>%2Fposts%2F201607%2F2016-07-28-test-on-apache-sqoop.html</url>
    <content type="text"><![CDATA[前四篇分別裝了Hadoop, Oracle, ROracle跟Python的cx_Oracle套件 上兩篇分別利用了ROracle跟cx_Oracle塞了一些資料進去Oracle 接下來是安裝sqoop，試試看用sqoop從Oracle DB把資料撈進HBase 這篇僅是紀錄而已，並沒有成功撈進 準備工作 基本上同Hadoop那篇，這裡就不贅述 我這裡是直接裝在Hadoop的master (sparkServer0)上 安裝sqoop 從官網上下載下來，然後解壓縮，並加入環境變數： 12345678910111213# 下載安裝sqoopcurl -v -j -k -L http://apache.stu.edu.tw/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -o sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gztar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gzsudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha /usr/local/sqoopsudo chown -R tester /usr/local/sqoop# 加入環境變數sudo tee -a /etc/bashrc &lt;&lt; "EOF"export SQOOP_HOME=/usr/local/sqoopexport PATH=$PATH:$SQOOP_HOME/binexport ZOOCFGDIR=$ZOOKEEPER_HOME/confEOFsource /etc/bashrc Note: HBase 0.98版本才能兼容sqoop 1.4.6，如果是1.0以上版本請重新安裝HBase 加入oracle連線jar 到oracle官網去下載oracle的ojdbc6.jar上傳到tester的home目錄中 執行mv ~/ojdbc6.jar $SQOOP_HOME/lib搬去sqoop下的lib 開始執行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123## 在sparkServer0開啟hadoopstart-dfs.sh &amp; start-yarn.sh &amp; zkServer.sh start &amp; start-hbase.sh## 把oracleServer開啟，他會自動開啟oracle database1## 先列出所有的database看看sqoop list-databases --connect jdbc:oracle:thin:@192.168.0.120:1521:orcl --username system --P# Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.# Please set $HCAT_HOME to the root of your HCatalog installation.# Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.# Please set $ACCUMULO_HOME to the root of your Accumulo installation.# 16/07/31 19:14:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6# Enter password:# 16/07/31 19:14:06 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.# 16/07/31 19:14:06 INFO manager.SqlManager: Using default fetchSize of 1000# SLF4J: Class path contains multiple SLF4J bindings.# SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.# SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]# 16/07/31 19:14:37 INFO manager.OracleManager: Time zone has been set to GMT# ORACLE_OCM# OJVMSYS# SYSKM# XS$NULL# GSMCATUSER# MDDATA# SYSBACKUP# DIP# SYSDG# APEX_PUBLIC_USER# SPATIAL_CSW_ADMIN_USR# SPATIAL_WFS_ADMIN_USR# GSMUSER# AUDSYS# FLOWS_FILES# DVF# MDSYS# ORDSYS# DBSNMP# WMSYS# APEX_040200# APPQOSSYS# GSMADMIN_INTERNAL# ORDDATA# CTXSYS# ANONYMOUS# XDB# ORDPLUGINS# DVSYS# SI_INFORMTN_SCHEMA# OLAPSYS# LBACSYS# OUTLN# SYSTEM# SYS# C##DATASETS_1# C##DATASETS_2# C##DATASETS_3## 再來測試看看拉oracle中的表格C##DATASETS_1.IRIS到HDFS上去sqoop import \--connect jdbc:oracle:thin:@192.168.0.120:1521:orcl \--username system --P \--query &quot;SELECT * FROM C##DATASETS_1.IRIS&quot; \--split-by SPECIES -m 5 --target-dir /user/tester/iris## 出現錯誤：# 16/07/31 19:57:28 ERROR tool.ImportTool: Encountered IOException running import job: # java.io.IOException: Query [SELECT * FROM C##DATASETS_1.IRIS] must contain # &apos;$CONDITIONS&apos; in WHERE clause.## 加入CONDITIONS再一次export CONDITIONS=1=1sqoop import \--connect jdbc:oracle:thin:@192.168.0.120:1521:orcl \--username system --P \--query &quot;SELECT * FROM C##DATASETS_1.IRIS WHERE 1=1 AND \$CONDITIONS&quot; \--split-by SPECIES -m 5 --target-dir /user/tester/iris## 出現錯誤：# 16/07/31 20:00:25 ERROR manager.SqlManager: Error executing statement: # java.sql.SQLRecoverableException: IO Error: Connection reset# java.sql.SQLRecoverableException: IO Error: Connection reset## 換成匯到HBase試試看export CONDITIONS=1=1sqoop import \--connect jdbc:oracle:thin:@192.168.0.120:1521:orcl \--username system --P \--query &quot;SELECT SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES FROM C##DATASETS_1.IRIS WHERE 1=1 AND \$CONDITIONS&quot; \--hbase-table C##DATASETS_1.IRIS --hbase-create-table \--hbase-row-key id --split-by SPECIES -m 10 \--column-family SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES## 出現錯誤：# 16/07/31 20:01:36 ERROR manager.SqlManager: Error executing statement: # java.sql.SQLRecoverableException: IO Error: Connection reset# java.sql.SQLRecoverableException: IO Error: Connection reset## 試試看網路解法，出現不同錯誤sqoop import -D mapred.child.java.opts=&quot;\-Djava.security.egd=file:/dev/../dev/urandom&quot; \--connect jdbc:oracle:thin:@192.168.0.120:1521:orcl \--username system --P \--query &quot;SELECT SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES FROM C##DATASETS_1.IRIS WHERE 1=1 AND \$CONDITIONS&quot; \--hbase-table C##DATASETS_1.IRIS --hbase-create-table \--hbase-row-key id --split-by SPECIES -m 10 \--column-family SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES## 出現錯誤# ERROR tool.ImportTool: Imported Failed: Illegal character code:35, &lt;#&gt; at 1. # User-space table qualifiers can only contain &apos;alphanumeric characters&apos;: # i.e. [a-zA-Z_0-9-.]: C##DATASETS_1.IRIS## 換使用者在一次，還是出現一樣的錯誤sqoop import -D mapred.child.java.opts=&quot;\-Djava.security.egd=file:/dev/../dev/urandom&quot; \--connect jdbc:oracle:thin:@192.168.0.120:1521:orcl \--username C##DATASETS_1 --P \--query &quot;SELECT SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES FROM C##DATASETS_1.IRIS WHERE 1=1 AND \$CONDITIONS&quot; \--hbase-table IRIS --hbase-create-table \--hbase-row-key id --split-by SPECIES -m 10 \--column-family SEPALLENGTH,SEPALWIDTH,PETALLENGTH,PETALWIDTH,SPECIES## 出現錯誤# 16/08/02 20:12:17 ERROR manager.SqlManager: Error executing statement: # java.sql.SQLRecoverableException: IO Error: Connection reset# java.sql.SQLRecoverableException: IO Error: Connection reset 然後我就放棄再嘗試了，我中間還有把HBase從1.1.0降板到0.98，但是還是沒用，特此紀錄。 Reference http://www.cnblogs.com/byrhuangqiang/p/3922594.html https://www.zybuluo.com/aitanjupt/note/209968 http://blog.csdn.net/u010330043/article/details/51441135 http://www.cnblogs.com/smartloli/p/4202710.html]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>HBase</tag>
        <tag>Sqoop</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Python用cx_Oracle去操作Oracle資料庫]]></title>
    <url>%2Fposts%2F201607%2F2016-07-26-use-cx_Oracle-to-manipulate-oracle-database-in-Python.html</url>
    <content type="text"><![CDATA[前一篇用R去操作了Oracle資料庫 結果不幸發現兩件事情： 表的名字會自動有quote，你預期的表明應該是airlines，會變成”airlines” column name也會自動有quote，你預期的表明應該是name，會變成”name” 這一篇實現用Python去操作Oracle資料庫的部分 準備工作 基本上同R那篇，這裡不贅述 安裝cx_Oracle 其實跟R那篇一樣，都要安裝Oracle instant client 差別只在python使用pip install cx_Oracle去做安裝套件的動作而已 這裡推薦大家直接使用Anaconda，並用conda install cx_Oracle去安裝 conda會幫你把Oracle instant client下載好，並放在適當目錄中 (2018/04/21備註：在centos上可以用這個做法：Link) 使用cx_Oracle去上傳資料 使用的資料都是來自UC Irvine Machine Learning Repository 我分別去抓了iris, forest fires, Car Evaluation, default of credit card clients跟adult這五個資料集 放在下方程式目錄下的testData資料夾中 資料都是csv檔案，含有表頭(從UCI下載之後要自己做一點更動) 完整程式如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184# -*- coding: utf-8 -*-import StringIO, csv, cx_Oracle, sys, os, datetime, timeclass LengthNotEqualErr(Exception): """Err type for too many arguments.""" pass#%% functions to read the csv file with auto type convertiondef getFieldnames(csvFile): """ Read the first row and store values in a tuple """ with open(csvFile) as csvfile: firstRow = csvfile.readlines(1) fieldnames = tuple(firstRow[0].strip('\n').split(",")) return fieldnamesdef parseValues(values, dataType = None): from ast import literal_eval import re regxpPattern = re.compile('\d+[^.\d]+') for i, value in enumerate(values): if (dataType is None) or (dataType[i] == 'float' or dataType[i] == 'int'): try: if len(regxpPattern.findall(value)) is 0: nValue = literal_eval(value) values[i] = nValue except ValueError: pass except SyntaxError: pass return valuesdef readCsvWithTypeConv(csvFileName, dataType = None): """ Convert csv rows into an array of dictionaries All data types are automatically checked and converted """ from itertools import islice fieldnames = getFieldnames(csvFileName) dataTypeMap = None if dataType is not None: dataTypeMap = [dataType[x] for x in list(fieldnames)] if len(dataType) is not len(fieldnames): raise LengthNotEqualErr('The length of dataType must be equal to the lenght of fieldnames.') cursor = [] # Placeholder for the dictionaries/documents with open(csvFileName) as csvFile: for row in islice(csvFile, 1, None): values = parseValues(row.strip('\n').split(","), dataTypeMap) cursor.append(dict(zip(fieldnames, values))) return cursor#%% main functionif __name__ == "__main__": # information for connecting to oracle oracleSystemLoginInfo = u'system/qscf12356@192.168.0.120:1521/orcl' # create a dict to map the data type of python to the one in Oracle DB pythonOracleTypeMap = dict([["str", "VARCHAR(255)"], ["int", "NUMBER"], \ ["float", "FLOAT"]]) # Read list of data table tableList = """dataName,path,uploadShema,uploadTblNameiris,testData/iris.csv,datasets_1,irisforestfires,testData/forestfires.csv,datasets_1,forestfirescar,testData/car.csv,datasets_1,carcredit,testData/credit.csv,datasets_2,creditadult,testData/adult.csv,datasets_2,adult""" fid = StringIO.StringIO(tableList) reader = csv.reader(fid, delimiter=',') dataTableList = list(); for i,row in enumerate(reader): if i &gt; 0: dataTableList.append(row) # create connection oracleConn = cx_Oracle.connect(oracleSystemLoginInfo) # activate cursor oracleCursor = oracleConn.cursor() # find the all users in oracle oracleCursor.execute("SELECT USERNAME FROM all_users") orclUserNames = [x[0] for x in oracleCursor.fetchall()] uniqueUserNames = list(set([x for x in ['C##' + y[2].upper() for y in dataTableList]])) nonexistOrcl = [x not in orclUserNames for x in uniqueUserNames] # find the all tables in oracle oracleCursor.execute('''SELECT t.TABLE_NAME FROM all_tables t WHERE t.OWNER NOT IN ('SYSTEM', 'SYS', 'MDSYS', 'LBACSYS', 'CTXSYS', 'WMSYS', 'XDB', 'APPQOSSYS', 'ORDSYS', 'OJVMSYS', 'DVSYS')''') oracleAllTables = [x[0] for x in oracleCursor.fetchall()] # if the schema is not existent, then print out the SQL to create if any(nonexistOrcl): print 'Please create users first with following SQL:' for i,x in enumerate(uniqueUserNames): if nonexistOrcl[i]: print ''' CREATE TABLESPACE userNameVar DATAFILE 'userNameVar.dat' SIZE 40M REUSE AUTOEXTEND ON; CREATE TEMPORARY TABLESPACE userNameVar_tmp TEMPFILE 'userNameVar_tmp.dbf' SIZE 10M REUSE AUTOEXTEND ON; CREATE USER C##userNameVar IDENTIFIED BY userNameVar DEFAULT TABLESPACE userNameVar TEMPORARY TABLESPACE userNameVar_tmp quota unlimited on userNameVar; GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW, CREATE PROCEDURE TO C##userNameVar;'''.replace('userNameVar', x.replace('C##', '')) sys.exit(0) # clost the connection to Oracle oracleCursor.close() oracleConn.close() for row in dataTableList: print "Now uplaod &#123;&#125; data to Oracle DB...".format(row[0]) if os.path.isfile(row[1]): # read the data dataTable = readCsvWithTypeConv(row[1]) colNames = [name.replace(' ', '_').replace('-', '_').upper() for name in dataTable[0].keys()] colDataType = [type(r).__name__ for r in dataTable[0].values()] newDataType = [type(r).__name__ for r in dataTable[0].values()] convert = False for rowDataTbl in dataTable: otherRowDataType = [type(r).__name__ for r in rowDataTbl.values()] typeNotEqual = [x != y for x,y in zip(colDataType, otherRowDataType)] if any(typeNotEqual): missConversion = [(i, otherRowDataType[i], colDataType[i]) for\ i, x in enumerate(typeNotEqual) if x] for i,x,y in missConversion: if x != y: if x == 'str' or y == 'str': newDataType[i] = 'str' elif (x == 'float' and y == 'int') or (x == 'float' and y == 'int'): newDataType[i] = 'float' if any([x != y for x,y in zip(colDataType, newDataType)]): dataTable = readCsvWithTypeConv(row[1], \ dict(zip(dataTable[0].keys(), newDataType))) colDataType = newDataType oracleType = [pythonOracleTypeMap[dataType] for dataType in colDataType] orclLogin = oracleSystemLoginInfo.replace('system', 'C##' + row[2].upper())\ .replace('qscf12356', row[2]) # create connection oracleConn = cx_Oracle.connect(orclLogin) # activate cursor oracleCursor = oracleConn.cursor() # create table if row[3].upper() not in oracleAllTables: oracleCursor.execute('''CREATE TABLE &#123;&#125;(&#123;&#125;)'''.format(row[3].upper(), \ ','.join([x + ' ' + y for (x,y) in zip(colNames, oracleType)]))) # insert data print 'start upload at %s...' % datetime.datetime.now() st = time.clock() oracleCursor.executemany( '''INSERT INTO &#123;&#125;(&#123;&#125;) values (&#123;&#125;)'''.format(row[3], ','.join(colNames), \ ','.join([':&#123;&#125;'.format(i) for i in range(len(colNames))])), \ [d.values() for d in dataTable]) print 'End upload at %s...' % datetime.datetime.now() print 'The total upload time for %i cells is %s seconds...\n' % \ (len(dataTable) * len(dataTable[0]), time.clock() - st) # commit change oracleConn.commit() # clost the connection to Oracle oracleCursor.close() oracleConn.close() 刪除全部測試資料的程式碼： 123456789101112131415161718192021222324252627282930313233343536# -*- coding: utf-8 -*-import StringIO, csv, cx_Oracleif __name__ == "__main__": oracleSystemLoginInfo = u'system/qscf12356@192.168.0.120:1521/orcl' # Read list of data table tableList = """dataName,path,uploadShema,uploadTblNameiris,testData/iris.csv,datasets_1,irisforestfires,testData/forestfires.csv,datasets_1,forestfirescar,testData/car.csv,datasets_1,carcredit,testData/credit.csv,datasets_2,creditadult,testData/adult.csv,datasets_2,adult""" fid = StringIO.StringIO(tableList) reader = csv.reader(fid, delimiter=',') dataTableList = list(); for i,row in enumerate(reader): if i &gt; 0: dataTableList.append(row) # create connection oracleConn = cx_Oracle.connect(oracleSystemLoginInfo) # activate cursor oracleCursor = oracleConn.cursor() for row in dataTableList: try: oracleCursor.execute("DROP TABLE &#123;&#125;".format('C##' + row[2].upper() + '.' + row[3].upper())) except cx_Oracle.DatabaseError: pass # clost the connection to Oracle oracleCursor.close() oracleConn.close()]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BigData</tag>
        <tag>Oracle</tag>
        <tag>cx_Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在R用ROracle去操作Oracle資料庫]]></title>
    <url>%2Fposts%2F201607%2F2016-07-25-use-ROracle-to-manipulate-oracle-database-in-R.html</url>
    <content type="text"><![CDATA[前兩篇裝了Hadoop跟Oracle 為了接下來可以測試sqoop，使用ROracle去塞一下資料表進去 在windows下，安裝ROracle，也測試看看在centos下安裝看看 (Note: modified at 2018/04/19) (ubuntu, mint部分前面有文章介紹怎麼裝R，就不在贅述，至於裝ROracle就跟centos大同小異了) 準備工作 基本上同Hadoop那篇，這裡就不贅述 我這裡是直接裝在Hadoop的master (sparkServer0)上 centos部分： (windows請往下轉) 安裝Microsoft R Open 主要參考我自己前幾篇文章 - Some installations of centos 簡單敘述一下(不同的地方是centos最小安裝沒有wget)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263## 更新repo，並安裝Rsu -c 'rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm'sudo yum updatesudo yum install gcc-c++ R R-devel R-java## remove Rsudo rm -rf /usr/lib64/R# install MRO## Make sure the system repositories are up-to-date prior to installing Microsoft R Open.sudo yum clean all## get the installerscurl -v -j -k -L https://mran.microsoft.com/install/mro/3.3.0/MRO-3.3.0.el7.x86_64.rpm -o MRO-3.3.0.el7.x86_64.rpmcurl -v -j -k -L https://mran.microsoft.com/install/mro/3.3.0/RevoMath-3.3.0.tar.gz -o RevoMath-3.3.0.tar.gz## 安裝MROsudo yum install MRO-3.3.0.el7.x86_64.rpm## 安裝MKLtar -xzf RevoMath-3.3.0.tar.gz## 備註一點，這裡如果直接用 sudo bash ./RevoMath/RevoMath.sh會失敗## 失敗之後要先 sudo yum remove MRO重裝一次cd RevoMathsudo bash ./RevoMath.sh## 開啟Library讀寫權限sudo chmod -R 777 /usr/lib64/MRO-3.3.0/R-3.3.0/lib64/R## 安裝之後打開RR## 會出現下方訊息# R version 3.3.0 (2016-05-03) -- "Supposedly Educational"# Copyright (C) 2016 The R Foundation for Statistical Computing# Platform: x86_64-pc-linux-gnu (64-bit)## R is free software and comes with ABSOLUTELY NO WARRANTY.# You are welcome to redistribute it under certain conditions.# Type 'license()' or 'licence()' for distribution details.## Natural language support but running in an English locale## R is a collaborative project with many contributors.# Type 'contributors()' for more information and# 'citation()' on how to cite R or R packages in publications.## Type 'demo()' for some demos, 'help()' for on-line help, or# 'help.start()' for an HTML browser interface to help.# Type 'q()' to quit R.## Microsoft R Open 3.3.0# Default CRAN mirror snapshot taken on 2016-06-01# The enhanced R distribution from Microsoft# Visit https://mran.microsoft.com/ for information# about additional features.## Multithreaded BLAS/LAPACK libraries detected. Using 2 cores for math algorithms.## 有上面這行代表MKL才有裝成功## 安裝一些ROracle的必要套件R -e 'install.packages("DBI")'## 也可以打開R輸入指令## 安裝一些等等資料用到的套件R -e 'install.packages(c("data.table", "pipeR", "dplyr", "GGally", "ggplot2", "nycflights13"))'## 也可以打開R輸入指令 安裝ROracle 先從官方網站下載Oracle instant client，下載basck.rpm跟devel.rpm(詳細檔名看下方)，然後上傳到VM 接著下載ROracle原始包，利用R CMD INSTALL並同時configure做安裝的動作，指令如下： 12345678910111213141516# 安裝Oracle instant clientsudo yum install oracle-instantclient12.1-basic-12.1.0.2.0-1.x86_64.rpmsudo yum install oracle-instantclient12.1-devel-12.1.0.2.0-1.x86_64.rpm# 設定環境變數sudo tee -a /etc/bashrc &lt;&lt; "EOF"export ORACLE_HOME=/usr/lib/oracle/12.1/client64export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/libexport PATH=$PATH:$ORACLE_HOME/binEOF# 下載ROracle套件curl -v -j -k -L https://cran.r-project.org/src/contrib/ROracle_1.2-2.tar.gz -o ROracle_1.2-2.tar.gz# 安裝ROracleR CMD INSTALL --configure-args='--with-oci-lib=/usr/lib/oracle/12.1/client64/lib --with-oci-inc=/usr/include/oracle/12.1/client64' ROracle_1.2-2.tar.gz windows部分： 安裝MRO跟MKL 到官方網站下載安裝包 然後安裝MRO跟MKL，安裝之後，利用installr安裝Rtools 打開R輸入 12install.packages("installr")installr::install.Rtools() 接著安裝一些需要的套件： 12345## 安裝一些ROracle的必要套件install.packages("DBI")## 安裝一些等等資料用到的套件install.packages(c("data.table", "pipeR", "dplyr", "GGally", "ggplot2", "nycflights13")) 然後一樣到Oracle的官方網站下載instant client，要下載basic跟sdk 解壓縮放到C:\instantclient_12_1 接著下載R的套件包：https://cran.r-project.org/src/contrib/ROracle_1.3-1.tar.gz 在安裝的目錄下新增一個install_ROracle.bat的檔案 按右鍵編輯貼上下方內容後執行就可以成功安裝ROracle了 1234SET OCI_LIB64=C:\instantclient_12_1SET OCI_INC=C:\instantclient_12_1\sdk\includeSET PATH=%PATH%;C:\instantclient_12_1R CMD INSTALL --build ROracle_1.2-2.tar.gz PS: 這裡我還沒有測試過，因為避免牽扯到windwos的環境變數就省掉 如果有人測試過不行，我在提供增加的環境變數 我是直接安裝Rtools手動設定環境變數… 比較不適合一般初學者 創造適當的使用者 我想要創造三個使用者，等等利用ROracle上傳到這三個使用者下的schema，SQL內容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071CREATE TABLESPACE nycflights13 DATAFILE 'nycflights13.dat' SIZE 40M REUSE AUTOEXTEND ON;CREATE TEMPORARY TABLESPACE nycflights13_tmp TEMPFILE 'nycflights13_tmp.dbf' SIZE 10M REUSE AUTOEXTEND ON;CREATE USER C##nycflights13IDENTIFIED BY nycflights13DEFAULT TABLESPACE nycflights13TEMPORARY TABLESPACE nycflights13_tmpquota unlimited on nycflights13;GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW, CREATE PROCEDURETO C##nycflights13;CREATE TABLESPACE datasets DATAFILE 'datasets.dat' SIZE 40M AUTOEXTEND ON;CREATE TEMPORARY TABLESPACE datasets_tmp TEMPFILE 'datasets_tmp.dbf' SIZE 10M AUTOEXTEND ON;CREATE USER C##datasetsIDENTIFIED BY datasetsDEFAULT TABLESPACE datasetsTEMPORARY TABLESPACE datasets_tmpquota unlimited on datasets;GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW, CREATE PROCEDURETO C##datasets;CREATE TABLESPACE hadley DATAFILE 'hadley.dat' SIZE 40M AUTOEXTEND ON;CREATE TEMPORARY TABLESPACE hadley_tmp TEMPFILE 'hadley_tmp.dbf' SIZE 10M AUTOEXTEND ON;CREATE USER C##hadleyIDENTIFIED BY hadleyDEFAULT TABLESPACE hadleyTEMPORARY TABLESPACE hadley_tmpquota unlimited on hadley;GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW, CREATE PROCEDURETO C##hadley;/* DELETE USERS AND TABLESPACESDROP USER C##hadley;DROP USER C##datasets;DROP USER C##nycflights13;DROP TABLESPACE hadley;DROP TABLESPACE datasets;DROP TABLESPACE nycflights13;DROP TABLESPACE hadley_tmp;DROP TABLESPACE datasets_tmp;DROP TABLESPACE nycflights13_tmp;*/ PS: C##是Oracle要求的，請搜尋common users vs local users oracle就知道差異，還有命名要求 把上面的SQL下載下來命名成createUser.sql並上傳到Oracle的server 使用$ORACLE_HOME/bin/sqlplus system/password@oracleServer:1539/orcl @createUser.sql來執行創造使用者的SQL 使用ROracle去上傳資料 程式如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118library(ROracle)library(data.table)library(pipeR)Sys.setenv(TZ = "Asia/Taipei", ORA_SDTZ = "Asia/Taipei")# 創造資料列表# 全部資料大小: 60 MbuploadDataList &lt;- 'TableName,pkgName,TableSize,savingSchema,savingTblNameairlines,nycflights13,"2.9 Kb",nycflights13,airlinesairports,nycflights13,"228.9 Kb",nycflights13,airportsflights,nycflights13,"38.7 Mb",nycflights13,flightsplanes,nycflights13,"347.9 Kb",nycflights13,planesweather,nycflights13,"2.8 Mb",nycflights13,weatheriris,datasets,"6.9 Kb",datasets,iriscars,datasets,"1.5 Kb",datasets,carsUSCancerRates,latticeExtra,"504.8 Kb",datasets,uscancerratesgvhd10,latticeExtra,"6.5 Mb",datasets,gvhd10nasa,GGally,"4.6 Mb",hadley,nasahappy,GGally,"2.7 Mb",hadley,happydiamonds,ggplot2,"3.3 Mb",hadley,diamondstxhousing,ggplot2,"541.8 Kb",hadley,txhousing'# 讀取列表tblListDT &lt;- fread(uploadDataList)# 讀取資料tblListDT %&gt;&gt;% apply(1, function(v)&#123; data(v[1], package = v[2])&#125;) %&gt;&gt;% invisible#連線資訊host &lt;- "192.168.0.120"port &lt;- 1539sid &lt;- "orcl"connectString &lt;- paste( "(DESCRIPTION=", "(ADDRESS=(PROTOCOL=TCP)(HOST=", host, ")(PORT=", port, "))", "(CONNECT_DATA=(SID=", sid, ")))", sep = "")# 先用system權限登入查看usercon &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString)# query all_usersuserList &lt;- dbSendQuery(con, "select * from ALL_USERS") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# 印出表格，user按照創造時間排列# 可以看到已經創造了C##NYCFLIGHTS13, C##DATASETS跟C##HADLEY三個userprint(userList[order(CREATED)])# system帳號斷線dbDisconnect(con)st &lt;- proc.time()uniqueUserNames &lt;- unique(tblListDT$savingSchema)sapply(uniqueUserNames, function(userName)&#123; # filter要上傳的表 uploadDataDT &lt;- tblListDT[savingSchema == userName] mapply(function(tblName, pkgName) data(list = tblName, package = pkgName), uploadDataDT$TableName, uploadDataDT$pkgName) %&gt;&gt;% invisible # 用user去上傳，這樣才會傳到以該user命名的schema下 con &lt;- dbConnect(dbDriver("Oracle"), username = paste0("C##", userName), password = userName, dbname = connectString) # 把每一張要上傳的表 上傳上去 mapply(function(tblName, tblNameUpload)&#123; if (!dbExistsTable(con, tblNameUpload)) dbWriteTable(con, tblNameUpload, as.data.frame(get(tblName)), row.names = FALSE) &#125;, uploadDataDT$TableName, uploadDataDT$savingTblName) %&gt;&gt;% invisible dbDisconnect(con)&#125;) %&gt;&gt;% invisibleproc.time() - st# user system elapsed# 0.95 0.16 11.56# 移除所有表格rm(list = tblListDT$TableName)con &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString)# list schemadbListTables(con)# find the table in current schema (parameter, schema = NULL)dbExistsTable(con, "airlines")# find the table in the schemadbExistsTable(con, "airlines", schema = 'C##NYCFLIGHTS13')# remove tabledbRemoveTable(con, "airlines")# query data, fetch all data and convert to data.table (這是查詢全部的tablespaces)dbSendQuery(con, "select * from C##NYCFLIGHTS13.\"airlines\"") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# query rowid and name from airlines data,# fetch all data and convert to data.table (這是查詢全部的tablespaces)dbSendQuery(con, "select rowid,t.\"name\" from C##NYCFLIGHTS13.\"airlines\" t") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# query data, fetch all data and convert to data.table (這是查詢全部的tablespaces)dbSendQuery(con, "select * from dba_tablespaces") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# query data, fetch all data and convert to data.table (這是查詢全部的tables)dbSendQuery(con, "select * from all_tables") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# query data, fetch all data and convert to data.table (這是查詢全部的users)dbSendQuery(con, "select * from all_users") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table# 從DB斷線dbDisconnect(con)# remove all tables (用最高權限刪除)st &lt;- proc.time()con &lt;- dbConnect(dbDriver("Oracle"), username = "system", password = "qscf12356", dbname = connectString)mapply(function(schemaName, tblNameUpload)&#123; sn &lt;- paste0("C##", schemaName) %&gt;&gt;% toupper if (dbExistsTable(con, tblNameUpload, schema = sn)) dbRemoveTable(con, tblNameUpload, schema = sn)&#125;, tblListDT$savingSchema, tblListDT$savingTblName) %&gt;&gt;% invisibledbDisconnect(con)proc.time() - st# user system elapsed# 0.02 0.00 0.35 小抱怨 ROracle寫入表格的時候，會自動加上double quote框住表格 因為這一個自動功能，讓我debug，de了一天晚上 最後是去下載Oracle PL/SQL Developer用介面的自動提示才發現這件事情… 然後才回去看dbWriteTable下面的解釋 Table, schema, and column names are case sensitive, e.g., table names ABC and abc are not the same. All database schema object names should not include double quotes as they are enclosed in double quotes when the corresponding SQL statement is generated. 只是最神奇的事情是 1dbWriteTable(con, "airlines", as.data.frame(airlines), row.names = FALSE) 寫入表格之後，query的表格名字要加quote，像這樣： 1dbSendQuery(con, "select * from \"airlines\"") %&gt;&gt;% fetch(n = -1) %&gt;&gt;% data.table 我查表是否存在不用quote 1dbExistsTable(con, &quot;airlines&quot;) 害我一直想說為啥我找不到我的表 最後只能去安裝Oracle SQL Developer查看真正的表格名稱 7/31補充： 更扯的事情是column name都有包quote，select時候，都要多打””去框住column name 8/12更新： 全部使用大寫字元，就不會產生上面的情況]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>BigData</tag>
        <tag>Oracle</tag>
        <tag>ROracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deployment of Oracle database]]></title>
    <url>%2Fposts%2F201607%2F2016-07-24-deployment-of-oracle-database.html</url>
    <content type="text"><![CDATA[這一篇文章主要是在CentOS 7.4最小安裝下去部署Oracle database 我會建立Oracle database的主要原因是 為了下一篇測試從Oracle database拉資料到sqoop (Note: modified at 2018/04/19) 準備工作 這部分照著前一篇spark的布置即可 安裝java 12345678910# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" \http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.rpm \-o jdk-8u171-linux-x64.rpmsudo yum install -y jdk-8u171-linux-x64.rpm# setup environmentsudo tee -a /etc/profile &lt;&lt; "EOF"export JAVA_HOME=/usr/java/jdk1.8.0_171-amd64EOFsource /etc/profile 安裝Oracle database a. set up hostname 用sudo hostnamectl set-hostname oracleTest.test.com就會立即生效了 b. 創建Oracle database的group, user 12345sudo groupadd oinstallsudo groupadd dbasudo useradd -g oinstall -G dba oracle# 更改密碼sudo passwd oracle c. 設定系統變數 12345678910111213sudo tee -a /etc/sysctl.conf &lt;&lt; "EOF"fs.aio-max-nr = 1048576fs.file-max = 6815744kernel.shmall = 2097152kernel.shmmax = 1987162112kernel.shmmni = 4096kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048586EOF 可以用sysctl -p跟sysctl -a來確定是否設定成功 d. 設定系統安全性 12345678910sudo tee -a /etc/security/limits.conf &lt;&lt; "EOF"oracle soft nproc 131072oracle hard nproc 131072oracle soft nofile 131072oracle hard nofile 131072oracle soft core unlimitedoracle hard core unlimitedoracle soft memlock 50000000oracle hard memlock 50000000EOF e. 修改hosts 123sudo tee -a /etc/hosts &lt;&lt; "EOF"192.168.0.120 oracleTest oracleTest.test.com localhost localhost.localdomainEOF f. 安裝需要元件 1234567sudo yum install -y zip unzip binutils.x86_64 compat-libcap1.x86_64 gcc.x86_64 gcc-c++.x86_64 \glibc.i686 glibc.x86_64 glibc-devel.i686 glibc-devel.x86_64 ksh compat-libstdc++-33 libaio.i686 \libaio.x86_64 libaio-devel.i686 libaio-devel.x86_64 libgcc.i686 libgcc.x86_64 libstdc++.i686 \libstdc++.x86_64 libstdc++-devel.i686 libstdc++-devel.x86_64 libXi.i686 libXi.x86_64 libXtst.i686 \libXtst.x86_64 make.x86_64 sysstat.x86_64 unixODBC.x86_64 unixODBC-devel.x86_64 libaio.i386# for installationsudo yum groupinstall "X Window System" "Fonts" -y g. 下載安裝檔案 可以從oracle網站點這下載下來，然後用sftp上傳到你的VM 上傳常見的工具是FileZilla，在google很容易找到 至於使用方法搜尋一下也能找的到 假設下載到目前最新版本：12c Release 1 那檔名應該是：linuxamd64_12102_database_1of2.zip跟linuxamd64_12102_database_2of2.zip h. 解壓縮檔案 12sudo unzip linuxamd64_12102_database_1of2.zip -d /stage/sudo unzip linuxamd64_12102_database_2of2.zip -d /stage/ i. 建立需要的資料夾 12345678sudo mkdir /u01sudo mkdir /u02sudo chown -R oracle:oinstall /u01sudo chown -R oracle:oinstall /u02sudo chmod -R 775 /u01sudo chmod -R 775 /u02sudo chmod g+s /u01sudo chmod g+s /u02 j. 利用Xming安裝 (putty in windows，如果其他系統要再用別的方式) (i) 下載Xming：https://sourceforge.net/projects/xming/ (ii) 安裝Xming(iii) Putty設定中的Connection/SSH/X11的分頁裡面，啟用X11，並設定X display location為localhost:0 (iv) 登入伺服器，使用者用oracle (v) 使用/stage/database/runInstaller (出現要設定DISPLAY問題可以先打xhost +，然後在執行一次) (vi) 細部的安裝設定可以參考 https://wiki.centos.org/zh-tw/HowTos/Oracle12onCentos7 k. 設定防火牆 1234567sudo firewall-cmd --zone=public --add-port=1521/tcp --add-port=1539/tcp \--add-port=5500/tcp --add-port=5520/tcp --add-port=3938/tcp \--permanentsudo firewall-cmd --reload# 確定有開啟portsudo firewall-cmd --list-ports# 1521/tcp 1539/tcp 3938/tcp 5500/tcp 5520/tcp l. 設定環境變數 1234567891011121314tee -a /home/oracle/.bash_profile &lt;&lt; EOFexport TMPDIR=$TMPexport ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/12.1.0/dbhome_1export ORACLE_SID=orclexport PATH=$ORACLE_HOME/bin:$PATHexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib:/usr/lib64export CLASSPATH=$ORACLE_HOME/jlib:$ORACLE_HOME/rdbms/jlibEOFtee -a /etc/bashrc &lt;&lt; "EOF"export ORACLE_HOME=/u01/app/oracle/product/12.1.0/dbhome_1EOFsource /etc/bashrc m. 設定對外連線 編輯/u01/app/oracle/product/12.1.0/dbhome_1/network/admin/listener.ora檔案，增加一行： 1(ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.0.120)(PORT = 1539)) n. 設定自動啟動 12345678910111213141516171819202122232425262728293031323334353637383940tee /etc/sysconfig/oral_envs &lt;&lt; EOFORACLE_BASE=/u01/app/oracleORACLE_HOME=/u01/app/oracle/product/12.1.0/dbhome_1ORACLE_SID=orclEOFtee /usr/lib/systemd/system/oral@lsnrctl.service &lt;&lt; EOF[Unit]Description=Oracle Net ListenerAfter=network.target[Service]Type=forkingEnvironmentFile=/etc/sysconfig/oral_envsExecStart=/u01/app/oracle/product/12.1.0/dbhome_1/bin/lsnrctl startExecStop=/u01/app/oracle/product/12.1.0/dbhome_1/bin/lsnrctl stopUser=oracle[Install]WantedBy=multi-user.targetEOFtee /usr/lib/systemd/system/oral@oracledb.service &lt;&lt; EOF[Unit]Description=Oracle Database serviceAfter=network.target lsnrctl.service[Service]Type=forkingEnvironmentFile=/etc/sysconfig/oral_envsExecStart=/u01/app/oracle/product/12.1.0/dbhome_1/bin/dbstart \$ORACLE_HOMEExecStop=/u01/app/oracle/product/12.1.0/dbhome_1/bin/dbshut \$ORACLE_HOMEUser=oracle[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable oral@lsnrctl oral@oracledb o. 確定database狀態 確定LISTENER：$ORACLE_HOME/bin/lsnrctl status LISTENER 或是 systemctl status oral@lsnrctl 確定db是否有正常啟動服務：ps -ef | grep ora 或是 systemctl status oral@oracledb 也可以透過$ORACLE_HOME/bin/sqlplus system/password@oracleServer:1539/orcl來連線 (password請記得換成你在安裝時設定的密碼) 去試試看是否有正常啟動，並且也可以寫SQL試試看 正常啟動的文字顯示如下： 1234567891011SQL*Plus: Release 12.1.0.2.0 Production on Fri Jul 29 21:27:14 2016Copyright (c) 1982, 2014, Oracle. All rights reserved.Last Successful login time: Fri Jul 29 2016 21:20:01 +08:00Connected to:Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit ProductionWith the Partitioning, OLAP, Advanced Analytics and Real Application Testing optionsSQL&gt; 連上後可以測試看看select OWNER,TABLE_NAME from all_tables;就會跑出所有表格的名字跟主人名字 另外補充一個工具，Oracle PL/SQL Developer，可以從這裡下載到 是一個簡單、方便操作的GUI去查詢SQL，比起直接使用sqlplus良善許多 Reference http://dbaora.com/install-oracle-11g-release-2-11-2-on-centos-linux-7/ https://www.unixmen.com/how-to-install-oralce-11gr2-database-server-on-centos-6-3/ http://www.linuxidc.com/Linux/2016-04/130559.htm http://superuser.com/questions/576006/linker-error-while-installing-oracle-11g-on-fedora-18 https://dotblogs.com.tw/jamesfu/2016/02/02/oracle12c_install https://wiki.centos.org/zh-tw/HowTos/Oracle12onCentos7 http://stackoverflow.com/questions/8937933/installing-oracle-11g-r2-in-ubuntu-10-04 (Err: must be configured to display at least 256 colors) https://dotblogs.com.tw/jamesfu/2016/02/02/oracle12c_install http://www.cnblogs.com/interboy/archive/2008/07/24/1250077.html https://www.server-world.info/en/note?os=CentOS_7&amp;p=oracle12c&amp;f=6]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基於Hadoop的Spark, Phoenix, HBase, Yarn, Zookeeper部署]]></title>
    <url>%2Fposts%2F201607%2F2016-07-23-deployment-spark-phoenix-hbase-yarn-zookeeper-hadoop.html</url>
    <content type="text"><![CDATA[之前有一系列文章是在mint 17上部署R-hadoop的環境 這篇主要是在centos 7.2最小安裝下去部署hadoop, yarn, zookeeper, hbase, phoenix以及spark 一樣是透過VM做部署，所以目標是部署好其中一台後 再把那一台的映像檔做clone，變成slaves 準備工作 i. 安裝好VMware，然後新增一台VM (網路連接方式使用bridged即可)，引進centos 7.2安裝映像檔 ii. 選擇最小安裝，並新增使用者: tester iii. 安裝完後要先configure： a. 給予使用者sudoer權限 1234su # 切換到rootvisudo # 打開設定檔# 打/root\tALL找到這行 root ALL=(ALL) ALL# 在下面新增 tester ALL=(ALL) ALL b. 網路設定 先查看自己電腦的網段是哪一個(使用撥接就無法，要透過IP分享器) 在cmd上找ipconfig就可以找到，像是我的電腦是192.168.0.111 預設閘道192.168.0.1，沒有設定DNS 接著用ip a看VM網路卡的裝置名稱 我的VM網路卡名稱是eno16777736 然後就使用sudo ifup eno16777736去啟用網路 接著使用sudo vi /etc/sysconfig/network-scripts/ifcfg-eno16777736去修改網路設定 改成下方這樣： 12345678910111213141516171819TYPE=EthernetBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=eno16777736UUID=213f0a79-5d73-45c5-afce-16b48e6c9c65DEVICE=eno16777736ONBOOT=yesIPADDR=192.168.0.161PREFIX=24GATEWAY=192.168.0.1DNS1=192.168.0.1IPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_PRIVACY=no 之後使用sudo service network restart重啟網路服務 這樣網路設定就完成了。測試方式為：ping 192.168.0.1(DNS)跟 ping www.google.com就可以知道網路有沒有設定成功了。 c. 安裝ssh跟設定ssh資料夾權限 12345678910111213# 安裝SSHsudo yum -y install rsync openssh-server-*# 產生SSH Keyssh-keygen -t rsa -P ""# 授權SSH Keycat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 設定.ssh的權限sudo chmod 700 /home/testersudo chmod 700 /home/tester/.sshsudo chmod 644 /home/tester/.ssh/authorized_keyssudo chmod 600 /home/tester/.ssh/id_rsasudo service sshd restart d. 編輯/etc/hosts 123456sudo tee -a /etc/hosts &lt;&lt; "EOF"192.168.0.161 sparkServer0192.168.0.162 sparkServer1192.168.0.163 sparkServer2192.168.0.164 sparkServer3EOF e. 編輯/etc/hostname 12sudo vi /etc/hostname# 對應的電腦修改成對應的名稱 f. 斷掉防火牆 12sudo systemctl stop firewalldsudo systemctl disable firewalld 開始部署i. 關掉ip v6 12345sudo tee -a /etc/sysctl.conf &lt;&lt; "EOF"net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1EOF ii. 下載檔案並移到適當位置 1234567891011121314151617181920212223242526272829303132# 下載並安裝javacurl -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u101-b13/jdk-8u101-linux-x64.rpm -o jdk-8u101-linux-x64.rpmsudo yum install -y jdk-8u101-linux-x64.rpm# 下載並部署Hadoopcurl -v -j -k -L http://apache.stu.edu.tw/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz -o hadoop-2.6.4.tar.gztar -zxvf hadoop-2.6.4.tar.gzsudo mv hadoop-2.6.4 /usr/local/hadoopsudo chown -R tester /usr/local/hadoop# 下載並部署zookeepercurl -v -j -k -L http://apache.stu.edu.tw/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz -o zookeeper-3.4.8.tar.gztar -zxvf zookeeper-3.4.8.tar.gzsudo mv zookeeper-3.4.8 /usr/local/zookeepersudo chown -R tester /usr/local/zookeeper# 下載並部署HBasecurl -v -j -k -L http://apache.stu.edu.tw/hbase/0.98.20/hbase-0.98.20-hadoop2-bin.tar.gz -o hbase-0.98.20-hadoop2-bin.tar.gztar -zxvf hbase-0.98.20-hadoop2-bin.tar.gzsudo mv hbase-0.98.20-hadoop2 /usr/local/hbasesudo chown -R tester /usr/local/hbase# 下載並部署phoenixcurl -v -j -k -L http://apache.stu.edu.tw/phoenix/phoenix-4.7.0-HBase-1.1/bin/phoenix-4.7.0-HBase-1.1-bin.tar.gz -o phoenix-4.7.0-HBase-1.1-bin.tar.gztar -zxvf phoenix-4.7.0-HBase-1.1-bin.tar.gz## phoenix部署比較不同，後面再處理# 下載並部署scalacurl -v -j -k -L http://downloads.lightbend.com/scala/2.10.6/scala-2.10.6.tgz -o scala-2.10.6.tgztar -zxvf scala-2.10.6.tgzsudo mv scala-2.10.6 /usr/local/scalasudo chown -R tester /usr/local/scala# 下載並部署sparkcurl -v -j -k -L http://apache.stu.edu.tw/spark/spark-1.6.2/spark-1.6.2-bin-hadoop2.6.tgz -o spark-1.6.2-bin-hadoop2.6.tgztar -zxvf spark-1.6.2-bin-hadoop2.6.tgzsudo mv spark-1.6.2-bin-hadoop2.6 /usr/local/sparksudo chown -R tester /usr/local/spark iii. 環境變數設置 1234567891011121314151617181920212223242526272829303132333435sudo tee -a /etc/bashrc &lt;&lt; "EOF"# JAVAexport JAVA_HOME=/usr/java/jdk1.8.0_101# HADOOPexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"export HADOOP_PREFIX=$HADOOP_HOMEexport HADOOP_PID_DIR=$HADOOP_HOME/pids/# ZOOKEEPERexport ZOOKEEPER_HOME=/usr/local/zookeeper# HBASEexport HBASE_HOME=/usr/local/hbaseexport HBASE_CLASSPATH=$HBASE_HOME/confexport HBASE_PID_DIR=$HBASE_HOME/pids export HBASE_MANAGES_ZK=false# PHOENIXexport PHOENIX_HOME=/usr/local/phoenixexport PHOENIX_CLASSPATH=$PHOENIX_HOME/libexport PHOENIX_LIB_DIR=$PHOENIX_HOME/lib# SCALAexport SCALA_HOME=/usr/local/scala# SPARKexport SPARK_HOME=/usr/local/spark# PATHexport PATH=$PATH:$JAVA_HOME:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$HBASE_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PHOENIX_HOME/binEOFsource /etc/bashrc iv. 配置Hadoop a. core-site.xml用vi $HADOOP_CONF_DIR/core-site.xml編輯，改成下面這樣： 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://sparkServer0:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; b. mapred-site.xml先用cp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-site.xml，然後用vi $HADOOP_CONF_DIR/mapred-site.xml編輯，改成下面這樣： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; c. hdfs-site.xml用vi $HADOOP_CONF_DIR/hdfs-site.xml編輯，改成下面這樣： 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/hadoop/tmp/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/hadoop/tmp/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///usr/local/hadoop/tmp/name/chkpt&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 建立node需要的資料夾： 123mkdir -p $HADOOP_HOME/tmpmkdir -p $HADOOP_HOME/tmp/datamkdir -p $HADOOP_HOME/tmp/name d. yarn-site.xml用vi $HADOOP_CONF_DIR/yarn-site.xml編輯，改成下面這樣： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;sparkServer0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; e. 配置slaves 123456# 傳入slaves的電腦名稱tee $HADOOP_CONF_DIR/slaves &lt;&lt; "EOF"sparkServer1sparkServer2sparkServer3EOF ii. 配置Zookeeper先用cp $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg，然後用vi $ZOOKEEPER_HOME/conf/zoo.cfg編輯，改成下面這樣： 123456789dataDir=/usr/local/zookeeper/dataserver.1=sparkServer0:2888:3888server.1=sparkServer1:2888:3888server.1=sparkServer2:2888:3888# 接著創立需要的資料夾，並新增檔案mkdir $ZOOKEEPER_HOME/datatee $ZOOKEEPER_HOME/data/myid &lt;&lt; "EOF"1EOF 在sparkServer2跟sparkServer3分別設定為2跟3。 iii. 配置HBase用vi $HBASE_HOME/conf/hbase-site.xml編輯，改成下面這樣： 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;sparkServer0:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://sparkServer0:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sparkServer0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;file:///usr/local/zookeeper/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 接著，用cp $HADOOP_CONF_DIR/slaves $HBASE_HOME/conf/regionservers複製hadoop的slaves iv. 配置phoenix 1234567891011121314151617# 縮短名稱mv phoenix-4.7.0-HBase-1.1-bin phoenix-4.7.0# 複製lib檔案到HBase/lib下sudo cp phoenix/phoenix-4.7.0-HBase-1.1-server.jar $HBASE_HOME/lib# 複製hbase設定到phoenix下cp $HBASE_HOME/conf/hbase-site.xml $PHOENIX_HOME/bincp $HBASE_HOME/conf/hbase-env.sh $PHOENIX_HOME/bin# 複製lib檔, binsudo mkdir /usr/local/phoenixsudo chown -R tester $PHOENIX_HOMEmkdir $PHOENIX_HOME/libcp phoenix-4.7.0/phoenix-4.7.0-HBase-1.1-client-spark.jar $PHOENIX_HOME/lib/phoenix-4.7.0-HBase-1.1-client-spark.jarcp phoenix-4.7.0/phoenix-4.7.0-HBase-1.1-client.jar $PHOENIX_HOME/lib/phoenix-4.7.0-HBase-1.1-client.jarcp -R phoenix-4.7.0/bin $PHOENIX_HOMEcp -R phoenix-4.7.0/examples $PHOENIX_HOMEcp phoenix-4.7.0/LICENSE $PHOENIX_HOME/LICENSEchmod +x $PHOENIX_HOME/bin/*.py 並且在用vi $PHOENIX_HOME/bin/hbase-site.xml加入下面的設定 1234&lt;property&gt;&lt;name&gt;fs.hdfs.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;/value&gt;&lt;/property&gt; v. 配置scala and spark 12345678910111213# 複製hadoop的slavescp $HADOOP_CONF_DIR/slaves $SPARK_HOME/conf/slaves# 複製檔案cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh# 傳入設定sudo tee -a $SPARK_HOME/conf/spark-env.sh &lt;&lt; "EOF"export HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopSPARK_MASTER_IP=sparkServer0SPARK_LOCAL_DIRS=/usr/local/sparkSPARK_DRIVER_MEMORY=2GEOF vi. slaves的部署 因為是VM，所以剩下的就是把映像檔clone複製成各個nodes，然後針對需要個別配置的地方做配置： 1234567891011121314# 改hostnamesudo vi /etc/hostname# 改網路設定sudo vi /etc/sysconfig/network-scripts/ifcfg-eno16777736# 配置玩各台電腦，並透過`sudo service network restart`重啟網路服務後# 生成新的SSH keyssh-keygen -t rsa -P ""# 在sparkServer0，把他的ssh key傳到各台電腦去tee run.sh &lt;&lt; "EOF"#!/bin/bashfor hostname in `cat $HADOOP_CONF_DIR/slaves`; do ssh-copy-id -i ~/.ssh/id_rsa.pub $hostnamedoneEOF 啟動hadoop server / zookeeper server / hbase server / 12345678910# 執行hadoop的namenode formathdfs namenode -format # 啟動hadoop serverstart-dfs.sh &amp; start-yarn.sh# 啟動zookeeper serverzkServer.sh startssh tester@cassSpark2 "zkServer.sh start"ssh tester@cassSpark3 "zkServer.sh start"# 啟動hbase serverstart-hbase.sh 測試i. Hadoop MapReduce例子 - pi estimation 1234hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar pi 10 1000# output會像下面這樣# Job Finished in 2.413 seconds# Estimated value of Pi is 3.14800000000000000000 ii. zookeeper再來是測試看看zookeeper是否有部署成功，先輸入zkCli.sh -server cassSpark1:2181,cassSpark2:2181,cassSpark3:2181可以登錄到zookeeper的server上，如果是正常運作會看到下面的訊息： 1[zk: cassSpark1:2181,cassSpark2:2181,cassSpark3:2181(CONNECTED) 0] 此時試著輸入看看create /test01 abcd，然後輸入ls /看看是否會出現[test01, zookeeper] 如果是，zookeeper就是設定成功，如果中間有出現任何錯誤，則否 最後用delete /test01做刪除即可，然後用quit離開。 iii. HBase鍵入hbase shell會出現下面的訊息： 12345678910SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Type "exit&lt;RETURN&gt;" to leave the HBase ShellVersion 1.1.5, r239b80456118175b340b2e562a5568b5c744252e, Sun May 8 20:29:26 PDT 2016hbase(main):001:0&gt; 測試建表、塞資料、擷取資料跟刪除表(#後面是會出現的訊息)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 建表create 'testData','cf' # 0 row(s) in 1.3420 seconds# # =&gt; Hbase::Table - testData# 塞資料put 'testData','row1','cf:a','value1' # 0 row(s) in 0.1170 secondsput 'testData','row2','cf:b','value2' # 0 row(s) in 0.0130 secondsput 'testData','row3','cf:c','value3' # 0 row(s) in 0.0240 seconds# 列出所有表list# TABLE# testData# 1 row(s) in 0.0040 seconds# # =&gt; ["testData"]# 列出資料scan 'testData'# ROW COLUMN+CELL# row1 column=cf:a, timestamp=1469372859574, value=value1# row2 column=cf:b, timestamp=1469372859613, value=value2# row3 column=cf:c, timestamp=1469372860195, value=value3# 3 row(s) in 0.0450 seconds# 擷取特定資料get 'testData','row1' # COLUMN CELL# cf:a timestamp=1469372859574, value=value1# 1 row(s) in 0.0200 seconds# 刪除表## 先無效表disable 'testData'## 再刪除表drop 'testData'# 列出所有表list# TABLE# 0 row(s) in 0.0070 seconds# # =&gt; [] 最後可以用exit離開hbase shell。 iv. phoenix 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 創表SQLcat $PHOENIX_HOME/examples/STOCK_SYMBOL.sql# -- creates stock table with single row# CREATE TABLE IF NOT EXISTS STOCK_SYMBOL (SYMBOL VARCHAR NOT NULL PRIMARY KEY, COMPANY VARCHAR);# UPSERT INTO STOCK_SYMBOL VALUES ('CRM','SalesForce.com');# SELECT * FROM STOCK_SYMBOL;# create tablepsql.py sparkServer0:2181 $PHOENIX_HOME/examples/STOCK_SYMBOL.sql# SLF4J: Class path contains multiple SLF4J bindings.# SLF4J: Found binding in [jar:file:/usr/local/phoenix/lib/phoenix-4.7.0-HBase-1.1-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.# 16/07/24 23:25:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable# no rows upserted# Time: 1.246 sec(s)# # 1 row upserted# Time: 0.102 sec(s)# # SYMBOL COMPANY# ---------------------------------------- ----------------------------------------# CRM SalesForce.com# Time: 0.028 sec(s)# 要插入的資料cat $PHOENIX_HOME/examples/STOCK_SYMBOL.csv# AAPL,APPLE Inc.# CRM,SALESFORCE# GOOG,Google# HOG,Harlet-Davidson Inc.# HPQ,Hewlett Packard# INTC,Intel# MSFT,Microsoft# WAG,Walgreens# WMT,Walmart# insert datapsql.py -t STOCK_SYMBOL sparkServer0:2181 $PHOENIX_HOME/examples/STOCK_SYMBOL.csv# SLF4J: Class path contains multiple SLF4J bindings.# SLF4J: Found binding in [jar:file:/usr/local/phoenix/lib/phoenix-4.7.0-HBase-1.1-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.# 16/07/24 23:32:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable# csv columns from database.# CSV Upsert complete. 9 rows upserted# Time: 0.074 sec(s)# 查詢資料sqlline.py sparkServer0:2181# Setting property: [incremental, false]# Setting property: [isolation, TRANSACTION_READ_COMMITTED]# issuing: !connect jdbc:phoenix:sparkServer0:2181 none none org.apache.phoenix.jdbc.PhoenixDriver# Connecting to jdbc:phoenix:sparkServer0:2181# SLF4J: Class path contains multiple SLF4J bindings.# SLF4J: Found binding in [jar:file:/usr/local/phoenix/lib/phoenix-4.7.0-HBase-1.1-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]# SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.# 16/07/24 23:34:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable# Connected to: Phoenix (version 4.7)# Driver: PhoenixEmbeddedDriver (version 4.7)# Autocommit status: true# Transaction isolation: TRANSACTION_READ_COMMITTED# Building list of tables and columns for tab-completion (set fastconnect to true to skip)...# 85/85 (100%) Done# Done# sqlline version 1.1.8### 計算COUNT# 0: jdbc:phoenix:sparkServer0:2181&gt; select count(*) from STOCK_SYMBOL;## +-----------+## | COUNT(1) |## +-----------+## | 9 |## +-----------+## 1 row selected (0.046 seconds)### 拉出表# 0: jdbc:phoenix:sparkServer0:2181&gt; select * from STOCK_SYMBOL;## +---------+-----------------------+## | SYMBOL | COMPANY |## +---------+-----------------------+## | AAPL | APPLE Inc. |## | CRM | SALESFORCE |## | GOOG | Google |## | HOG | Harlet-Davidson Inc. |## | HPQ | Hewlett Packard |## | INTC | Intel |## | MSFT | Microsoft |## | WAG | Walgreens |## | WMT | Walmart |## +---------+-----------------------+## 9 rows selected (0.03 seconds) 離開請按CTRL+Z或是用!quit，輸入exit()是沒用的。接下來，我們試試看在hbase裡面看不看的到我們剛剛插入的表，打hbase shell進入，然後開始測試： 12345678910111213141516171819202122# scan看看就可以發現表已經存進來了# hbase(main):001:0&gt; scan 'STOCK_SYMBOL'## ROW COLUMN+CELL## AAPL column=0:COMPANY, timestamp=1469374336318, value=APPLE Inc.## AAPL column=0:_0, timestamp=1469374336318, value=x## CRM column=0:COMPANY, timestamp=1469374336318, value=SALESFORCE## CRM column=0:_0, timestamp=1469374336318, value=x## GOOG column=0:COMPANY, timestamp=1469374336318, value=Google## GOOG column=0:_0, timestamp=1469374336318, value=x## HOG column=0:COMPANY, timestamp=1469374336318, value=Harlet-Davidson Inc.## HOG column=0:_0, timestamp=1469374336318, value=x## HPQ column=0:COMPANY, timestamp=1469374336318, value=Hewlett Packard## HPQ column=0:_0, timestamp=1469374336318, value=x## INTC column=0:COMPANY, timestamp=1469374336318, value=Intel## INTC column=0:_0, timestamp=1469374336318, value=x## MSFT column=0:COMPANY, timestamp=1469374336318, value=Microsoft## MSFT column=0:_0, timestamp=1469374336318, value=x## WAG column=0:COMPANY, timestamp=1469374336318, value=Walgreens## WAG column=0:_0, timestamp=1469374336318, value=x## WMT column=0:COMPANY, timestamp=1469374336318, value=Walmart## WMT column=0:_0, timestamp=1469374336318, value=x## 9 row(s) in 0.1840 seconds 最後是進去sqlline.py去刪掉剛剛建立的表： 1234567891011121314sqlline.py sparkServer0:2181# 0: jdbc:phoenix:sparkServer0:2181&gt; DROP TABLE STOCK_SYMBOL;## No rows affected (3.556 seconds)# 列出現有的表# 0: jdbc:phoenix:sparkServer0:2181&gt; !tables ## +------------+--------------+-------------+---------------+----------+------------+----------------------------+----------+## | TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENE |## +------------+--------------+-------------+---------------+----------+------------+----------------------------+----------+## | | SYSTEM | CATALOG | SYSTEM TABLE | | | | |## | | SYSTEM | FUNCTION | SYSTEM TABLE | | | | |## | | SYSTEM | SEQUENCE | SYSTEM TABLE | | | | |## | | SYSTEM | STATS | SYSTEM TABLE | | | | |## +------------+--------------+-------------+---------------+----------+------------+----------------------------+----------+ v. spark利用spark提供的例子去測試看看 (記得要先開啟hadoop) 12345678spark-submit --class org.apache.spark.examples.SparkPi \ --deploy-mode cluster \ --master yarn \ --num-executors 3 \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 2 \ /usr/local/spark/lib/spark-examples-1.6.2-hadoop2.6.0.jar 最後就可以看到任務成功，如圖所示： 用上面的連結就可以看到任務的成功情況： Reference vm: https://dotblogs.com.tw/jhsiao/2013/09/26/120726 https://read01.com/DQO2jg.html https://read01.com/KEQ6GP.html centos: https://www.howtoforge.com/creating_a_local_yum_repository_centos http://www.serverlab.ca/tutorials/linux/network-services/creating-a-yum-repository-server-for-red-hat-and-centos/ http://ask.xmodulo.com/change-network-interface-name-centos7.html http://yenpai.idis.com.tw/archives/240-%E6%95%99%E5%AD%B8-centos-6-3-%E5%AE%89%E8%A3%9D-2%E7%B6%B2%E8%B7%AF%E8%A8%AD%E5%AE%9A%E7%AF%87?doing_wp_cron=1468854397.3452270030975341796875 hadoop, hbase, zookeeper: http://tsai-cookie.blogspot.tw/2015/09/hadoop-hbase-hive.html http://lyhpcha.pixnet.net/blog/post/60903916-hadoop%E3%80%81zookeeper%E3%80%81hbase%E5%AE%89%E8%A3%9D%E9%85%8D%E7%BD%AE%E8%AA%AA%E6%98%8E http://blog.csdn.net/smile0198/article/details/17660205 phoenix: http://www.aboutyun.com/thread-12403-1-1.html http://www.zhangshuai.top/2015/09/01/phoenix%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/ http://toutiao.com/i6222878197948613122/ https://phoenix.apache.org/faq.html http://ju.outofmemory.cn/entry/237491]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>CentOS</tag>
        <tag>Spark</tag>
        <tag>Phoenix</tag>
        <tag>Yarn</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pipe Operators in R]]></title>
    <url>%2Fposts%2F201607%2F2016-07-10-pipe-operators-in-R.html</url>
    <content type="text"><![CDATA[我在ptt分享過magrittr的文章(連結)，做為資料處理系列文章的第一篇 後來有一些額外的心得(連結)，所以又有一篇補充了一些觀念 一個大陸人Kun Ren(任堃)後來在2015年上傳了一個pipeR套件 宣稱比magrittr的pipe operator更好用，效能更佳，而且沒有模糊的界線(他的文章連結：Blog Post) 效能差異大概是快了近8倍，後面會在提及這方面 magrittr背景介紹先介紹一下magrittr的來由，根據這篇blog文章所提到的： The |&gt; operator in F# is indeed very simple: it is defined as let (|&gt;) x f = f x. However, the usefulness of this simplicity relies heavily on a concept that is not available in R: partial application. Furthermore, functions in F# almost always adhere to certain design principles which make the simple definition sufficient. Suppose that f is a function of two arguments, then in F# you may apply f to only the first argument and obtain a new function as the result — a function of the second argument alone. F#的pipe operator, |&gt;, 讓他覺得非常的方便以及實用，因此，決定把這個東西實作到R裡面 (注：其實shell script也有pipe operator, |, 而且更廣為人知。) 這就是R為什麼會有magrittr這個套件的背景原由了，所以才會有了%&gt;%, %&lt;&gt;%跟%T&gt;%這幾個operator。 (注：我這裡補充一點，其實此時開發ggplot2, plyr, dplyr等套件的Hadley已經在dplyr上 推出了%.%這個operator，而且也把dplyr的函數全部都設計第一個放data.frame做為input， 只是magrittr的推出，把整個功能用到更加完善，並且還涵蓋了alias function， 因此，Hadley後來加入了magrittr的發展計畫，就未繼續開發%.%， 在Hadley後面發展的套件都可以看到他都有直接把%&gt;% import在他的套件中， 例如：很好用的爬蟲套件rvest、開發用來處理list的purrr等都可以看到magrittr的身影。) pipe operator的基本概念假設現在有一個函數f，其input為a，一般在寫程式的時候， 我們都可以透過f(a)獲得f函數在a這一點的值 而%&gt;%可以把LHS(注：left hand side of operator)當作RHS(注：right hand side of operator)的函數的第一個input 因此，透過 a %&gt;% f 或是 a %&gt;% f() 都等同於 f(a) 同理，如果現在還有一個函數g，有兩個input，x跟y，則g(x,y)可以寫做x %&gt;% g(y) 這樣的好處是當我們有層層推疊的(時，就可以輕易地拆解開來並執行 例如： 12a_list &lt;- list(1:6, 3:5, 4:7)sort_uni_a &lt;- sort(unique(unlist(a_list))) 第二行這樣連在一起非常的難讀，但是改成下面這樣，又有效能問題(暫存變數)： 12unlist_list &lt;- unlist(a_list)sort_uni_a &lt;- sort(unique(unlist_list)) 或是改成這樣，卻沒覺得可讀性上升多少： 1sort_uni_a &lt;- sort(unique(unlist(a_list))) 但是用%&gt;%改寫就可以兼顧可讀性跟效能問題： 1sort_uni_a &lt;- a_list %&gt;% unlist %&gt;% unique %&gt;% sort 這樣就可以很容易知道他把a_list先拉成一個向量(unlist)，然後取唯一(unique) 接著排列所有元素(sort)，照著%&gt;%的順序去讀就可以順利解讀 此時，magrittr的特性就可以被看到，他省去了暫存變數，也增加了易讀性。 %&gt;%介紹舉一個簡單的例子，來說明%&gt;%的用法 123456a &lt;- 1f &lt;- function(a) a + 1f(a)a %&gt;% fa %&gt;% f()a %&gt;% f(.) 跑上面的程式可以發現，最後四個output都一樣 其實%&gt;%做的就是把左邊變數放進右邊函數裡做執行 也就是說f(a)等同於a %&gt;% f(或是上面其他三種) 但是怎麼最後一個a %&gt;% f(.)看起來怪怪的，為啥有個.在那 其實在magrittr中，.是用來代表%&gt;%前面的變數 所以a %&gt;% f(.)程式會把.的位置換成a，變成f(a) . 在magrittr的應用中，會佔很大的比例，也在Hadley往後的套件扮演重大的角色 像是do.call, Reduce第一個input是function，第二個是list 我們通常傳入list，所以此時必須用.做輸入位置的控制 再者，c, cbind, rbind會根據位置不同來決定是合併於何處 也是一個很重要的問題，因此，用 .做傳入位置的控制是必須的 針對這個，我給一段簡單的程式碼來示範： 1234567891011a_list &lt;- list(1:5, 3:7, 6:10)a_list %&gt;% do.call(rbind, .)a_list %&gt;% Reduce(cbind, .)1:5 %&gt;% rbind(3:7, .)1:5 %&gt;% rbind(., 3:7)f &lt;- function(x, a, b) a * x^2 + b1:5 %&gt;% f(., 2, 5) # 同 1:5 %&gt;% f(2, 5)1:5 %&gt;% f(2, ., 5)1:5 %&gt;% f(2, 5, .) 但是，這點被Kun Ren攻擊，這裡有一些模稜兩可的問題，後面在提。 先介紹一下{}在%&gt;%的用法，用{}括住之後， 裡面的只要不是其他%&gt;%後面的 .都代表你前面傳入的值 這樣說有點難懂，舉個例子 1231:2 %&gt;% &#123; list(cbind(9:10, .), 3:4 %&gt;% cbind(9:10, .))&#125; 123456789## [[1]]## .## [1,] 9 1## [2,] 10 2## ## [[2]]## .## [1,] 9 3## [2,] 10 4 可以看到第一個可以很直覺的解讀，9:10是跟傳入的1:2做行合併 而第二個.，因為前面有了一個新的%&gt;% 所以這一個.就被前面的3:4取代 所以第二個output變成9:10跟3:4做行合併 回到Kun Ren攻擊的點 12345f &lt;- function(x, y, z = "nothing") &#123; cat("x =", x, "\n") cat("y =", y, "\n") cat("z =", z, "\n")&#125; 1:10 %&gt;% f(mean(.), median(.))到底是 f(1:10, mean(1:10), median(1:10))還是f(mean(1:10),median(1:10))？ 你可能會根據上面的rule很快回答出來是f(1:10, mean(1:10), median(1:10)) 但是a_list %&gt;% do.call(rbind, .)這個呢？ 剛剛1:10 %&gt;% f(mean(.), median(.))不是f(1:10, mean(1:10), median(1:10))這個嗎？ 怎麼此時a_list %&gt;% do.call(rbind, .)又是do.call(rbind, a_list)？ 還有剛剛看到的1:5 %&gt;% rbind(3:7, .)這個呢？ 剛剛1:5 %&gt;% rbind(3:7, .)是rbind(1:5, 3:7, 1:5)還是rbind(3:7, 1:5)這個嗎？ 這是magrittr為了一些函數做了特例，讓整個rule出現了一些模糊的情況 但是，通常也不會去寫成a_list %&gt;% {do.call(rbind, .)}這樣去避免模糊情況({}後面說明) 不過這個方式不能說不好，只是pipeR提供了更明確的方式去避免更複雜的方式使用do.call跟Reduce %&lt;&gt;%, %T&gt;%跟%$%介紹如果懂了%&gt;%， 這個就不難了 先看簡單的例子 (add是magrittr提供用在%&gt;%上的+ (這部分請看最後面的補充)) 12345a &lt;- 1a %&gt;% add(1) # 同 a %&gt;% '+'(1) or a %&gt;% '+'(., 1)a # 1a %&lt;&gt;% add(1)a # 2 這個例子可以看的出來%&lt;&gt;%除了傳入變數之外，也會改變傳入變數的值 也就是可以把a %&lt;&gt;% add(1)看成a = a + 1 你如果有一串要做最後賦值給你傳入的變數 只需要在第一個傳導變數的operator做改變即可，舉例來說： 12345dat &lt;- data.frame(a = 1:3, b = 8:10)dat &lt;- dat %&gt;% rbind(dat)dat2 &lt;- data.frame(a = 1:3, b = 8:10)dat2 %&lt;&gt;% rbind(dat2)all.eqaul(dat, dat2) # TRUE 至於%T&gt;%，他只傳遞變數，不回傳值，通常用來傳遞到不回傳值的function上 像是plot, library, install.packages, plyr的*_ply等 這個operator可以幫你把前面做好的值賦予一個變數 並且同時做後面function的動作，舉例來說： 123456dat &lt;- data.frame(a = rep(1:3, 2), b = rnorm(6))dat2 &lt;- dat %&gt;% &#123; tapply(.$b, .$a, sum)&#125; %&gt;% &#123; data.frame(a = names(.) %&gt;% as.integer, b = .)&#125; %T&gt;% plot(.$a, .$b) 這裡dat2就是一個新的data.frame，同時，我們也把a, b的scatter plot畫出來 這部分可以用dplyr的group_by以及summarise完成 還沒提到dplyr，所以我們先用替代方法做 再來是第四個operator %$% 舉個例來說，雖然有了%&gt;%，但是dat %&gt;% {tapply(.$b, .$a, sum)}還是會覺得冗長 而且也容易忘記要放.$，此時%$%就提供了直接把前面變數的元素直接以名字做操作 再也不需要.$name這麼麻煩，直接用name做你想要的操作就好 所以，就可以簡單寫成dat %$% tapply(b, a, sum) 是不是就變得簡單的很多？ 再給一個例子說明%$%就好 12345678a &lt;- 3b &lt;- -2x &lt;- rnorm(100)y &lt;- a + b * x + rnorm(100)fit &lt;- lm(y ~ x)sigma_hat &lt;- fit %$% &#123; crossprod(residuals)/df.residual&#125; magrittr一些補充magrittr提供很多其他function的別名 像是+, *, [, [[, &lt;- rownames()等等 有興趣請去magrittr的manual查看extract的部分 這個可以讓你寫pipe chain的時候更加順手，像是 1vals &lt;- 1:3 %&gt;% data.frame(a = ., b = .^2) %&gt;% set_rownames(LETTERS[1:3]) %&gt;% lm(b ~ a, data = .) %&gt;% predict 不然你可能會這樣寫 123dat &lt;- 1:3 %&gt;% data.frame(a = ., b = .^2)rownames(dat) &lt;- LETTERS[1:3]vals = dat %&gt;% lm(b ~ a, data = .) %&gt;% predict 你可能只是要vals這個變數，你卻還要多創一個dat這個暫存變數，而中斷chain pipeR這部分是未在ptt上分享過的部分，我主要根據pipeR Tutorial的內容作介紹 我先說明差異，pipeR提供了另一個pipe operator, %&gt;&gt;%, 其中最重要的改進就是上面說的模糊問題 強制LHS一定是RHS的第一個input 例如： 1:5 %&gt;&gt;% rbind(3:7)是rbind(1:5, 3:7) 1:5 %&gt;&gt;% rbind(3:7, .)是rbind(1:5, 3:7, 1:5) 1:5 %&gt;&gt;% rbind(., 3:7)是rbind(1:5, 1:5, 3:7) 那a_list %&gt;% do.call(rbind, .)怎麼辦 pipeR設定只要寫上第一個的parameter name之後，他就會自動傳到第二個 舉例來說，a_list %&gt;&gt;% do.call(what = rbind)，他就會看成do.call(rbind, a_list) 因為第一個parameter的名字已經被用了，所以會自動傳到第二個參數去 或像是tutorial裡面提到的lm函數，打mtcars %&gt;&gt;% lm(formula = mpg ~ cyl + wt) 就會等於lm(formula = mpg ~ cyl + wt, mtcars)，其中的mtcars就會自動放在第二個data的input 這樣就規則就會沒有模糊問題，而且不需要再用{}去界定位置了 () in pipeR再來是一些新功能，pipeR提供了()去使用單行命令，做指定位置的動作 不需要再開block ({})去產生新的environment，可以增進效率 例如上面的a_list %&gt;&gt;% do.call(what = rbind)也可以寫成 a_list %&gt;&gt;% (do.call(rbind, .))或像是lm的例子也可以寫成 mtcars %&gt;&gt;% (lm(mpg ~ cyl + wt, .))，至於要用上面的方式還是這裡的模式去寫 就看個人的coding習慣了，基本上，我個人是偏好前面那一種，第二種會多出太多() 而且()在pipeR還會有其他功能，在讀程式時，有時候會有點搞混，不建議用() ()在pipeR的另一個用途就是side effect，下面就來介紹他 side effect in pipeR那什麼是side effect? 如果想要在pipe的過程中，想要作指派、輸出一些資訊 而輸出一些資訊就跟%T&gt;%的功能一樣，但指派就是新東西了 所有side effect的指令都需要用(~跟)包起來 例如： 12a_list &lt;- list(1:5, 3:7, 6:10)sort_uni_a &lt;- a_list %&gt;&gt;% unlist %&gt;&gt;% (~cat("what is it? show me:\n", .)) %&gt;&gt;% unique %&gt;&gt;% sort 這樣一來就可以在pipe的過程中把途中的變數show出來 這樣有兩個好處，一個是不需要在額外print東西出來，可以直接pipe，另一個則是方便debug 接著是指派，舉個例子，你需要把a取unique排序後，去掉全資料的平均 以往的寫法會像是： 1234a_list &lt;- list(1:5, 3:7, 6:10)a_list_to_vec &lt;- a_list %&gt;&gt;% unlistmean_list_a &lt;- mean(a_list_to_vec)sort_uni_demean_a &lt;- a_list_to_vec - mean_list_a 這樣寫就需要兩個暫存變數，a_list_to_vec跟mean_list_a，而且不能一路pipe 那改用side effect之後，就可以寫成這樣： 12a_list &lt;- list(1:5, 3:7, 6:10)sort_uni_demean_a &lt;- a_list %&gt;&gt;% unlist %&gt;&gt;% (~mean_list_a &lt;- mean(.)) %&gt;&gt;% unique %&gt;&gt;% sort %&gt;&gt;% -mean_list_a 至於%&gt;%跟%&gt;&gt;%的效能比較可以看下面這段程式(例子取自前面提及的Kun Ren部落格文章)： 12345678910111213library(magrittr)library(pipeR)library(microbenchmark)eq_f &lt;- `==`microbenchmark(magrittr = &#123; lapply(1:10000, function(i) &#123; sample(letters, 6, replace = T) %&gt;% paste(collapse = "") %&gt;% eq_f("rstats") &#125;)&#125;, pipeR = &#123; lapply(1:10000, function(i) &#123; sample(letters, 6, replace = T) %&gt;&gt;% paste(collapse = "") %&gt;&gt;% eq_f("rstats") &#125;)&#125;, times = 20L) 1234## Unit: milliseconds## expr min lq mean median uq max neval## magrittr 1215.1390 1253.883 1282.4720 1279.271 1304.1553 1400.9079 20## pipeR 377.7212 386.284 395.8731 393.892 406.5949 418.6119 20 可以看出效能改進相當顯著，大概快了3倍，隨著loop次數增加 效能改進的倍率還會再增加，這裡就留給有興趣的人自己測試了 而pipeR還有兩個函數Pipe跟pipeline，不過我就沒研究了，有興趣的人在自己看吧 以上是我這篇文章的分享]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>magrittr</tag>
        <tag>Pipe Operator</tag>
        <tag>pipeR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some installations of centos]]></title>
    <url>%2Fposts%2F201605%2F2016-05-11-initialization_of_centos.html</url>
    <content type="text"><![CDATA[Just for records. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# update systemsudo yml update# installation of sublime textcd ~/Downloadswget https://download.sublimetext.com/sublime_text_3_build_3103_x64.tar.bz2sudo tar -vxjf sublime_text_3_build_3103_x64.tar.bz2 -C /opt## make a symbolic link to the installed Sublime3sudo ln -s /opt/sublime_text_3/sublime_text /usr/bin/subl## create Gnome desktop launchersudo subl /usr/share/applications/subl.desktop## add following lines into file# [Desktop Entry]# Name=Subl# Exec=subl# Terminal=false# Icon=/opt/sublime_text_3/Icon/48x48/sublime-text.png# Type=Application# Categories=TextEditor;IDE;Development# X-Ayatana-Desktop-Shortcuts=NewWindow## [NewWindow Shortcut Group]# Name=New Window# Exec=subl -n# TargetEnvironment=Unity# installation of java 8sudo wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u66-b17/jdk-8u66-linux-x64.rpm"sudo rpm -ivh jdk-8u66-linux-x64.rpm## Setup JAVA Environment Variablessudo nano ~/.bashrc## add following lines into file# JAVA_HOME="/usr/java/jdk1.8.0_66/bin/java"# JRE_HOME="/usr/java/jdk1.8.0_66/jre/bin/java"# PATH=$PATH:$HOME/bin:JAVA_HOME:JRE_HOMEsource ~/.bashrc# installation of required packages for building Rsudo yum install libxml2-devel libxml2-static tcl tcl-devel tk tk-devel libtiff-static libtiff-devellibjpeg-turbo-devel libpng12-devel cairo-tools libicu-devel openssl-devel libcurl-devel freeglutreadline-static readline-devel cyrus-sasl-devel texlive texlive-xetex# install Rsu -c 'rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm'sudo yum updatesudo yum install R R-devel R-java## remove Rsudo rm -rf /usr/lib64/R# install MRO## Make sure the system repositories are up-to-date prior to installing Microsoft R Open.sudo yum clean all## get the installerswget https://mran.microsoft.com/install/mro/3.2.4/MRO-3.2.4.el7.x86_64.rpmwget https://mran.microsoft.com/install/mro/3.2.4/RevoMath-3.2.4.tar.gz## install MROsudo yum install MRO-3.2.4.el7.x86_64.rpm## install MKLtar -xzf RevoMath-3.2.4.tar.gzcd RevoMathsudo bash ./RevoMath.sh### Choose option 1 to install MKL and follow the onscreen prompts.## change the right of folders makes owner can install packages in librarysudo chown -R celest.celest /usr/lib64/MRO-3.2.4/R-3.2.4/lib64/Rsudo chmod -R 775 /usr/lib64/MRO-3.2.4/R-3.2.4/lib64/R# for ssh connectionsudo apt-get install rsync openssh-server-sysvinitssh-keygen -t rsa -P "" # generate SSH key# Enable SSH Keycat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys The installation of rstudio server is recorded: 12345678910111213wget https://download2.rstudio.org/rstudio-server-rhel-0.99.896-x86_64.rpmsudo yum install --nogpgcheck rstudio-server-rhel-0.99.896-x86_64.rpm## start the rstudio-serversudo rstudio-server start## start rstudio-server on bootsudo cp /usr/lib/rstudio-server/extras/init.d/redhat/rstudio-server /etc/init.d/sudo chmod 755 /etc/init.d/rstudio-serversudo chkconfig --add rstudio-server## open the firewall for rstudio-serversudo firewall-cmd --zone=public --add-port=8787/tcp --permanentsudo firewall-cmd --reload## To browse localhost:8787 for using the rstudio-server The installation of shiny server is recorded: 1234567891011121314151617## install shiny-serverwget https://download3.rstudio.org/centos5.9/x86_64/shiny-server-1.4.2.786-rh5-x86_64.rpmsudo yum install --nogpgcheck shiny-server-1.4.2.786-rh5-x86_64.rpm## start the shiny-serversudo systemctl start shiny-server## start shiny-server on bootsudo cp /opt/shiny-server/config/init.d/redhat/shiny-server /etc/init.d/sudo chmod 755 /etc/init.d/shiny-serversudo chkconfig --add shiny-server## open the firewall for shiny-serversudo firewall-cmd --zone=public --add-port=3838/tcp --permanentsudo firewall-cmd --reload## the server file is in /srv/shiny-server## there are some examples, you can browser localhost:3838,## localhost:3838/sample-apps/hello and localhost:3838/sample-apps/rmd Also, the installation of mongoDB is recorded: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071## Configure the package management systemsudo subl /etc/yum.repos.d/mongodb-org-3.2.repo## add following lines into file# [mongodb-org-3.2]# name=MongoDB Repository# baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.2/x86_64/# gpgcheck=1# enabled=1# gpgkey=https://www.mongodb.org/static/pgp/server-3.2.asc## install mongoDBsudo yum install -y mongodb-org## prevent updating version of mongodbsudo subl /etc/yum.conf## add following line into file# exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools## start mongod on bootsudo chkconfig mongod on## increase the soft limit of number of process for mongodsudo subl /etc/security/limits.conf## add following line into file (third line must be added, other lines are optional.)# mongod soft nofile 64000# mongod hard nofile 64000# mongod soft nproc 32000# mongod hard nproc 32000## to disable transparent huge pages# https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/sudo subl /etc/init.d/disable-transparent-hugepages## add following line into file# #!/bin/sh# ### BEGIN INIT INFO# # Provides: disable-transparent-hugepages# # Required-Start: $local_fs# # Required-Stop:# # X-Start-Before: mongod mongodb-mms-automation-agent# # Default-Start: 2 3 4 5# # Default-Stop: 0 1 6# # Short-Description: Disable Linux transparent huge pages# # Description: Disable Linux transparent huge pages, to improve# # database performance.# ### END INIT INFO## case $1 in# start)# if [ -d /sys/kernel/mm/transparent_hugepage ]; then# thp_path=/sys/kernel/mm/transparent_hugepage# elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then# thp_path=/sys/kernel/mm/redhat_transparent_hugepage# else# return 0# fi## echo 'never' &gt; $&#123;thp_path&#125;/enabled# echo 'never' &gt; $&#123;thp_path&#125;/defrag## unset thp_path# ;;# esac## make it executablesudo chmod 755 /etc/init.d/disable-transparent-hugepages## run it on bootsudo chkconfig --add disable-transparent-hugepages## Give a permissive policy on your /mongo/ directory that permits access to daemons (like the mongod service.).# http://stackoverflow.com/questions/30182016/unable-to-start-mongodb-3-0-2-service-on-centos-7# https://www.centos.org/docs/5/html/5.1/Deployment_Guide/sec-sel-enable-disable.htmlsudo subl /etc/sysconfig/selinux# set SELINUX=disabled Automatically enabling your network connection at startup on CentOS 7: 12# In my case, it is ifcfg-eno1. It may be different for other machine.sudo sed -i -e 's@^ONBOOT=no@ONBOOT=yes@' /etc/sysconfig/network-scripts/ifcfg-eno1 Installation of ftp server on CentOS 7: 1234567891011121314sudo yum install -y vsftpd# edit the environment of vsftpdsudo subl /etc/vsftpd/vsftpd.conf# stop the anonymous loggingsudo sed -i -e 's@^anonymous_enable=YES@anonymous_enable=NO@' /etc/vsftpd/vsftpd.conf# start the service and start on bootsudo service vsftpd startchkconfig vsftpd on## open the firewall for xrdpsudo firewall-cmd --zone=public --add-port=21/tcp --permanentsudo firewall-cmd --reload Installation of xrdp on CentOS 7 / RHEL 7 (remote desktop from windows): 123456789101112131415161718sudo subl /etc/yum.repos.d/xrdp.repo## add following lines into file# [xrdp]# name=xrdp# baseurl=http://li.nux.ro/download/nux/dextop/el7/x86_64/# enabled=1# gpgcheck=0sudo yum install -y xrdp tigervnc-server## open the firewall for xrdpsudo firewall-cmd --zone=public --add-port=3389/tcp --permanentsudo firewall-cmd --reload# start servicesudo systemctl start xrdp.service# enable service on bootsudo systemctl enable xrdp.service## Attention: the bpp color must be set to be 24bit. (in windows.) Mount another network disk: 1234567sudo mount -t cifs -o username="xxxxxxx",password="yyyyyyy" //ip-address/folder /mnt/folder# to mount on bootsubl /etc/fstab## add following lines into file# //ip-address/folder /mnt/folder username=xxxxxxx,password=yyyyyyy,uid=1000,gid=1000,sec=ntlm,iocharset=utf8 0 0sudo mount -a]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of centos on fake raid machine (intel RST)]]></title>
    <url>%2Fposts%2F201605%2F2016-05-10-centos-fakeraid.html</url>
    <content type="text"><![CDATA[I try to install Unix system on a machine with fake raid supported by intel RST in BIOS.But I spent 36 hours to install ubuntu 16.04, it did not come to succeed.Accidentally, I saw a massage that installation of centos on fake raid is easier than ubuntu,so I try to install and succeed. The reference website is PowerRC. Simply record the steps: Build the raid in the BIOS. (On my X99 board, it is to set controller in RAID mode and restart.Then find the Intel RST in advanced page. Follow the lead and construct a raid 5 array.)And enable the CSM (compatibility supportive modules) option. I use centos 7 disc to boot in UEFI mode. (Intel RST only support the UEFI BIOS for X99 chipset.)Using the manual partition to build /boot (about 500M), /boot/efi (about 500M), swap and / and starting installation. Reboot and use centos!]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>raid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of MRO in ubuntu]]></title>
    <url>%2Fposts%2F201604%2F2016-04-22-Installation_of_RRO_in_ubuntu.html</url>
    <content type="text"><![CDATA[Revolution R Open (Now named Microsoft R Open) provides Intel MKL multi-threaded BLAS.This post is to record the installation of MRO in ubuntu 16.04. First, go to the official website of MRO to check the download link (Link).The installation is refer to the manual at the website (Link). 12345678910111213# download the installation filewget https://mran.revolutionanalytics.com/install/mro/3.2.4/MRO-3.2.4-Ubuntu-15.4.x86_64.debwget https://mran.revolutionanalytics.com/install/mro/3.2.4/RevoMath-3.2.4.tar.gz# install MROsudo dpkg -i MRO-3.2.4-Ubuntu-15.4.x86_64.deb# unpack MKLtar -xzf RevoMath-3.2.4.tar.gzcd RevoMath# install MKLsudo bash ./RevoMath.shsudo chown -R celest.celest /usr/lib64/MRO-3.2.4/R-3.2.4/lib/Rsudo chmod -R 775 /usr/lib64/MRO-3.2.4/R-3.2.4/lib/R]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>R</tag>
        <tag>ubuntu</tag>
        <tag>BLAS</tag>
        <tag>Microsoft R Open</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of mongodb in ubuntu]]></title>
    <url>%2Fposts%2F201604%2F2016-04-22-Installation_of_mongodb_in_ubuntu.html</url>
    <content type="text"><![CDATA[mongodb is a noSQL database. I use it to construct the vd database. 123456789101112131415161718192021222324252627282930313233# install mongodbsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927echo "deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.listsudo apt-get updatesudo apt-get install -y mongodb-org# test whether mongodb can be implementedsudo service mongod start# # to new file 'mongod.service' in /lib/systemd/system with content:# [Unit]# Description=High-performance, schema-free document-oriented database# Documentation=man:mongod(1)# After=network.target## [Service]# Type=forking# User=mongodb# Group=mongodb# RuntimeDirectory=mongod# PIDFile=/var/run/mongod/mongod.pid# ExecStart=/usr/bin/mongod -f /etc/mongod.conf --pidfilepath /var/run/mongod/mongod.pid --fork# TimeoutStopSec=5# KillMode=mixed## [Install]# WantedBy=multi-user.target# # Reference:# http://askubuntu.com/questions/690993/mongodb-3-0-2-wont-start-after-upgrading-to-ubuntu-15-10# port to mongodbiptables -A INPUT -p tcp --dport 27017 -j ACCEPT# check whether it successcat /var/log/mongodb/mongod.log use subl /etc/mongod.conf to edit the configuration of mongodb. The default file: 1234567891011121314151617181920212223242526272829303132333435363738394041# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.storage: dbPath: /var/lib/mongodb journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log# network interfacesnet: port: 27017 bindIp: 127.0.0.1#processManagement:#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options:#auditLog:#snmp: After some edits: 123456789101112131415161718192021222324252627282930313233343536373839404142# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.storage: dbPath: /var/lib/mongodb journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log# network interfacesnet: port: 27017 bindIp: 127.0.0.1## for remote connection, bind_ip need to be set 0.0.0.0.#processManagement:#security:security: authorization: enable#operationProfiling:#replication:#sharding:## Enterprise-Only Options:#auditLog:#snmp: For security, to add admin user and create users: 123456789101112131415161718192021222324252627282930313233343536373839use admin# the user managing usersdb.createUser( &#123; user: "adminname", pwd: "password", roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;)# dbOwner# read/write all databases userdb.createUser( &#123; user: "adminname", pwd: "password", roles: [ &#123; role: "dbOwner", db: "admin" &#125; ] &#125;)# read/write all databases userdb.createUser( &#123; user: "adminname", pwd: "password", roles: [ &#123; role: "readWriteAnyDatabase", db: "admin" &#125; ] &#125;)# general usersdb.createUser( &#123; user: "adminname", pwd: "password", roles: [ &#123; role: "read", db: "reporting" &#125;, &#123; role: "read", db: "products" &#125;, &#123; role: "read", db: "sales" &#125;, &#123; role: "readWrite", db: "accounts" &#125; ] &#125;)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>ubuntu</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Initialization of ubuntu]]></title>
    <url>%2Fposts%2F201604%2F2016-04-11-initialization_of_ubuntu.html</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546sudo add-apt-repository ppa:webupd8team/sublime-text-3sudo apt-get updatesudo apt-get install sublime-text-installersudo add-apt-repository ppa:webupd8team/java &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install oracle-java8-installer &amp;&amp; sudo apt-get install oracle-java8-set-defaultapt-cache search readline xorg-dev &amp;&amp; sudo apt-get install libreadline6 libreadline6-dev xorg-dev tcl8.6-dev tk8.6-dev libtiff5 libtiff5-dev libjpeg-dev libpng12-dev libcairo2-dev libglu1-mesa-dev libgsl0-dev libicu-dev R-base R-base-dev libnlopt-dev libstdc++6 build-essential libcurl4-openssl-dev libxml2-dev aptitude r-base r-base-dev libnlopt-dev libstdc++6 build-essential libcurl4-openssl-dev libxml2-dev libssl-dev# installation of rstudio-serversudo apt-get install gdebi-corewget https://download2.rstudio.org/rstudio-server-0.99.896-amd64.debsudo gdebi rstudio-server-0.99.896-amd64.debsudo cp /usr/lib/rstudio-server/extras/init.d/debian/rstudio-server /etc/init.d/sudo apt-get install sysv-rc-confsudo sysv-rc-conf rstudio-server on# check whether rstudio-server open when bootsysv-rc-conf --list rstudio-server# port to rstudio-serveriptables -A INPUT -p tcp --dport 8787 -j ACCEPT# remove Rrm -r /usr/local/lib/R# download the installation filewget https://mran.revolutionanalytics.com/install/mro/3.2.4/MRO-3.2.4-Ubuntu-15.4.x86_64.debwget https://mran.revolutionanalytics.com/install/mro/3.2.4/RevoMath-3.2.4.tar.gz# install MROsudo dpkg -i MRO-3.2.4-Ubuntu-15.4.x86_64.deb# unpack MKLtar -xzf RevoMath-3.2.4.tar.gzcd RevoMath# install MKLsudo bash ./RevoMath.shsudo chown -R celest.celest /usr/lib64/MRO-3.2.4/R-3.2.4/lib/Rsudo chmod -R 775 /usr/lib64/MRO-3.2.4/R-3.2.4/lib/R# install texlivesudo apt-get install texinfo texlive texlive-binaries texlive-latex-base texlive-latex-extra texlive-fonts-extra# for serversudo apt-get install ssh rsync openssh-serverssh-keygen -t rsa -P "" # generate SSH key# Enable SSH Keycat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# for VM, to use unity modesudo apt-get install gnome-shell]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RcppParallel and RRO: about Intel MKL multithread]]></title>
    <url>%2Fposts%2F201601%2F2016-01-18-RcppParallel_and_RRO_about_intel_MKL_multithread.html</url>
    <content type="text"><![CDATA[Revolution R Open (Now named Microsoft R Open) provides Intel MKL multi-threaded BLAS.RcppParallel provides parallel tools for R via Intel TBB. (Note: On other platforms, it use TinyThread library.)However, does RcppParallel conflict with Intel MKL multi-threaded BLAS? We do a simple test on this issue. The cross-validation is simple but time-consuming which is a great method for testing the power of parallel.Therefore, we use leave-one-out-cross-validation to compute the mean squared errors of regression model.Leave-one-out-cross-validation is a important tool for measuring the performance of model.We use Rcpp, RcppArmadillo and RcppParallel to Leave-one-out-cross-validation. You can find the code of test below.Results: expr (Unit: seconds) min lq mean median uq max neval LOOCV_R(y, X) 44.094387 44.293443 45.217615 44.712347 45.149469 51.166242 20 ———————–: ———–: ———–: ———–: ———–: ———–: ———–: ——- fastLOOCV_noRP(y, X) 26.710105 26.811101 26.869819 26.854140 26.905504 27.094317 20 ———————–: ———–: ———–: ———–: ———–: ———–: ———–: ——- fastLOOCV(y, X) 6.411246 6.494743 6.720013 6.574210 6.697720 8.210375 20 ———————–: ———–: ———–: ———–: ———–: ———–: ———–: ——- fastLOOCV_mkl_1(y, X) 6.527082 6.997851 7.099617 7.148261 7.327877 7.465877 20 ———————–: ———–: ———–: ———–: ———–: ———–: ———–: ——- fastLOOCV_mkl_2(y, X) 6.364546 6.529459 6.564495 6.557765 6.637208 6.668344 20 According to our test, the performance of RcppArmadillo is about 1.6x as fast as R.For our concern, the performance of using default number of MKL threads (12) is very close to using 2 MKL threads,and is better than using 1 MKL threads. From this test, we think that the performance of RcppParallel isdeeply affected the number of threads Intel MKL used.We suggest not to change the number of threads Intel MKL used, just keep it default. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179library(Rcpp)library(RcppArmadillo)library(RcppParallel)sourceCpp(code = '// [[Rcpp::plugins(cpp11)]]// [[Rcpp::depends(RcppArmadillo, RcppParallel)]]#include &lt;RcppArmadillo.h&gt;#include &lt;RcppParallel.h&gt;using namespace RcppParallel;using namespace arma;using namespace Rcpp;double fastMSE_RcppArma(vec y, mat X, vec yTest, mat XTest)&#123; vec coef = solve(join_rows(ones&lt;vec&gt;(y.n_elem), X), y); return as_scalar(pow(yTest - join_rows(ones&lt;vec&gt;(yTest.n_elem), XTest)*coef, 2));&#125;struct MSE_Compute: public Worker &#123; mat&amp; X; vec&amp; y; uvec&amp; index; vec&amp; mse; MSE_Compute(mat&amp; X, vec&amp; y, uvec&amp; index, vec&amp; mse): X(X), y(y), index(index), mse(mse) &#123;&#125; void operator()(std::size_t begin, std::size_t end) &#123; for (uword row_index = begin; row_index &lt; end; row_index++) &#123; mse(row_index) = fastMSE_RcppArma(y.elem(find(index != row_index)), X.rows(find(index != row_index)), y.elem(find(index == row_index)), X.rows(find(index == row_index))); &#125; &#125;&#125;;// [[Rcpp::export]]NumericVector fastLOOCV(NumericVector yr, NumericMatrix Xr)&#123; int n = Xr.nrow(), p = Xr.ncol(); mat X(Xr.begin(), n, p, false); if (yr.size() != n) Rcpp::stop("The size of y must be equal to the number of rows of X." ); vec y(yr.begin(), n, false), mse = zeros&lt;vec&gt;(n); uvec index = linspace&lt;uvec&gt;(0, y.n_elem - 1, y.n_elem); MSE_Compute mseResults(X, y, index, mse); parallelFor(0, n, mseResults); return wrap(mse);&#125;// [[Rcpp::export]]NumericVector fastLOOCV_noRP(NumericVector yr, NumericMatrix Xr)&#123; int n = Xr.nrow(), p = Xr.ncol(); mat X(Xr.begin(), n, p, false); if (yr.size() != n) Rcpp::stop("The size of y must be equal to the number of rows of X." ); vec y(yr.begin(), n, false), mse = zeros&lt;vec&gt;(n); uvec index = linspace&lt;uvec&gt;(0, y.n_elem - 1, y.n_elem); for (uword row_index = 0; row_index &lt; y.n_elem; row_index++) &#123; mse(row_index) = fastMSE_RcppArma(y.elem(find(index != row_index)), X.rows(find(index != row_index)), y.elem(find(index == row_index)), X.rows(find(index == row_index))); &#125; return wrap(mse);&#125;')start_RppParallel &lt;- function(verbose=FALSE, numMKLthreads = 1)&#123; if (!identical(system.file(package="RevoUtilsMath"), "")) &#123; RevoUtilsMath::setMKLthreads(numMKLthreads) RcppParallel::setThreadOptions(parallel::detectCores(FALSE, TRUE)/numMKLthreads) if (verbose) &#123; cat(sprintf('RcppParallel uses %i threads.\n', parallel::detectCores(FALSE, FALSE))) cat('RevoUtilsMath is installed with multi-threaded BLAS, change the number of MKL threads.\n') cat(sprintf('Multithreaded BLAS/LAPACK libraries detected. Using %s cores for math algorithms.\n', RevoUtilsMath::getMKLthreads())) &#125; &#125;&#125;end_RppParallel &lt;- function(verbose=FALSE)&#123; if (!identical(system.file(package="RevoUtilsMath"), "")) &#123; RevoUtilsMath::setMKLthreads(parallel::detectCores(FALSE, FALSE)) if (verbose) &#123; cat('RevoUtilsMath is installed with multi-threaded BLAS, recover the number of MKL threads.\n') cat(sprintf('Multithreaded BLAS/LAPACK libraries detected. Using %s cores for math algorithms.\n', RevoUtilsMath::getMKLthreads())) &#125; &#125;&#125;fastLOOCV_mkl_1 &lt;- function(y, X, verbose = FALSE)&#123; start_RppParallel(verbose, 1) mses &lt;- fastLOOCV(y, X) end_RppParallel(verbose) return(mses)&#125;fastLOOCV_mkl_2 &lt;- function(y, X, verbose = FALSE)&#123; start_RppParallel(verbose, 2) mses &lt;- fastLOOCV(y, X) end_RppParallel(verbose) return(mses)&#125;LOOCV_R &lt;- function(y, X)&#123; mses &lt;- vector('numeric', length(y)) for (i in seq_along(y)) &#123; lmCoefs &lt;- coef(lm(y[-i] ~ X[-i, ])) mses[i] &lt;- mean((y[i] - c(1, X[i, ]) %*% lmCoefs)**2) &#125; return(mses)&#125;set.seed(10)N &lt;- 500p &lt;- N*0.95X &lt;- matrix(rnorm(N*p), N)lm_coef &lt;- replicate(p, rnorm(1, rnorm(1, 2, 3), rgamma(1,10,2)))addingCol = matrix(sample(p, p, TRUE), p/2, 2)addingCol = addingCol[addingCol[ ,1] != addingCol[ ,2], ]for (i in 1:ncol(addingCol)) X[ , addingCol[i,1]] = X[ , addingCol[i,1]] + X[ , addingCol[i,2]]y &lt;- 9 + rowSums(sweep(X, 2, lm_coef, '*')) + rnorm(N, 0, 10)st &lt;- proc.time()mses_R &lt;- LOOCV_R(y, X)proc.time() - st# user system elapsed# 39.61 0.92 43.97st &lt;- proc.time()mses_cpp1 &lt;- fastLOOCV_noRP(y, X)proc.time() - st# user system elapsed# 38.88 6.54 27.02all.equal(mses_R, as.vector(mses_cpp1)) # TRUEst &lt;- proc.time()mses_cpp2 &lt;- fastLOOCV(y, X)proc.time() - st# user system elapsed# 52.77 7.91 6.52all.equal(mses_R, as.vector(mses_cpp2)) # TRUEst &lt;- proc.time()mses_cpp3 &lt;- fastLOOCV_mkl_1(y, X)proc.time() - st# user system elapsed# 63.21 12.31 7.18all.equal(mses_R, as.vector(mses_cpp3)) # TRUEst &lt;- proc.time()mses_cpp4 &lt;- fastLOOCV_mkl_2(y, X)proc.time() - st# user system elapsed# 53.32 7.62 6.47all.equal(mses_R, as.vector(mses_cpp4)) # TRUElibrary(microbenchmark)microbenchmark(LOOCV_R(y, X), fastLOOCV_noRP(y, X), fastLOOCV(y, X), fastLOOCV_mkl_1(y, X), fastLOOCV_mkl_2(y, X), times = 20L)# Unit: seconds# expr min lq mean median uq max neval# LOOCV_R(y, X) 44.094387 44.293443 45.217615 44.712347 45.149469 51.166242 20# fastLOOCV_noRP(y, X) 26.710105 26.811101 26.869819 26.854140 26.905504 27.094317 20# fastLOOCV(y, X) 6.411246 6.494743 6.720013 6.574210 6.697720 8.210375 20# fastLOOCV_mkl_1(y, X) 6.527082 6.997851 7.099617 7.148261 7.327877 7.465877 20# fastLOOCV_mkl_2(y, X) 6.364546 6.529459 6.564495 6.557765 6.637208 6.668344 20# The information of machine: i7-5820K@4.0GHz with 32GB ram.# The information of session: RRO 3.2.3 on windows 7 64bit.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>R</tag>
        <tag>BLAS</tag>
        <tag>Microsoft R Open</tag>
        <tag>RcppParallel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[introduction to tidyr]]></title>
    <url>%2Fposts%2F201507%2F2015-07-04-tidyr.html</url>
    <content type="text"><![CDATA[這是我自己在PTT PO的文，詳細介紹tidyr，以下是正文~~ 本篇是最後一篇，主要介紹tidyr，以下是要介紹的內容： data.table:::dcast.data.table data.table:::melt tidyr:::gather tidyr:::spread tidyr:::separate 這一章主要講述注重在資料的呈現方式 - 橫表跟直表的呈現 還有一些表格整合的問題 dcast.data.table dcast提供直表加總的函數 學過統計的話，應該是contingency table (列聯表) 或是熟悉EXCEL，知道樞紐分析表，它其實就是樞紐分析表 Y就是列聯表中的列變數，X就是行變數 製作列聯表也可以說它的應用之一 這個function，需要先require(reshape2) 有人可能會問reshape2就有dcast為啥要用dcast.data.table 原因很簡單，因為dcast.data.table快更多！！ 速度直接?dcast.data.table下面例子就有，直接來簡介怎麼用 第一個input是data.table，第二個是給一個公式 舉例來說，如果公式是 Y ~ X，Y的元素會展開在列，X就會在行 第三個input是加總函數，你如果有相同類別的X, Y 它會把相同類別的值用這個函數做加總，預設是length 先用一個簡單例子來說明 123456789101112131415161718192021222324 set.seed(100) DT = expand.grid(LETTERS[1:2], LETTERS[3:4]) %&gt;% data.table %&gt;% setnames(c("col1","col2")) %&gt;% '['(rep(1:nrow(.), 2)) %&gt;% '['(,values := rpois(8,2)) DT# col1 col2 values# 1: A C 1# 2: B C 1# 3: A D 2# 4: B D 0# 5: A C 2# 6: B C 2# 7: A D 3# 8: B D 1 dcast.data.table(DT, col1~col2)# col1 C D# 1: A 2 2# 2: B 2 2 dcast.data.table(DT, col1~col2, sum)# col1 C D# 1: A 3 5# 2: B 3 1 產生資料的函數、operator，我們都講過了，往前找找看 我們專注到第一個dcast，dcast.data.table(DT, col1~col2) 可以看的出來 col1就在列，col2就在行展開，然後計算col1, col2有相同類別的length 第二個dcast就是把有相同的類別，把values做總和 但是，我們怎麼知道它加總的是values 它會告訴你自動找尋data.table，然後選定values做為加總的column 至於改法就是修改value.var這個input，舉例來說 123456789101112131415161718 DT[, values2 := rpois(8, 3)] dcast.data.table(DT, col1~col2, sum, value.var = "values")# col1 C D# 1: A 3 5# 2: B 3 1 dcast.data.table(DT, col1~col2, sum, value.var = "values2")# col1 C D# 1: A 5 7# 2: B 3 9 DT[, col3 := rep(LETTERS[5:6],,,4)] dcast.data.table(DT, col1+col2~col3, sum, value.var = "values")# col1 col2 E F# 1: A C 1 2# 2: A D 2 3# 3: B C 1 2# 4: B D 0 1 dcast.data.table說明到此 melt dcast做直表加總，melt做橫表轉直表 舉個簡單的例子 我們有數個病人，每個病人有數個觀察值 表格紀錄的樣子是 id O1 O2P1 12 18P2 13 15.. .. .. 我們想要轉成直表長這樣： id O VP1 O1 12P1 O1 18P2 O2 13P2 O2 15.. .. .. 那麼在R code可以這樣做： 1234567891011 (DT = data.table(id = paste0("P", 1:2), O1 = c(12,13), O2 = c(18,15)))# id O1 O2# 1: P1 12 18# 2: P2 13 15 (DT_long = melt(DT, "id", variable.name = "O", value.name = "V"))# id O V# 1: P1 O1 12# 2: P2 O1 13# 3: P1 O2 18# 4: P2 O2 15 melt的第一個input是data.table (註一) 第二個input是id.vars，也就是你要展開的變數名稱 第三個input是measure.vars，你要展開的變數名稱 前面例子未指定的情況下，就是全部的column 前面我還有用到variable.name 跟 value.name variable.name是指定集合其他columns之後的column name 在前例就是把 O1, O2兩個columns集合之後的變數名稱，我改成了”O” value.name是你集合其他columns之後那些變數值的column 在前例就是把 O1, O2兩個columns集合之後的變數值，我改成了”V” 註一：此指data.table:::melt，跟reshape2:::melt的差異部分 請看data.table:::melt的help 我們再看一個複雜一點的例子 123456789101112131415161718 DT = data.table(ID1 = paste0("ID1_", 1:20), ID2 = sample(paste0("ID2_", 1:20)), O1 = rnorm(20), O2 = rnorm(20), O3 = rnorm(20))## 以ID1跟ID2作為展開，其他column (O1 ~ O3)會疊成一個變數## 還會有一個新類別去label後面的value來自哪一個變數 melt(DT, c("ID1", "ID2"), c("O1", "O2", "O3"), variable.name = "O", value.name = "V")## 以ID1作為展開，其他column (O1 ~ O3)會疊成一個變數## 還會有一個新類別去label後面的value來自哪一個變數 melt(DT, "ID1", c("O1", "O2", "O3"), variable.name = "O", value.name = "V")## 以ID1作為展開，其他column (O1 ~ O2)會疊成一個變數## 還會有一個新類別去label後面的value來自哪一個變數 melt(DT, "ID1", c("O1", "O2"), variable.name = "O", value.name = "V") gather 其實就是melt，只是比較好寫 我們把melt的例子改成用gather寫 只是melt一次到位的指令用gather寫之後 要用select跟filter做 (但是我覺得gather比較好寫) 123456789101112131415161718 (DT = data.table(id = paste0("P", 1:2), O1 = c(12,13), O2 = c(18,15)))# id O1 O2# 1: P1 12 18# 2: P2 13 15 (DT_long = gather(DT, O, V, -id))# id O V# 1: P1 O1 12# 2: P2 O1 13# 3: P1 O2 18# 4: P2 O2 15 DT = data.table(ID1 = paste0("ID1_", 1:20), ID2 = sample(paste0("ID2_", 1:20)), O1 = rnorm(20), O2 = rnorm(20), O3 = rnorm(20)) gather(DT, O, V, -ID1, -ID2) gather(DT, O, V, -ID1, -ID2) %&gt;% select(-ID2) gather(DT, O, V, -ID1, -ID2) %&gt;% select(-ID2) %&gt;% filter(O!="O3") spread 提供gather的反向操作 123456 DT = data.table(id = paste0("P", 1:2), O1 = c(12,13), O2 = c(18,15)) DT_long = gather(DT, O, V, -id) DT_long %&gt;% spread(O, V)# id O1 O2# 1: P1 12 18# 2: P2 13 15 separate 把特定column做strsplit，並設定成新的變數 一個簡單的例子 12DT = data.table(x = paste0(sample(LETTERS, 5), ",", sample(LETTERS, 5)))DT %&gt;% separate(x, paste0("V", 1:2)) 這個函數要注意的是以下的程式是會出現錯誤的 12DT = data.table(x = paste0(sample(LETTERS, 5), sample(LETTERS, 5)))DT %&gt;% separate(x, paste0("V", 1:2)) separate無法分開沒有間隔字元的字串 你要分開這個只能做適當的轉換，像是： 123DT = data.table(x = paste0(sample(LETTERS, 5), sample(LETTERS, 5)))DT %&lt;&gt;% mutate(x = gsub("([A-Z])", "\\1, ", x))DT %&gt;% separate(x, paste0("V", 1:3)) %&gt;% select(-V3) 如果有人有更好的方法，麻煩告知我一下，謝謝 資料介紹套件就到這裡結束 有任何問題，歡迎在板上回文詢問，我有看到都會回覆 (麻煩盡量不要用私信，希望可以讓板眾一起看問題該怎麼解決) 有任何補充或是建議也歡迎推文或回文，感謝大家 我並沒有講到plyr的 aply, lply, dply, rply系列 其實他們跟apply, lapply, tapply, replicate相對應，只是output型式不同 如果未來有機會寫有關*ply系列函數時，我再好好介紹plyr]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>ptt</tag>
        <tag>tidyr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[introduction to dplyr]]></title>
    <url>%2Fposts%2F201507%2F2015-07-03-dplyr.html</url>
    <content type="text"><![CDATA[這是我自己在PTT PO的文，詳細介紹dplyr，以下是正文~~ 這篇重點放在dplyr plyr與dplyr有不少函數是重疊的 不過都會以dplyr為主 plyr跟dplyr有一些名字不同，但功能相似的函數 我會一併介紹 先列一下這章要介紹的函數 (沒標註的就是來自dplyr) A. 基本整理的函數：arragnge, filter, mutate, select, group_by, summarise, n B. 增併rownames為變數：add_rownames, plyr:::name_rows C. list to data.frame：as_data_frame D. by var 合併函數：join, plyr:::join, data.table:::merge, base:::merge E. col/row 合併函數：bind_rows, data.table:::rbindlist, bind_cols F. 取唯一列：distinct, data.table:::unique G. 列行運算：rowwise, plyr:::colwise H. 值映射(對應修改)：plyr:::mapvalues, plyr:::revalue I. 其他函數：summarise_each, mutate_each J. 特殊函數：plyr:::here 基本整理函數 arrange: 根據你選定的變數做排列 (可以是多個變數) filter: 根據你設定的條件做row 篩選(or selection) mutate: 根據你給定的值賦予新變數，或是變更舊變數 select: 根據給定的變數名稱做選擇，也可以做刪除變數 group_by: 根據給定變數做group，以銜接summarise summarise: 資料整併 n: 計算資料個數 用一個簡單例子來展示用法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445 set.seed(100) (dt = data.table(V1 = rpois(5, 3), V2 = sample(c("g1", "g2"), 5, 1), V3 = rnorm(5)))# V1 V2 V3# 1: 2 g1 0.3186301# 2: 2 g2 -0.5817907# 3: 3 g1 0.7145327# 4: 1 g2 -0.8252594# 5: 3 g1 -0.3598621 dt %&gt;% arrange(V1, V2, V3)# V1 V2 V3# 1: 1 g2 -0.8252594# 2: 2 g1 0.3186301# 3: 2 g2 -0.5817907# 4: 3 g1 -0.3598621# 5: 3 g1 0.7145327 dt %&gt;% filter(V1 &lt;= 2, V3 &lt; 0)# V1 V2 V3# 1: 2 g2 -0.5817907# 2: 1 g2 -0.8252594 dt %&gt;% mutate(V5 = V1 * V3, V6 = substr(V2, 2, 2), V7 = round(V3), V8 = 1L, V3 = V3 **2)# V1 V2 V3 V5 V6 V7 V8# 1: 2 g1 0.1015251 0.6372602 1 0 1# 2: 2 g2 0.3384804 -1.1635814 2 -1 1# 3: 3 g1 0.5105570 2.1435981 1 1 1# 4: 1 g2 0.6810531 -0.8252594 2 -1 1# 5: 3 g1 0.1295008 -1.0795864 1 0 1 dt %&gt;% select(V1, V2)# V1 V2# 1: 2 g1# 2: 2 g2# 3: 3 g1# 4: 1 g2# 5: 3 g1 dt %&gt;% group_by(V2) %&gt;% summarise(size_g = n(), m_V3 = mean(V3), s_V1 = sum(V1))# V2 size_g m_V3 s_V1# 1 g1 3 0.2244336 8# 2 g2 2 -0.7035251 3 上面的例子是一些簡單運用的範例 先介紹一下tbl_df, tbl_dt的class tbl_df跟tbl_dt只會列出一部分的資料 做操作時比較不會因為太多資料的輸出造成當機 要更改列出的資料量，可以這樣做 123456789101112131415161718192021222324 set.seed(100) (dt = data.table(V1 = rpois(50, 3), V2 = sample(c("g1", "g2"), 50, 1), V3 = rnorm(50))) %&gt;% tbl_dt(FALSE)# V1 V2 V3# 1 2 g1 -0.4470622# 2 2 g1 -1.7385979# 3 3 g1 0.1788648# 4 1 g1 1.8974657# 5 3 g2 -2.2719255# 6 3 g1 0.9804641# 7 4 g1 -1.3988256# 8 2 g1 1.8248724# 9 3 g2 1.3812987# 10 1 g1 -0.8388519# .. .. .. ... print(dt, n = 5)# V1 V2 V3# 1 2 g1 -0.4470622# 2 2 g1 -1.7385979# 3 3 g1 0.1788648# 4 1 g1 1.8974657# 5 3 g2 -2.2719255# .. .. .. ... 再介紹一些這些函數的其他用法 1234567891011121314151617 dt = data.table(V1 = rpois(20, 3), V2 = sample(c("g1", "g2"), 20, 1), V3 = rnorm(20), V4 = rgamma(20, 5, 3))# 你可以直接用一個你想要使用的變數放入，不須先立變數 dt %&gt;% arrange(V1*V3, V3) dt %&gt;% filter(abs(V1*V3) &gt; 1)# desc是dplyr的函數提供反向排列 dt %&gt;% arrange(V1) dt %&gt;% arrange(desc(V1))# 兩種做變數刪除的方式 (我偏好第二種) dt %&gt;% mutate(V4 = NULL) dt %&gt;% select(-V4)# select 還有提供各種特別函數於select中使用 dt %&gt;% select(starts_with("V")) dt %&gt;% select(ends_with("1")) dt %&gt;% select(contains("2")) dt %&gt;% select(matches("\\w\\d")) dt %&gt;% select(num_range("V", 1:2)) 增併rownames為變數 如標題所示，直接看範例 1234dat = data.frame(A = 1:5, row.names = paste0("City_", LETTERS[1:5]))dat %&gt;% name_rowsdat %&gt;% add_rownamesdat %&gt;% add_rownames("city") # add_rownames可以改成你要的名稱 list to data.frame as_data_frame提供比as.data.frame有效率的轉換方法 我之前也沒用過，不過看到manual寫到這個函數，就忍不住想分享一下 不過這個函數強迫list的element要有name，使用上要注意一下 1234567891011121314 library(microbenchmark) dat_list = lapply(rep(1e6, 200), rnorm) names(dat_list) &lt;- paste0("A", 1:200) microbenchmark( as_data_frame(dat_list), as.data.frame(dat_list) )#Unit: milliseconds# expr min lq mean median uq# as_data_frame(dat_list) 1.22642 1.281156 1.418296 1.311944 1.339027# as.data.frame(dat_list) 19.83196 20.199147 21.397833 20.350524 21.143335# expr max neval# as_data_frame(dat_list) 6.957693 100# as.data.frame(dat_list) 33.307182 100 看起來是沒差很多啦(汗顏，可能資料不夠大 by var 合併函數 先介紹base的merge，這個函數是用來合併兩個data.frame 除了input的兩個data.frame，還有其他五個input (其他input之後再提) a. by - 合併根據的變數b. by.x - 合併根據的變數 於第一個data.frame的名稱c. by.y - 合併根據的變數 於第二個data.frame的名稱d. all.x - 是否保留來自第一個data.frame的valuese. all.y - 是否保留來自第一個data.frame的values 註：還有一個input是 all 可以一次控制all.x跟all.y 我用簡單的範例去介紹這幾個選項 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758## 產生資料 set.seed(75) (x = data.frame(cat1 = sample(c("A", "B", NA), 5, 1), cat2 = sample(c(1, 2, NA), 5, 1), v = rpois(5, 3), stringsAsFactors = FALSE))# cat1 cat2 v# 1 A 1 4# 2 A 2 3# 3 &lt;NA&gt; NA 4# 4 B NA 4# 5 A 1 4 (y = data.frame(cat1 = sample(c("A", "B", NA), 5, 1), cat2 = sample(c(1, 2, NA), 5, 1), v = rpois(5, 3), stringsAsFactors = FALSE))# cat1 cat2 v# 1 A 2 1# 2 A 1 8# 3 &lt;NA&gt; NA 2# 4 B 2 5# 5 &lt;NA&gt; 1 3## 兩個data.frame的資料都不保留 (預設值) merge(x, y, by = c("cat1","cat2"), all.x = FALSE, all.y = FALSE)# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 1 4 8# 3 A 2 3 1# 4 &lt;NA&gt; NA 4 2## 保留第一個data.frame的全部資料 merge(x, y, by = c("cat1","cat2"), all.x = TRUE, all.y = FALSE)# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 1 4 8# 3 A 2 3 1# 4 B NA 4 NA# 5 &lt;NA&gt; NA 4 2## 保留第二個data.frame的全部資料 merge(x, y, by = c("cat1","cat2"), all.x = FALSE, all.y = TRUE)# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 1 4 8# 3 A 2 3 1# 4 B 2 NA 5# 5 &lt;NA&gt; 1 NA 3# 6 &lt;NA&gt; NA 4 2## 保留兩個data.frame全部的資料 merge(x, y, by = c("cat1","cat2"), all.x = TRUE, all.y = TRUE)# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 1 4 8# 3 A 2 3 1# 4 B 2 NA 5# 5 B NA 4 NA# 6 &lt;NA&gt; 1 NA 3# 7 &lt;NA&gt; NA 4 2 all.x跟all.y這四種組合分別對應到dplyr的四種join a. inner_join - merge(…, all.x = FALSE, all.y = FALSE)b. left_join - merge(…, all.x = TRUE , all.y = FALSE)c. right_join - merge(…, all.x = FASLE, all.y = TRUE)d. full_join - merge(…, all.x = TRUE , all.y = TRUE) 但是merge跟dplyr的join還是有些微不同 dplyr的join不會去比對by variable都是NA的情況 給一個例子就好 123456 inner_join(x, y, by = c("cat1","cat2"))# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 2 3 1# 3 &lt;NA&gt; NA 4 2# 4 A 1 4 8 PS: If you use dplyr 0.4.1, there is something wrong. You’re gonna find the output do not contain the line: 3 NA 4 2. Please update your dplyr to 0.4.2 or higher version. 至於plyr:::join就沒有這個問題 123456 join(x, y, by = c("cat1","cat2"), 'inner')# cat1 cat2 v v# 1 A 1 4 8# 2 A 2 3 1# 3 &lt;NA&gt; NA 4 2# 4 A 1 4 8 plyr:::join用法其實大同小異，它是用type去控制join方式 最後是data.table:::merge 12345678 setDT(x) setDT(y) merge(x, y, by = c("cat1","cat2"))# cat1 cat2 v.x v.y# 1: NA NA 4 2# 2: A 1 4 8# 3: A 1 4 8# 4: A 2 3 1 其實用法跟merge一模一樣，不贅述 介紹完by, all.x, 跟all.y之後，我們來介紹by.x跟by.y 用一個簡單例子： 12345678910111213 set.seed(75) x = data.frame(cat1 = sample(c("A", "B", NA), 5, 1), cat2 = sample(c(1, 2, NA), 5, 1), v = rpois(5, 3), stringsAsFactors = FALSE) y = data.frame(cat3 = sample(c("A", "B", NA), 5, 1), cat4 = sample(c(1, 2, NA), 5, 1), v = rpois(5, 3), stringsAsFactors = FALSE) merge(x, y, by.x = c("cat1","cat2"), by.y = c("cat3","cat4"))# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 1 4 8# 3 A 2 3 1# 4 &lt;NA&gt; NA 4 2 我想這個例子已經很好說明了by.x跟by.y了 接著是再dplyr怎麼做？ 12345 inner_join(x, y, by = c("cat1" = "cat3", "cat2" = "cat4"))# cat1 cat2 v.x v.y# 1 A 1 4 8# 2 A 2 3 1# 3 A 1 4 8 至於plyr:::join跟data.table:::merge就沒有支援這種功能了 dplyr還提供兩種join: semi_join跟anti_join 簡單說明一下，semi_join就是只保留第一個data.frame變數的inner_join anti_join則semi_join沒有配對的組合 這兩個有興趣再去玩玩看，這裡就不提供例子了 最後是一個實際問題 我如果要merge超過三個的df怎麼辦？ 可以參考一下 #1LaHm_aH (R_Language) 這裡完整介紹一下使用這幾個套件要怎麼解決 123456789 DF_list = replicate(5, data.frame(cat1 = sample(c("A", "B"), 5, 1), cat2 = sample(c(1, 2), 5, 1), v = rnorm(5)), simplify = FALSE)# 下列兩種會變成橫表，每一個data.frame的v都會保留 Reduce(function(x, y) merge(x, y, by = c("cat1","cat2"), all=TRUE), DF_list) Reduce(function(x, y) full_join(x, y, by = c("cat1","cat2")), DF_list)# 只保留第一個data.frame的值 join_all(DF_list, by = c("cat1","cat2"), type = "full")# 直表，保留全部的v，等同於全部做rbind join_all(DF_list, by = c("cat1","cat2", "v"), type = "full") 其實這樣每一個方法的結果都會很混亂，非常不建議，除非你知道你目標是什麼 col/row 合併函數 bind_rows跟rbindlist其實就是在做 do.call(rbind, .)或是 Reduce(rbind, .) 只是這兩個function更加有效率 如果還不懂do.call(rbind, .)跟Reduce(rbind, .)再做什麼 剛好可以利用這個機會去弄懂他們在幹嘛 1234DF_list = replicate(5, data.frame(cat1 = sample(c("A", "B"), 5, 1), cat2 = sample(c(1, 2), 5, 1), v = rnorm(5)), simplify = FALSE)bind_rows(DF_list)rbindlist(DF_list) bind_cols等同於 do.call(cbind, .) 123DT_list = lapply(1:5, function(x) data.table(rnorm(5)) %&gt;% setnames(paste0("V", x)))bind_cols(DT_list) 此為下半部部分，下部分要介紹的function如下： A. 取唯一列：distinct, data.table:::unique B. 列行運算：rowwise, plyr:::colwise C. 值映射(對應修改)：plyr:::mapvalues, plyr:::revalue D. 其他函數：summarise_each, mutate_each E. 特殊函數：plyr:::here F. function with underscore suffix 取唯一列 我在第一章 data.table提過 unique distinct是一樣的方法 就從data.table:::unique的例子開始 1234567891011121314151617181920212223242526272829 DT = data.table(A = rbinom(5, 1, 0.5), B = rbinom(5, 1, 0.5))# A B# 1: 0 0# 2: 0 1# 3: 1 0# 4: 0 1# 5: 0 0 unique(DT)# A B# 1: 0 0# 2: 0 1# 3: 1 0 distinct(DT)# A B# 1: 0 0# 2: 0 1# 3: 1 0 unique(DT, by = "A")# A B# 1: 0 0# 2: 1 0 distinct(DT, A)# A B# 1: 0 0# 2: 1 0 但是如mutate, filter等函數 distinct也可以給由變數組成的其他條件來做取唯一列的動作 給個簡單例子： 123 distinct(DT, A*B)# A B A * B# 1: 0 0 0 列行運算 mutate提供了列運算的函數 rowwise 雖然沒有apply好寫，不過效率確實好上不少 至少在chain上可以比較輕易地套用了 要介紹rowwise，還需要介紹do這個函數 do提供一些廣泛的運算方式 像是你可以根據不同變數設立模型如範例所示 12345678910111213141516171819202122 set.seed(100) DF = data.frame(A = sample(1:5, 50, TRUE), y = rnorm(50), x = rnorm(50)) models = DF %&gt;% group_by(A) %&gt;% do(model = lm(y ~ x, data = .)) models %&gt;% summarise(mse = mean(model$residuals**2))# mse# 1 0.07560431# 2 0.03617592# 3 0.71000090# 4 0.35385491# 5 1.39799410 DT = data.table(A = sample(1:5, 50, TRUE), y = rnorm(50), x = rnorm(50)) models = group_by(DT, A) %&gt;% do(model = lm(y ~ x, data = .)) models %&gt;% rowwise %&gt;% summarise(mse = mean(model$residuals**2))# mse# 1 0.8953635# 2 0.3954457# 3 0.1469698# 4 0.6303710# 5 0.2966472 data.table出來的結果不會自動group_by row 要自己手動加入rowwise，這點需要注意 do應該有更多應用，需要各位好好開發。 再來，給一個rowwise的應用： 123456789101112131415161718 set.seed(100) (DT = data.table(A = rnorm(5), B = rnorm(5), C = rnorm(5)))# A B C# 1: -0.50219235 0.3186301 0.08988614# 2: 0.13153117 -0.5817907 0.09627446# 3: -0.07891709 0.7145327 -0.20163395# 4: 0.88678481 -0.8252594 0.73984050# 5: 0.11697127 -0.3598621 0.12337950 DT %&gt;% rowwise %&gt;% do(k = c(.$A, .$B, .$C)[between(c(.$A, .$B, .$C), 0, 1)]) %&gt;% summarise(size = length(k), sums = sum(k))# size sums# 1 2 0.4085162# 2 2 0.2278056# 3 1 0.7145327# 4 2 1.6266253# 5 2 0.2403508 colwise提供每一行的做一個動作的方法 其實等同於你用sapply做 只是它還可以讓你選擇你要做的column去做 簡單例子如下： 123456789101112131415161718192021222324 set.seed(100) (DT = data.table(A = rnorm(5), B = rnorm(5), C = rnorm(5)))# A B C# 1: -0.50219235 0.3186301 0.08988614# 2: 0.13153117 -0.5817907 0.09627446# 3: -0.07891709 0.7145327 -0.20163395# 4: 0.88678481 -0.8252594 0.73984050# 5: 0.11697127 -0.3598621 0.12337950 colwise(function(x) c(mean(x), sd(x)))(DT)# A B C# 1 0.1600685 0.1890685 -0.6537084# 2 0.8140039 0.7824803 0.5610936 colwise(function(x) c(mean(x), sd(x)), .cols = .(A, B))(DT)# A B# 1 0.1600685 0.1890685# 2 0.8140039 0.7824803 f = function(x) quantile(x, c(0.05, 0.95)) colwise(f)(DT)# A B C# 1 -0.4175373 -0.7765657 -0.1433299# 2 0.7357341 0.6353522 0.6165483 值映射(對應修改) 這一節要介紹兩個很好用的函數 plyr:::mapvalues, plyr:::revalue 這兩個函數再處理資料時一定很常用到 我們再整理資料時，有時候會遇到奇異值 我們要把它改成我們想要的值時 大部分人應該都會遇到這個問題 或是有時候我們需要組別整併時也會遇到 給一個簡單的example 1234567891011121314151617181920DT = data.table(col1 = sample(c("A", "B", "N"), 30, TRUE), col2 = sample(c("A1", "B3", "C4", "U9", "Z5"), 30, TRUE), col3 = sample(-2:2, 30, TRUE))DT2 = copy(DT)## example 1 - 當初keyin資料出問題，col1出現了"N"這個值，我們要通通改成NA## 此處提供數種方法做set(DT, which(DT$col1 == "N"), "col1", NA)DT2 %&gt;% mutate(col1 = ifelse(col1 == "N", NA, col1)) # transform同理DT2 %&gt;% mutate(col1 = revalue(col1, c(N = NA)))DT2 %&gt;% mutate(col1 = mapvalues(col1, "N", NA))## example 2 - 我們想要把col2做調整 A1 改成 B3， U9改成Z5## 這裡就直接提供用revalue以及mapvalues的方法DT2 %&gt;% mutate(col2 = revalue(col2, c(A1 = "B3", U9 = "Z5")))DT2 %&gt;% mutate(col2 = mapvalues(col2, c("A1", "U9"), c("B3", "Z5")))## example 3 - 當初keyin時不小心搞錯col3的-2跟2的正負號，我們要把-2跟2做互換DT2 %&gt;% mutate(col3 = mapvalues(col3, c(-2, 2), c(2, -2))) 最後提示幾個重點， revalue可以用在factor跟character上面，不能用在數值上 mapvalues可以用在integer, double跟character上面，factor要先轉chr 其他函數 這兩個函數 summarise_each, mutate_each，我自己也很少用 我提供一些例子做操作，先用簡單的例子 12345678910 DT = data.table(X = rnorm(100), Y = rnorm(100)) DT %&gt;% summarise_each(funs(c(median(.), mean(.))))# X Y# 1: -0.02923809 0.08238550# 2: -0.09154615 0.05153845 DT = data.table(X = c(3,2), Y = c(1, 4)) DT %&gt;% mutate_each(funs(half = . / 2))# X Y# 1: 1.5 0.5# 2: 1.0 2.0 這例子只是很簡單的介紹他們的功能 可能需要時間來累積看看是否有什麼好方法去用這兩個函數 特殊函數 介紹一個我覺得很有趣的函數 here here可以幫助你再使用sapply系列時 可以使用mutate等 提一個簡單的例子 12DTs = resplicate(5, data.table(A = rnorm(5)), simplify = FALSE)DTs = lapply(DTs, here(mutate), Asq = A**2) function with underscore suffix 本章最後一個重點 怎麼使用像是filter_, select_, mutate_, …這些以_結尾的function 這些以_結尾的function提供使用者使用string作為input 中間執行的過程透過 lazyeval 來執行 (其實dplyr大部分函數都透過這個套件) 來看幾個簡單的例子 12345DT = data.table(A = rnorm(5), B = rnorm(5))DT %&gt;% select_("A")DT %&gt;% select(A)DT %&gt;% mutate(C = A*B)(DT2 = DT %&gt;% mutate_("A*B") %&gt;% setnames("A*B", "C")) 這幾個例子應該可以很簡單的解釋有_的函數要怎麼使用 最後給個.dots的用法，這個代表說你可以製作適當的string放入 1234DT = data.table(x = rnorm(100), y = rnorm(100))(DT %&lt;&gt;% mutate_(.dots = c("x^2", "y^2")) %&gt;% setnames(c("x^2", "y^2"), c("xsq", "ysq")))DT %&gt;% select_(.dots = c("y", "xsq")) 再給一個簡單的例子 1234DT = data.table(x1 = rnorm(100), x2 = rnorm(100))cmds = paste0("x", 1:2, "^2")newnames = c("xsq", "ysq")DT %&lt;&gt;% mutate_(.dots = cmds) %&gt;% setnames(cmds, newnames))]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>dplyr</tag>
        <tag>plyr</tag>
        <tag>ptt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[introduction to data.table]]></title>
    <url>%2Fposts%2F201507%2F2015-07-02-data-table.html</url>
    <content type="text"><![CDATA[這是我自己在PTT PO的文，詳細介紹data.table，以下是正文~~ data.table包含的東西很多 但是很多東西都可以被plyr, dplyr的function取代 所以data.table很多function，我都不太熟 這裡簡單介紹一下data.table 如果你想要了解更多，請自行去看manual 要了解data.table，我們可以先從package的description來看 “Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins,fast add/modify/delete of columns by group using no copies at all, listcolumns and a fast file reader (fread). Offers a natural and flexible syntax,for faster development.” 簡單翻譯一下，大資料(例如，記憶體中大小為100GB的資料)的在不創建複本下，根據 類別(group)變數進行快速整合、排列、合併、增加/修改/刪除行資料等動作。… 重點就在不創建複本，因為R修改data.frame時，會先複製一次再修改， 然後傳回複本，因此，會浪費不少記憶體，而且很容易拖累速度，因此， data.table提供這方面更有效率的操作。 (這方面的速度比較可以參考#1LeXNCKV (R_Language) [分享] 資料數據處理修改) data.table 這個函數基本上data.frame使用差不多，而且data.frame的參數都可以放進 像是很常用到的stringsAsFactors，只是data.table預設是FALSE， 這點跟data.frame不同，使用上需要注意，範例如下： 123456789 t = data.table(a = LETTERS[1:3]) str(t)# Classes ‘data.table’ and 'data.frame': 3 obs. of 1 variable:# $ a: chr "A" "B" "C"# - attr(*, ".internal.selfref")=&lt;externalptr&gt; t2 = data.frame(a = LETTERS[1:3]) str(t2)# 'data.frame': 3 obs. of 1 variable:# $ a: Factor w/ 3 levels "A","B","C": 1 2 3 第二個差異是data.table不包含rownames， 在轉換data.frame到data.table時，要注意這點 下一章會提到把rowname轉成column的函數 附註一條：data.table都包含data.frame的class 可以用在data.frame的方法都可以在data.table上實現 但是data.table還多了一個引數 “key”，我對它的解讀是一種索引的概念 而透過索引的動作都會被加速。 key可以是一個變數，也可以是多個變數，這點看個人使用。 再來，就是data.table的’[‘，這部分跟data.frame不太一樣 所以需要特別說明，但是這部分，我自己也不是很熟悉，我只能大概講過 a. 我們很常在data.frame做取多行的動作，在data.table是不可行的，舉例： 12345678910 vars = data.frame(X = rnorm(3), Y = rnorm(3), Z = rnorm(3)) vars[,1:2]# X Y# 1 -0.5677575 2.1831285# 2 -0.7161529 0.3714633# 3 1.2665120 0.7837508 vars_dt = data.table(vars) vars_dt[,1:2]# [1] 1 2 但是你想這麼做，怎麼辦？ 加上with=FALSE就好了，或是用list包住column name 1234567891011 vars_dt[,1:2,with=FALSE]# X Y# 1: -0.5677575 2.1831285# 2: -0.7161529 0.3714633# 3: 1.2665120 0.7837508 vars_dt[j=list(X, Y)]# X Y# 1: -0.5677575 2.1831285# 2: -0.7161529 0.3714633# 3: 1.2665120 0.7837508 剩下像是by, .SD, .SDcols等自行?data.table查看吧 data.table的部分就先說明到這，接下來，講一些相關的function b. setkey: 改變key的值, setnames: 改變column name，但是一樣不製造複本 c. copy: 製造data.table的複本 d. setDF: 在不製作複本下，把data.table的class改為data.frame 舉例： 12345678910111213141516171819202122 DT = data.table(X = rnorm(3), Y = rnorm(3)) str(DT)# Classes ‘data.table’ and 'data.frame': 3 obs. of 2 variables:# $ X: num -1.3738 0.167 -0.0578# $ Y: num 0.487 1.728 0.646# - attr(*, ".internal.selfref")=&lt;externalptr&gt; setDF(DT) str(DT)# 'data.frame': 3 obs. of 2 variables:# $ X: num -1.3738 0.167 -0.0578# $ Y: num 0.487 1.728 0.646 DT = data.table(X = rnorm(3), Y = rnorm(3)) tracemem(DT)# [1] "&lt;0000000006A1BE28&gt;" setDF(DT) # 沒有複製的動作 DF = data.frame(DT) retracemem(DF, retracemem(DT))# tracemem[&lt;0000000006A1BE28&gt; -&gt; 0x00000000061ec928]: ## 記憶體位置就發生改變了，就複製了DT一次 這部分可能不太懂，不過沒關係，記住一點，要轉成data.frame用setDF就好 e. setDT: setDF的反向 f. duplicated, unique duplicated提供一個跟data.table列數相等長度的邏輯值向量， TRUE代表前面有一樣的列，FALSE代表沒有 unique則是留下沒有重複的列，舉例來說： 1234567891011121314151617181920212223 set.seed(100) DT = data.table(A = rbinom(5, 1, 0.5), B = rbinom(5, 1, 0.5))# A B# 1: 0 0# 2: 0 1# 3: 1 0# 4: 0 1# 5: 0 0 duplicated(DT)# [1] FALSE FALSE FALSE TRUE TRUE unique(DT)# A B# 1: 0 0# 2: 0 1# 3: 1 0 DT[!duplicated(DT)]# A B# 1: 0 0# 2: 0 1# 3: 1 0 不過unique還有更多功能，它可以選擇變數做unique，舉例來說： 123456789 unique(DT, by = "A")# A B# 1: 0 0# 2: 1 0 unique(DT, by = "B")# A B# 1: 0 0# 2: 0 1 順便一提，dplyr的distinct，如果你input的class是data.table 它就是用unique做的 123456 library(dplyr) distinct(DT)# A B# 1: 0 0# 2: 0 1# 3: 1 0 你如果想看distinct怎麼做，可以在R上面打dplyr:::distinct_.data.table dplyr:::distinct_.data.tablefunction (.data, …, .dots){ dist &lt;- distinct_vars(.data, …, .dots = .dots) if (length(dist$vars) == 0) { unique(dist$data) } else { unique(dist$data, by = dist$vars) }} 之後提到distinct，我們再來講distinct 其他相關function像是subset, setcolorder, setorder (setorderv) 對這三個function有興趣，再去看manual，不贅述 這三個對應到dplyr的filter, select, arrange，之後我們會再提到這些 g. transform: 改變column的屬性、值等，舉例來說： 123456789101112 DT = data.table(a = 1:3, b = 2:4, c = LETTERS[1:3]) DT2 = copy(DT) DT[, b := b**2] DT2 %&lt;&gt;% transform(b = b**2) all.equal(DT, DT2) # TRUE DT %&lt;&gt;% transform(c = as.factor(c)) str(DT)# Classes ‘data.table’ and 'data.frame': 3 obs. of 3 variables:# $ a: int 1 2 3# $ b: num 4 9 16# $ c: Factor w/ 3 levels "A","B","C": 1 2 3# - attr(*, ".internal.selfref")=&lt;externalptr&gt; h. set: 用來變更特定column，某些列的值，舉個簡單的例子 12345DT = data.table(a = 1:3, b = 2:4)DT2 = copy(DT)DT[, b := 1]set(DT2,, "b", value = 1)all.equal(DT, DT2) # TRUE 一般來說都用’[‘來做，但是你如果需要用到for再來完成，再用set 還有一個function是 J，這裡就不提了，一樣請洽manual 最後，還有一個operator，’:=’，它是用來擴增data.table的column， 同樣，也不創造複本，這樣可以更快的增加column 那如果刪除怎麼辦？還記得前面學過 DT[, list(‘X’, ‘Y’)]，就用這個 再來，我們講一些data.table中其他function fread 功能可以用來取代read.table, read.csv 它可以用多種separate去分割columns，然後讀入R 而且讀入速度比read.table, read.csv快很多 但是注意，不規則的檔案會讀入失敗 這裡提幾個參數： a. sep: column跟column之間的分隔，如果是csv就是’,’， 如果是tab separated values就是’\t’ b. na.strings: 視作NA的字串，它可以是一個vector c. stringsAsFactors：是否要把字串轉成factor，預設是否 d. colClasses：各行的classes，可以自行設定 我愛用fread還有一個原因，第一個input可以直接放我要讀的字串， 但是read.table需要經過其他的方式，有點麻煩(我懶得記，其實沒記過) 舉例來說 12345678910text = "a b1 23 4"DT = fread(text)setDF(DT) # 轉成data.frame，前面學過，還記得嗎？DF = read.table(header = TRUE, text = text) # text formatDF2 = read.table(textConnection(text), header = TRUE) # file formatall.equal(DT, DF) # TRUEall.equal(DT, DF2) # TRUE fread很適合拿來讀大資料，所以有必要把table輸出成text 用文字方式處理時，讀入就變得很方便，可見 #1LegOjwB (R_Language) 還剩下 dcast.data.table, melt 跟 merge 它們會留到之後跟tidyr一起介紹 下一章重點會放在dplyr 補充： key，我也不是很熟悉，也很少用，因此，我這裡介紹的很少 如果對key有興趣，可能需要自行研究]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>ptt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[introduction to magrittr]]></title>
    <url>%2Fposts%2F201507%2F2015-07-01-magrittr.html</url>
    <content type="text"><![CDATA[這是我自己在PTT PO的文，詳細介紹magrittr，以下是正文~~ 鑒於andrew大大的提議，現下剛好有閒就來寫一系列資料整理套件的文章 版上比我熟這些套件的人也不少，如果不足的部分，再麻煩幫忙補充 目前想要介紹的套件有 (後面列一些我常用的function) magrittr : pipe operator如 ‘%&gt;%’, ‘%&lt;&gt;%’, ‘%T&gt;%’, ‘%$%’ data.table: class ‘data.table’, dcast.data.table, melt, fread,duplicated, transform, rbindlist, merge reshape2 : dcast, melt (required for data.table) plyr : name_rows, colwise, mapvalues dplyr : mutate, summarise, arrange, filter, distinct, group_by, n,select, rowwise, tbl_dt, mutate_each, summarise_each tidyr : gather, separate, spread 這篇的重點放在 magrittr，因為後面我會大量使用pipe operator來寫程式碼 我盡量每一個operator用簡單一點的方式說明，如果不太了解再麻煩告知 程式碼部分，會用block框起來 ‘%&gt;%’ 這個operator用來傳遞數值，避免過多的nest造成閱讀上的困難 像是 a_list = list(1:6, 3:5, 4:7) lapply(a_list, function(x) setdiff(sort(unique(unlist(a_list))), x)) 第二行拆解閱讀是很困難的，但是寫成 a_list %&gt;% unlist %&gt;% unique %&gt;% sort %&gt;% { lapply(a_list, setdiff, x = .)} 閱讀上會容易很多 舉一個簡單的例子，來說明 ‘%&gt;%’的用法 123456a = 1f = function(a) a + 1f(a)a %&gt;% fa %&gt;% f()a %&gt;% f(.) 跑上面的程式可以發現，最後四個output都一樣 其實 %&gt;% 做的就是把 左邊變數 放進 右邊函數裡做執行 也就是說 f(a) 等同於 a %&gt;% f (或是上面其他三種) 另外，可能會覺得 a %&gt;% f(.)會很奇怪 在magrittr中， .就是用來代表%&gt;%前面的變數 所以 a %&gt;% f(.) 程式會把.的位置換成a，變成 f(a) . 在magrittr的應用中，會佔很大的比例 像是do.call, Reduce第一個input是function，第二個是list 我們通常傳入list，所以此時必須用 . 做位置的控制 再者，c, cbind, rbind會根據位置不同來決定是合併於何處 也是一個很重要的問題，因此，用 .做傳入位置的控制是必須的 針對這個，我給一段簡單的程式碼讓你去試試看 1234567891011a_list = list(1:5, 3:7, 6:10)a_list %&gt;% do.call(rbind, .)a_list %&gt;% Reduce(cbind, .)1:5 %&gt;% rbind(3:7, .)1:5 %&gt;% rbind(., 3:7)f = function(x, a, b) a*x^2 + b1:5 %&gt;% f(., 2, 5) # 同 1:5 %&gt;% f(2, 5)1:5 %&gt;% f(2, ., 5)1:5 %&gt;% f(2, 5, .) 再者，%&gt;% 也可以傳入一個block (用{}括住的部分) 像是前面提到的 1234a_list = list(1:6, 3:5, 4:7)a_list %&gt;% unlist %&gt;% unique %&gt;% sort %&gt;% &#123; lapply(a_list, setdiff, x = .)&#125; 先說明怎麼閱讀 %&gt;%的部分 a_list %&gt;% unlist %&gt;% unique %&gt;% sort 就是 a_list把全部元素合併(unlist)，然後取唯一(unique)，接著排列所有元素(sort) 就照著%&gt;%的順序去讀就可以順利解讀 再來就是block的部分 {}括住之後，裡面的只要不是其他%&gt;%後面的 .都代表你前面傳入的值 這樣很難懂，舉個例子 1234561:2 %&gt;% &#123; list( cbind(9:10, .), 3:4 %&gt;% cbind(9:10, .) )&#125; output長這樣 [[1]] .[1,] 9 1[2,] 10 2 [[2]] .[1,] 9 3[2,] 10 4 可以看到第一個可以很直覺的解讀，9:10是跟傳入的1:2做行合併 而第二個.，因為前面有了一個新的 ‘%&gt;%’ 所以這一個.就被前面的 3:4取代 所以第二個output變成9:10跟3:4做行合併 ‘%&lt;&gt;%’ 如果懂了 %&gt;%， 這個就不難了 先看簡單的例子 (add是magrittr提供用在 %&gt;%上的 + (這部分請看最後面的補充)) 12345a = 1a %&gt;% add(1) # 同 a %&gt;% '+'(1) or a %&gt;% '+'(., 1)a # 1a %&lt;&gt;% add(1)a # 2 這個例子可以看的出來 %&lt;&gt;% 除了傳入變數之外，也會改變傳入變數的值 也就是可以把 a %&lt;&gt;% add(1) 看成 a = a + 1 你如果有一串要做最後賦值給你傳入的變數 只需要在第一個傳導變數的operator做改變即可，舉例來說： 12345dat = data.frame(a = 1:3, b = 8:10)dat = dat %&gt;% rbind(dat)dat2 = data.frame(a = 1:3, b = 8:10)dat2 %&lt;&gt;% rbind(dat2)all.eqaul(dat, dat2) # TRUE ‘%T&gt;%’ %T&gt;% 只傳遞變數，不回傳值，通常用來傳遞到不回傳值的function上 像是plot, library, install.packages, plyr的 *_ply等 這個operator可以幫你把前面做好的值賦予一個變數 並且同時做後面function的動作，舉例來說： 1234dat = data.frame(a = rep(1:3,2), b = rnorm(6))dat2 = dat %&gt;% &#123;tapply(.$b, .$a, sum)&#125; %&gt;% &#123; data.frame(a=names(.) %&gt;% as.integer, b = .) &#125; %T&gt;% plot(.$a, .$b) 這裡dat2就是一個新的data.frame，同時，我們也把a, b的scatter plot畫出來 這部分可以用dplyr的group_by以及summarise完成 還沒提到dplyr，所以我們先用替代方法做 這裡順便把第四個operator ‘%$%’一起說明 dat %&gt;% {tapply(.$b, .$a, sum)} 會不會覺得很冗長，也很容易忘記要放’.$’ 但是，’%$%’提供了直接把前面變數的元素 直接以名字做操作 再也不需要 .$name這麼麻煩，直接用 name做你想要的操作就好 因此，那行就可以簡單寫成 dat %$% tapply(b, a, sum) 是不是就變得簡單的很多？ ‘%$%’ 前面提到了，這裡就給一個例子就好 123456a = 3b = -2x = rnorm(100)y = a + b * x + rnorm(100)fit = lm(y ~ x)sigma_hat = fit %$% &#123;crossprod(residuals) / df.residual&#125; 下一章應該會介紹data.table跟reshape2，後會有期。 補充： magrittr提供很多其他function的別名 像是 ‘+’, ‘*’, ‘[‘, ‘[[‘, ‘&lt;- rownames()’等等 有興趣請去magrittr的manual查看extract的部分 這個可以讓你寫pipe chain的時候更加順手 像是 12vals = 1:3 %&gt;% data.frame(a = ., b = .^2) %&gt;% set_rownames(LETTERS[1:3]) %&gt;% lm(b ~ a, data = .) %&gt;% predict 不然你可能會這樣寫 12345dat = 1:3 %&gt;% data.frame(a = ., b = .^2)rownames(dat) = LETTERS[1:3]vals = dat %&gt;% lm(b ~ a, data = .) %&gt;% predict 你可能只是要vals這個變數，你卻還要多創一個dat這個暫存變數，而中斷chain]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>magrittr</tag>
        <tag>ptt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use xelatex in sublime text]]></title>
    <url>%2Fposts%2F201504%2F2015-04-27-how-to-use-xelatex-in-sublime-text.html</url>
    <content type="text"><![CDATA[To setup LatexTools in sublime text. You should install package control before installation of LaTexTools. Please go to the official website of package control and copy the commend of installation. Then open console in sublime text via ctrl+` and paste the commend copied from website. Now, you can start to install LatexTools in sublime text. Call install commend via ctrl+shift+p and key in install package. Enter LaTexTools to install. To use LaTexTools, you should have MikTex and SumatraPDF in windows OS or texlive and latexmk in linux. If you use windows, you should guarantee that path consist the path to xetex.exe and SumatraPDF.exe. Next, configure the settings of LaTexTools found in Preferences -&gt; Package Settings -&gt; LaTexTools and migrate settings (Select Reconfigure LaTexTools and migrate settings) first. Finally, select Settings-User and set the builder_settings as following. 12345678910111213141516171819202122"builder_settings" : &#123; "program": "xelatex", // General settings: // See README or third-party documentation // (built-ins): true shows the log of each command in the output panel "display_log" : false, "command": ["texify", "-b", "-p", "--engine=xetex", "--tex-option=\"--synctex=1\""], // Platform-specific settings: "osx" : &#123; // See README or third-party documentation &#125;, "windows" : &#123; // See README or third-party documentation &#125;, "linux" : &#123; // See README or third-party documentation &#125;&#125;]]></content>
      <categories>
        <category>Sublime Text</category>
      </categories>
      <tags>
        <tag>Sublime Text</tag>
        <tag>xelatex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiny App for Ising Model]]></title>
    <url>%2Fposts%2F201504%2F2015-04-20-shiny-app-for-ising-model.html</url>
    <content type="text"><![CDATA[Here is a demonstration for ising model with an interative interface created by R package shiny. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114library(lattice)library(shiny)app = shinyApp( ui = shinyUI(pageWithSidebar( headerPanel('Ising Model'), sidebarPanel( sliderInput('girdSize', 'Gird Size', 2, 200, 30), selectInput('updateAlgorithm', 'Algorithm', c("Metropolis-Hasting Algorithm", "Gibbs sampling"), "Metropolis-Hasting Algorithm"), numericInput('iteration', 'Iteration', 50000, 1, 4000000), numericInput('updateFreq', 'Draw Model Every N Iteration', 1000, 1, 10000), sliderInput('temperature', 'Reciprocal of Temperature', -5, 5, 2, step = 0.1), actionButton('reset', 'Reset'), actionButton('stop', 'Stop'), actionButton('start', 'Start') ), mainPanel( h3(textOutput("currentIteration")), plotOutput('IsingPlot', width = "600px", height = "600px") ) )), server = function(input, output, session) &#123; vals = reactiveValues() range_f = function(X, loc) c(X[loc[1], c(loc[2]-1, loc[2]+1)], X[c(loc[1]-1, loc[1]+1), loc[2]]) resetIsingMatrix = observe(&#123; input$reset runProcess$suspend() isolate(&#123; vals$IsingMatrix = replicate(input$girdSize+2, rbinom(input$girdSize+2, 1, .5)) vals$IsingMatrix[c(1, input$girdSize+2),] = vals$IsingMatrix[c(input$girdSize+1, 2),] vals$IsingMatrix[,c(1, input$girdSize+2)] = vals$IsingMatrix[,c(input$girdSize+1, 2)] &#125;) &#125;, priority=30) setup_to_run = observe(&#123; input$start isolate(&#123; if (is.null(vals$IsingMatrix)) &#123; vals$IsingMatrix = replicate(input$girdSize+2, rbinom(input$girdSize+2, 1, .5)) vals$IsingMatrix[c(1, input$girdSize+2),] = vals$IsingMatrix[c(input$girdSize+1, 2),] vals$IsingMatrix[,c(1, input$girdSize+2)] = vals$IsingMatrix[,c(input$girdSize+1, 2)] &#125; if (input$updateAlgorithm == "Metropolis-Hasting Algorithm") &#123; vals$algo_f = function(mat, temperature)&#123; N = nrow(mat) - 2 loc = floor(N*runif(2)) + 2 if (runif(1) &lt; exp(2*(2-sum(range_f(mat, loc) == mat[loc[1], loc[2]]))*temperature)) mat[loc[1],loc[2]] = 1 - mat[loc[1],loc[2]] mat[c(1, N+2),] = mat[c(N+1, 2),] mat[,c(1, N+2)] = mat[,c(N+1, 2)] mat &#125; &#125; else &#123; vals$algo_f = function(mat, temperature)&#123; N = nrow(mat) - 2 loc = floor(N*runif(2)) + 2 S = 2-sum(range_f(mat, loc) == mat[loc[1], loc[2]]) if (runif(1) &lt; 1/(exp(-S*temperature)**2+1)) mat[loc[1],loc[2]] = 1 - mat[loc[1],loc[2]] mat[c(1, N+2),] = mat[c(N+1, 2),] mat[,c(1, N+2)] = mat[,c(N+1, 2)] mat &#125; &#125; vals$iteration = input$iteration vals$updateFreq = input$updateFreq vals$temperature = input$temperature vals$iter = 0 &#125;) runProcess$resume() &#125;, priority=20) runProcess = observe(&#123; if (input$start == 0) return() isolate(&#123; result = vals$IsingMatrix i = 0 while (i &lt; vals$updateFreq) &#123; result = vals$algo_f(result, vals$temperature) i = i + 1 &#125; vals$IsingMatrix = result vals$iter = vals$iter + vals$updateFreq &#125;) if (isolate(vals$iter) &lt; isolate(vals$iteration)) invalidateLater(500, session) &#125;, priority=10) output$currentIteration = renderText(&#123; paste0("Current iteration: ", vals$iter) &#125;) stopProcess = observe(&#123; input$stop runProcess$suspend() &#125;) output$IsingPlot = renderPlot(&#123; levelplot(vals$IsingMatrix[2:(input$girdSize+1), 2:(input$girdSize+1)], col.regions = c("red", "green"), colorkey = FALSE, xlab = "", ylab = "") &#125;) session$onSessionEnded(function() &#123; runProcess$suspend() &#125;) &#125;)runApp(app)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>shiny</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Implementation for summing a list of matrices -- part 2]]></title>
    <url>%2Fposts%2F201504%2F2015-04-19-Implementation-for-summing-a-list-of-matrices-part-2.html</url>
    <content type="text"><![CDATA[I have showed several implementations for summing a list of matrices in the previous post. I introduce a function to be applied in do.call and test performance. 12345678910111213141516171819202122232425262728293031323334353637383940N = 10input = lapply(1:N**2, function(x) matrix(rnorm(N**2), N))a = do.call(function(...)&#123; arglist = list(...) #browser() out = arglist[[1]] for (i in 2:length(arglist )) out = out + arglist[[i]] return(out)&#125;, input)b = Reduce('+', input)for_sum = function(input)&#123; out = input[[1]] for (i in 2:length(input)) out = out + input[[i]] return(out)&#125;d = for_sum(input)all.equal(a, b)all.equal(a, d)N = 200K = 100library(microbenchmark)microbenchmark(a = do.call(function(...)&#123; arglist = list(...) out = arglist[[1]] for (i in 2:length(arglist )) out = out + arglist[[i]] return(out) &#125;, lapply(1:K, function(x) matrix(rnorm(N**2), N))), b = Reduce('+', lapply(1:K, function(x) matrix(rnorm(N**2), N))), d = for_sum(lapply(1:K, function(x) matrix(rnorm(N**2), N))), times = 20L)# Unit: milliseconds# expr min lq mean median uq max neval# a 392.7532 409.6669 431.2166 424.3133 443.7561 498.5549 20# b 399.3968 414.4893 446.8753 432.2366 444.0107 685.5244 20# d 379.0724 400.0660 411.3291 408.3251 427.2988 443.7041 20]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Reduce</tag>
        <tag>do.call</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bsxfun in R -- matrix-vector arithmetic operation]]></title>
    <url>%2Fposts%2F201504%2F2015-04-19-bsxfun-in-R.html</url>
    <content type="text"><![CDATA[The matrix-vector arithmetic operation in R is difficult to do before. We need for-loop or replicate to compute. Now, we have sweep to do. There are several examples to demonstrate. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106M = 3N = 5(mat = replicate(N, 1:M))# [,1] [,2] [,3] [,4] [,5]# [1,] 1 1 1 1 1# [2,] 2 2 2 2 2# [3,] 3 3 3 3 3(vec = 1:N)# [1] 1 2 3 4 5(sweep(mat, MARGIN=2, vec,`+`))# [,1] [,2] [,3] [,4] [,5]# [1,] 2 3 4 5 6# [2,] 3 4 5 6 7# [3,] 4 5 6 7 8(sweep(mat, MARGIN=2, vec,`*`))# [,1] [,2] [,3] [,4] [,5]# [1,] 1 2 3 4 5# [2,] 2 4 6 8 10# [3,] 3 6 9 12 15(sweep(mat, MARGIN=2, vec,`/`))# [,1] [,2] [,3] [,4] [,5]# [1,] 1 0.5 0.3333333 0.25 0.2# [2,] 2 1.0 0.6666667 0.50 0.4# [3,] 3 1.5 1.0000000 0.75 0.6(sweep(mat, MARGIN=2, vec,`-`))# [,1] [,2] [,3] [,4] [,5]# [1,] 0 -1 -2 -3 -4# [2,] 1 0 -1 -2 -3# [3,] 2 1 0 -1 -2(sweep(mat, MARGIN=2, vec, pmax))# [,1] [,2] [,3] [,4] [,5]# [1,] 1 2 3 4 5# [2,] 2 2 3 4 5# [3,] 3 3 3 4 5(sweep(mat, MARGIN=2, vec, pmin))# [,1] [,2] [,3] [,4] [,5]# [1,] 1 1 1 1 1# [2,] 1 2 2 2 2# [3,] 1 2 3 3 3## usage in scalingN = 10; p = 3dat = matrix(rnorm(N*p), N)dat_standardized = sweep(sweep(dat, 2, colMeans(dat)), 2, apply(dat, 2, sd), '/')dat_normalized = sweep(sweep(dat, 2, apply(dat, 2, min)), 2, apply(dat, 2, function(x) diff(range(x))), '/')## usage in computing kernel matrixkernelMatrix_f = function(X, center, sigma)&#123; exp(sweep(sweep(X %*% t(center), 1, rowSums(X**2)/2), 2, rowSums(center**2)/2) / (sigma**2))&#125;## compare with Rcpp and kernlablibrary(kernlab)library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]NumericMatrix kernelMatrix_cpp(NumericMatrix Xr, NumericMatrix Centerr, double sigma) &#123; uword n = Xr.nrow(), b = Centerr.nrow(), row_index, col_index; mat X(Xr.begin(), n, Xr.ncol(), false), Center(Centerr.begin(), b, Centerr.ncol(), false), KerX(X*Center.t()); colvec X_sq = sum(square(X), 1) / 2; rowvec Center_sq = (sum(square(Center), 1)).t() / 2; KerX.each_row() -= Center_sq; KerX.each_col() -= X_sq; KerX *= 1 / (sigma * sigma); KerX = exp(KerX); return wrap(KerX);&#125;')N = 8000p = 500b = 1000X = matrix(rnorm(N*p), ncol = p)center = X[sample(1:N, b),]sigma = 3all.equal(kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)@.Data, kernelMatrix_cpp(X, center, sigma))# TRUEall.equal(kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)@.Data, kernelMatrix_f(X, center, sigma))# TRUElibrary(microbenchmark)microbenchmark(Rfun = kernelMatrix_f(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), Rcpp = kernelMatrix_cpp(X, center, sigma), times = 20L)# Unit: milliseconds# expr min lq mean median uq max neval# Rfun 872.8854 981.1995 1036.2209 1074.0526 1098.9185 1132.9356 20# kernlab 773.7489 841.9028 1059.3098 862.7476 883.3874 2979.2541 20# Rcpp 490.2462 501.2993 520.1283 522.5060 532.5426 571.0949 20]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>bsxfun</tag>
        <tag>sweep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Rcpp function in doSNOW and snowfall]]></title>
    <url>%2Fposts%2F201504%2F2015-04-14-Using-Rcpp-function-in-doSNOW-and-snowfall.html</url>
    <content type="text"><![CDATA[I had been searching how to use Rcpp function in snowfall or doSNOW for a long time, but there is still not a solution. I recently come up an idea to implement. Since the error is printed when exporting the Rcpp function to nodes, I compile Rcpp function in nodes. Surprisingly, I success. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111library(data.table)library(plyr)library(dplyr)library(magrittr)library(foreach)library(doSNOW)library(snowfall)library(Rcpp)library(RcppArmadillo)RcppCode = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]List fastRidgeLM_Arma(NumericVector yr, NumericMatrix Xr, double lambda) &#123; int n = Xr.nrow(), k = Xr.ncol(); mat X(Xr.begin(), n, k, false); mat X2 = join_rows(ones&lt;mat&gt;(n, 1), X); colvec y(yr.begin(), n, false); colvec coef = solve(join_cols(X2, lambda*eye&lt;mat&gt;(k,k+1)), join_cols(y, zeros&lt;colvec&gt;(k))); colvec resid = y - X2*coef; double sig2 = sum(square(resid)) / (n-k); colvec stderrest = sqrt(sig2 * diagvec(inv((trans(X2) * X2)))); return List::create(Named("coefficients") = coef, Named("stderr") = stderrest);&#125;'sourceCpp(code = RcppCode)n = 10000p = 1000X = matrix(rnorm(n*p), n)tmp_B = matrix(0, p, p)tmp_B[sample(1:p, 2*p, replace = TRUE), sample(1:p, 2*p, replace = TRUE)] = runif(2*p)*2-1diag(tmp_B) = 1X = X %*% tmp_BY = X %*% sample(seq(-5, 5, length=p*3), p) + rnorm(n, 0, 10)X.N = scale(X)Y.N = scale(Y)lambda = seq(0, 1, length = 51)# Example by selecting regularizer in the ridge regressioncv_index_f = function(n, fold = 10)&#123; fold_n = floor(n / fold) rem = n - fold_n * fold size = rep(fold_n, fold) if(rem &gt; 0) size[1:rem] = fold_n + 1 cv_index = 1:fold %&gt;% sapply(function(i) rep(i, size[i])) %&gt;% sample(length(.)) return(cv_index)&#125;sfInit(TRUE, 2)sfExport("Y.N", "X.N", "lambda", "RcppCode")sfLibrary("Rcpp", character.only=TRUE)sfLibrary("RcppArmadillo", character.only=TRUE)sfClusterEval(sourceCpp(code = RcppCode))cl = makeCluster(2, type = "SOCK")registerDoSNOW(cl)clusterExport(cl, c("Y.N", "X.N", "lambda", "RcppCode"))clusterEvalQ(cl, library(Rcpp))clusterEvalQ(cl, library(RcppArmadillo))clusterEvalQ(cl, sourceCpp(code = RcppCode))CV_f = function(X, Y, lambda, n_fold, parallel, parPackage = "foreach")&#123; cvIndex = cv_index_f(nrow(X), n_fold) if ((parPackage == "foreach" &amp;&amp; parallel) || !parallel) &#123; if (parallel) clusterExport(cl, "cvIndex", environment()) CVScores = aaply(1:n_fold, 1, function(i)&#123; aaply(lambda, 1, function(l)&#123; tmp = fastRidgeLM_Arma(Y.N[cvIndex != i], X.N[cvIndex != i,], l) mean((Y.N[cvIndex == i] - cbind(1, X.N[cvIndex == i,]) %*% tmp$coefficients)^2) &#125;, .parallel = parallel) &#125;) if (parallel) clusterEvalQ(cl, rm(cvIndex)) &#125; else if(parPackage == "snowfall") &#123; if (parallel) sfExport("cvIndex") CVScores = aaply(1:n_fold, 1, function(i)&#123; sfSapply(lambda, function(l)&#123; tmp = fastRidgeLM_Arma(Y.N[cvIndex != i], X.N[cvIndex != i,], l) mean((Y.N[cvIndex == i] - cbind(1, X.N[cvIndex == i,]) %*% tmp$coefficients)^2) &#125;) &#125;) if (parallel) sfRemove("cvIndex") &#125; aaply(CVScores, 2, mean)&#125;library(rbenchmark)benchmark( CV_f(X, Y, lambda, 10, FALSE) , CV_f(X, Y, lambda, 10, TRUE, "foreach") , CV_f(X, Y, lambda, 10, TRUE, "snowfall") , columns = c("test", "replications", "user.self", "elapsed"), replications = 10, order = "relative")stopCluster(cl)sfStop()]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>parallel</tag>
        <tag>snowfall</tag>
        <tag>foreach</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop and python]]></title>
    <url>%2Fposts%2F201504%2F2015-04-10-hadoop-and-python.html</url>
    <content type="text"><![CDATA[A simple log for doing a job of mapreduce in python. We implement wordcount by using hadoop streaming. New two python script files named mapper.py and reducer.py, respectively. 12345678#!/usr/bin/env python## mapper.pyimport sysfor line in sys.stdin: line = line.strip() words = line.split() for word in words: print '%s\t%s' % (word, 1) 1234567891011121314151617181920212223#!/usr/bin/env python## reducer.pyfrom operator import itemgetterimport syscurrent_word = Nonecurrent_count = 0word = Nonefor line in sys.stdin: line = line.strip() word, count = line.split('\t', 1) try: count = int(count) except ValueError: continue if current_word == word: current_count += count else: if current_word: print '%s\t%s' % (current_word, current_count) current_count = count current_word = wordif current_word == word: print '%s\t%s' % (current_word, current_count) Using the example in previous article for hadoop and run hadoop streaming in the terminal:123456789101112cd ~/Downloads &amp;&amp; mkdir testData &amp;&amp; cd testDatawget http://www.gutenberg.org/ebooks/5000.txt.utf-8cd ..hdfs dfs -copyFromLocal testData/ /user/celest/hdfs dfs -ls /user/celest/testData/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \-files mapper.py,reducer.py -mapper "mapper.py -m" \-reducer "reducer.py -r" -input /user/celest/testData/* \-output /user/celest/testData-outputhdfs dfs -cat /user/celest/testData-output/part-00000 We can obtain the same result for wordcount.]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BigData</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installations of spyder2 and spyder3]]></title>
    <url>%2Fposts%2F201504%2F2015-04-09-installation-of-spyder2-and-spyder3.html</url>
    <content type="text"><![CDATA[A simple log for installation of spyder2 and spyder3. 12345678sudo apt-get install python-qt4 python-sphinx python-numpy python-scipy python-matplotlib# recommended modulessudo easy_install ipython rope pyflakes pylint pep8 psutilsudo easy_install spyder# python 3sudo apt-get install python3-pyqt4 python3-sphinx python3-numpy python3-scipy python3-matplotlibsudo easy_install3 ipython rope pylint pep8 pyflakes psutilsudo easy_install3 spyder 1234# python 2.7spyder# python 3spyder3]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Spyder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installations of rhdfs, rmr2, plyrmr and rhbase]]></title>
    <url>%2Fposts%2F201504%2F2015-04-08-installations-of-rhdfs-rmr2-plyrmr-and-hbase.html</url>
    <content type="text"><![CDATA[Rsudio provides a series of packages for the connection between R and hadoop. rhdfs provides the manipulation of HDFS in hadoop in R. rmr2 and plyrmr let user do mapreduce job in R. rhbase allow user to access data in hbase. Before using packages in R, we implement wordcount by using hadoop streaming. First, export the hadoop home in the terminal by command export HADOOP_HOME=/usr/local/hadoop/. New two R files named mapper.R and reducer.R, respectively. 12345678910111213#! /usr/bin/env Rscript## mapper.RtrimWhiteSpace &lt;- function(line) gsub("(^ +)|( +$)", "", line)splitIntoWords &lt;- function(line) unlist(strsplit(line, "[[:space:]]+"))con &lt;- file("stdin", open = "r")while (length(line &lt;- readLines(con, n = 1, warn = FALSE)) &gt; 0) &#123; line &lt;- trimWhiteSpace(line) words &lt;- splitIntoWords(line) ## **** can be done as cat(paste(words, "\t1\n", sep=""), sep="") for (w in words) cat(w, "\t1\n", sep="")&#125;close(con) 1234567891011121314151617181920212223#! /usr/bin/env Rscript# reducer.RtrimWhiteSpace &lt;- function(line) gsub("(^ +)|( +$)", "", line)splitLine &lt;- function(line) &#123; val &lt;- unlist(strsplit(line, "\t")) list(word = val[1], count = as.integer(val[2]))&#125;env &lt;- new.env(hash = TRUE)con &lt;- file("stdin", open = "r")while (length(line &lt;- readLines(con, n = 1, warn = FALSE)) &gt; 0) &#123; line &lt;- trimWhiteSpace(line) split &lt;- splitLine(line) word &lt;- split$word count &lt;- split$count if (exists(word, envir = env, inherits = FALSE)) &#123; oldcount &lt;- get(word, envir = env) assign(word, oldcount + count, envir = env) &#125; else assign(word, count, envir = env)&#125;close(con)for (w in ls(env, all = TRUE)) cat(w, "\t", get(w, envir = env), "\n", sep = "") Using the example in previous article for hadoop and run hadoop streaming in the terminal: 123456789cd ~/Downloads &amp;&amp; mkdir testData &amp;&amp; cd testDatawget http://www.gutenberg.org/ebooks/5000.txt.utf-8cd ..hdfs dfs -copyFromLocal testData/ /user/celest/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \-files mapper.R,reducer.R,/opt/intel/composer_xe_2013_sp1.3.174/compiler/lib/intel64/libiomp5.so \-mapper "mapper.R -m" -reducer "reducer.R -r" \-input /user/celest/testData/* -output /user/celest/testData2-outputhdfs dfs -cat /user/celest/testData2-output/part-00000 We can obtain the same result for wordcount. Next, we are going to install the following packages: 12345rhdfsrmrravroplyrmrrhbase Note that ravro is the data parser for avro in R. Some dependencies need to be installed in system: 12345678910111213141516171819202122232425sudo apt-get install libcurl4-openssl-dev gitR CMD javareconf # setting the environment for R# for rhbasesudo apt-get install libboost-dev libboost-test-dev libboost-program-options-dev libboost-system-dev libboost-filesystem-dev libevent-dev automake libtool flex bison pkg-config g++ libssl-dev# optional package to installsudo apt-get install php5-dev php5-cli phpunit libbit-vector-perl libclass-accessor-class-perl python-all python-all-dev python-all-dbg libglib2.0-dev ruby-full ruby-dev ruby-rspec rakesudo gem install daemons gem_plugin mongrelwget http://apache.stu.edu.tw//ant/binaries/apache-ant-1.9.4-bin.tar.gztar -zxvf apache-ant-1.9.4-bin.tar.gzcd apache-ant-1.9.4-binsudo mv apache-ant-1.9.4-bin /usr/local/antcd /usr/localsudo chown -R celest ant# install mongrel may occur error, to see http://stackoverflow.com/questions/13851741/install-mongrel-in-ruby-1-9-3# install thrift (for rhbase)cd ~/Downloadswget http://apache.stu.edu.tw/thrift/0.9.2/thrift-0.9.2.tar.gztar -zxvf thrift-0.9.2.tar.gzcd thrift-0.9.2./configuremakesudo make installsudo cp /usr/local/lib/libthrift-0.9.2.so /usr/lib/sudo /sbin/ldconfig /usr/lib/libthrift-0.9.2.so 1234567891011121314151617181920212223sudo subl /etc/bash.bashrc# add following 7 lines into file# # for rhbase# export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/# # for rhdfs and rmr2# export HADOOP_CMD=/usr/local/hadoop/bin/hadoop# # for rmr2# export HADOOP_STREAMING=/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jarsubl /usr/local/hadoop/etc/hadoop/hadoop-env.sh# # for compiled R by mkl# source /opt/intel/composer_xe_2013_sp1.3.174/mkl/bin/mklvars.sh intel64# source /opt/intel/composer_xe_2015.1.133/mkl/bin/mklvars.sh intel64# # for compiled R by icc# source /opt/intel/composer_xe_2013_sp1.3.174/bin/compilervars.sh intel64# source /opt/intel/composer_xe_2015.1.133/bin/compilervars.sh intel64# # for hive# export R_HOME=/usr/lib/Rsource /etc/bash.bashrc# in ubuntu, is &gt;&gt; etc/bash.bashrcstart-dfs.shstart-yarn.shhive --service hiveserver Install the dependent R packages: 123456789install.packages(c("rJava", "Rcpp", "rjson", "RJSONIO", "bit64", "reshape2", "data.table", "plyr", "dplyr", "digest", "functional", "stringr", "caTools", "lazyeval", "Hmisc", "testthat", "devtools", "iterators", "itertools", "pryr"))library(devtools)install_github("RevolutionAnalytics/quickcheck@3.2.0", subdir = "pkg")install_github("RevolutionAnalytics/memoise")install_github("RevolutionAnalytics/rhdfs", subdir = "pkg")install_github("RevolutionAnalytics/ravro", subdir = "pkg/ravro")install_github("RevolutionAnalytics/rmr2", subdir = "pkg")install_github("RevolutionAnalytics/plyrmr", subdir = "pkg")install_github("RevolutionAnalytics/rhbase", subdir = "pkg") When I install rhbase, I encounter a problem. The command pkg-config --cflags thrift does not return -I/usr/local/include instead the correct path -I/usr/local/include/thrift. So I copy all files in /usr/local/include/thrift into -I/usr/local/include by cp -R /usr/local/include/thrift/* /usr/local/include/. When I use the function hdfs.init() in package rhdfs, it come to a error massage: 12345sh: /usr/lib/hadoop/bin/: is a directoryError in .jnew("org/apache/hadoop/conf/Configuration") : java.lang.ClassNotFoundExceptionIn addition: Warning message:running command '/usr/lib/hadoop/bin/ classpath' had status 126 The reason why cause this problem is wrong setting of HADOOP_CMD, fix it and get work. install rhbase, I encounter a problem. The command pkg-config --cflags thrift does not return -I/usr/local/include instead the correct path -I/usr/local/include/thrift. So I copy all files in /usr/local/include/thrift into -I/usr/local/include by cp -R /usr/local/include/thrift/* /usr/local/include/. 1234567cd ~/Downloadsgit clone https://github.com/RevolutionAnalytics/rmr2.gitmv rmr2/pkg pkgrm -r rmr2mv pkg rmr2subl rmr2/R/streaming.RR CMD INSTALL rmr2 The lines shown in following: 12345paste.options( files = paste( collapse = ",", c(image.files, map.file, reduce.file, combine.file))) can be modified to following: 123456789101112131415161718192021222324## for intel parallel studio 2013paste.options( files = paste( collapse = ",", c(image.files, map.file, reduce.file, combine.file, "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libiomp5.so", "/usr/lib/jvm/java-8-oracle/jre/lib/amd64/server/libjvm.so"))),## for intel parallel studio 2015paste.options( files = paste( collapse = ",", c(image.files, map.file, reduce.file, combine.file, "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libiomp5.so", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libifport.so.5", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libifcoremt.so.5", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libimf.so", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libsvml.so", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libirc.so", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libirng.so", "/opt/intel/composer_xe_2015.1.133/compiler/lib/intel64/libintlc.so.5", "/usr/lib/jvm/java-8-oracle/jre/lib/amd64/server/libjvm.so"))), Test rhadoop series packages: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# test rhdfslibrary(rhdfs)library(data.table)library(dplyr)library(magrittr)N = 300mydata = replicate(3, rnorm(N)) %&gt;% tbl_dt() %&gt;% setnames(paste0("x", 1:3)) %&gt;% mutate(y = x1+2*x2+3*x3+rnorm(N,0,5))model = lm(y ~., mydata)hdfs.init()modelfilename = "mymodel"modelfile = hdfs.file(modelfilename, "w")hdfs.write(model, modelfile)hdfs.close(modelfile)model# Call:# lm(formula = y ~ ., data = mydata)## Coefficients:# (Intercept) x1 x2 x3# 0.1105 1.9879 2.6298 3.0040modelfile = hdfs.file(modelfilename, "r")m = hdfs.read(modelfile)model2 = unserialize(m)hdfs.close(modelfile)model2# Call:# lm(formula = y ~ ., data = mydata)## Coefficients:# (Intercept) x1 x2 x3# 0.1105 1.9879 2.6298 3.0040N = 3000mydata = replicate(3, rnorm(N)) %&gt;% tbl_dt() %&gt;% setnames(paste0("x", 1:3)) %&gt;% mutate(y = x1+2*x2+3*x3+rnorm(N,0,5))model = lm(y ~., mydata)modelfilename = "my_smart_unique_name"modelfile = hdfs.file(modelfilename, "w")hdfs.write(model, modelfile)hdfs.close(modelfile)model# Call:# lm(formula = y ~ ., data = mydata)## Coefficients:# (Intercept) x1 x2 x3# 0.2506 1.2038 1.9056 3.0477modelfile = hdfs.file(modelfilename, "r")m = hdfs.read(modelfile)itertoolsmodel2 = unserialize(m)# error in reading datahdfs.close(modelfile)length(serialize(model,NULL))# [1] 131535length(m)# [1] 65536# it return the wrong size!modelfile = hdfs.file(modelfilename, "r")fileSize = hdfs.file.info(paste0("/user/celest/", modelfilename))$sizem = NULLi = 1repeat&#123; break_out = 65536*i &gt; fileSize size = ifelse(break_out, 65536, fileSize - 65536*(i-1)) tmp = hdfs.read(modelfile, size, 65536*(i-1)) m = c(m, tmp) i = i + 1 if (break_out) break&#125;model2 = unserialize(m)hdfs.close(modelfile)length(m)# [1] 131535model2# to delete this data by using following command# hdfs.rm("/user/celest/my_smart_unique_name")# Call:# lm(formula = y ~ ., data = mydata)## Coefficients:# (Intercept) x1 x2 x3# 0.2506 1.2038 1.9056 3.0477 Back to terminal, you can use hdfs to find what R write. (There is some functions in R doing the same way.) 12345hdfs dfs -ls # hdfs.ls("/user/celest") in R## Found 1 items## -rw-r--r-- 3 celest supergroup 131535 2015-04-06 13:50 my_smart_unique_name# remove the filehdfs dfs -rm -r my_smart_unique_name # hdfs.rm("/user/celest/my_smart_unique_name") in R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Sys.setenv("HADOOP_CMD"="/usr/local/hadoop/bin/hadoop")# test the rmr2 plyrmrlibrary(data.table)library(dplyr)library(magrittr)library(rmr2)library(plyrmr)library(rhdfs)# a simple caseints = to.dfs(1:100)calc = mapreduce(input = ints, map = function(k, v) cbind(v, v=2*v))output = from.dfs(calc)outputints2 = to.dfs(matrix(rnorm(25), 5))calc2 = mapreduce(input = ints2, map = function(k, v) v %*% v)output2 = from.dfs(calc2)output2# $key# NULL## $val# v# [1,] 1 2# [2,] 2 4# [3,] 3 6# [4,] 4 8# [5,] 5 10# [6,] 6 12# [7,] 7 14# [8,] 8 16# ............N = 15dat = replicate(3, rnorm(N)) %&gt;% tbl_dt() %&gt;% setnames(paste0("x", 1:3)) %&gt;% mutate(y = x1+2*x2+3*x3+rnorm(N,0,5)) %&gt;% as.data.frame()hdfs.init()mydata = to.dfs(dat)as.data.frame(input(mydata), x1_x2 = x1*x2)bind.cols(input(mydata), x1_x2 = x1*x2)output(bind.cols(input(mydata),x1_x2 = x1*x2), "/tmp/mydata2")# test the rhbasehbase thrift startRR &gt; library(rhbase)R &gt; hb.list.tables() # list()R &gt; q("no")]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Mint</tag>
        <tag>R</tag>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>Hive</tag>
        <tag>rhdfs</tag>
        <tag>rmr2</tag>
        <tag>plyrmr</tag>
        <tag>rhbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build Hadoop environment in mint 17]]></title>
    <url>%2Fposts%2F201504%2F2015-04-06-build-hadoop-environment-in-mint-17.html</url>
    <content type="text"><![CDATA[Hadoop is one of the most popular tool to deal with the big data. I construct the environment of Hadoop in mint 17. Mint 17 is based on the ubuntu 14.04. The following steps also works in ubuntu 14.04. install java jdk 8 12345678910sudo apt-get install python-software-propertiessudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installerjavac -version # check the version of java# remove openjdk javasudo apt-get purge openjdk-\*# java version vs hadoop version# refer: http://wiki.apache.org/hadoop/HadoopJavaVersions install ssh 1234567sudo apt-get install ssh rsync openssh-serverssh-keygen -t rsa -P "" # generate SSH key# Enable SSH Keycat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# test whether it works, it should not need password if it worksssh localhostexit download hadoop 12345wget http://apache.stu.edu.tw/hadoop/common/stable2/hadoop-2.7.2.tar.gztar zxvf hadoop-2.7.2.tar.gzsudo mv hadoop-2.7.2 /usr/local/hadoopcd /usr/localsudo chown -R celest hadoop setting environment for java and hadoop 12345678910111213sudo subl /etc/bash.bashrc# add following 9 lines into file# export JAVA_HOME=/usr/lib/jvm/java-8-oracle/# export HADOOP_PID_DIR=/usr/local/hadoop/pids/# export HADOOP_INSTALL=/usr/local/hadoop# export PATH=$PATH:$HADOOP_INSTALL/bin# export PATH=$PATH:$HADOOP_INSTALL/sbin# export HADOOP_MAPRED_HOME=$HADOOP_INSTALL# export HADOOP_COMMON_HOME=$HADOOP_INSTALL# export HADOOP_HDFS_HOME=$HADOOP_INSTALL# export YARN_HOME=$HADOOP_INSTALLsource /etc/bash.bashrc# in ubuntu, is &gt;&gt; etc/bash.bashrc 4-1. network environment (for multi-node hadoop)If you use the VMware, you need to add another host-only network card. You can install hadoop successfully and clone it to be slaves. In following, master stands for the primary node and slaveXX for the other nodes. a. setting the network using ifconfig to check whether the network card is adding. 1sudo subl /etc/network/interfaces the content of file looks like this:(there must be eth1. Put a address for it on the machines master and slaves.) 12345678910111213# The loopback network interface for masterauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet dhcp# Host-Onlyauto eth1 iface eth1:0 inet static address 192.168.29.130 # (192.168.29.131 for slave01) netmask 255.255.0.0 restart network by command sudo /etc/init.d/networking restartand check the network again by ifconfig. setup for hadoop a. disabling IPv6 by editing /etc/sysctl.conf 1subl /etc/sysctl.conf paste the following into /etc/sysctl.conf 123net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1 checking whether ipv6 is disable: cat /proc/sys/net/ipv6/conf/all/disable_ipv6. (retuen 1) b. editing following files: create the folders for putting data. 123456789cd /usr/local/hadoopsudo mkdir -p /usr/local/hadoop/tmpsudo chown celest /usr/local/hadoop/tmpsubl /usr/local/hadoop/etc/hadoop/hadoop-env.shsubl /usr/local/hadoop/etc/hadoop/core-site.xmlcp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xmlsubl /usr/local/hadoop/etc/hadoop/mapred-site.xmlsubl /usr/local/hadoop/etc/hadoop/hdfs-site.xmlsubl /usr/local/hadoop/etc/hadoop/yarn-site.xml i. /usr/local/hadoop/etc/hadoop/hadoop-env.sh ii. /usr/local/hadoop/etc/hadoop/core-site.xml iii. /usr/local/hadoop/etc/hadoop/mapred-site.xml iv. /usr/local/hadoop/etc/hadoop/hdfs-site.xml v. /usr/local/hadoop/etc/hadoop/slaves # only need for multi-node hadoop a. hadoop-env.shreplace the line JAVA_HOME={JAVA_HOME} with your java root, in my case, it isexport JAVA_HOME=/usr/lib/jvm/java-8-oracle. b. core-site.xmlput the following content to the file. 123456789101112131415161718192021&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt; A base for other temporary directories. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- &lt;value&gt;hdfs://master:9000&lt;/value&gt; for the multi-node case --&gt; &lt;description&gt; The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem. &lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; c. mapred-site.xmlThere is no file mapred-site.xml, we get by copy the mapred-site.xml.template. This command would be helpful: cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml &amp;&amp; subl etc/hadoop/mapred-site.xml. We set the specification of job tracker like this: 123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; d. hdfs-site.xml 1234567891011121314151617&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;description&gt; Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/name&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; e. yarn-site.xml 123456789101112131415&lt;configuration&gt;&lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;/property&gt; --&gt;&lt;!-- for multi-node --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; f. slaves (for multi-node.) put names of your machines in the hadoop/etc/hadoop/slaves. Command: subl etc/hadoop/slaves. The file looks like: 12masterslave01 Starting hadoop 12345678# import environment variable# ubuntu: source ~/.bashrcsource /etc/bash.bashrc# format hadoop spacehdfs namenode -format# start the hadoopstart-dfs.sh &amp;&amp; start-yarn.sh# or start-all.sh The output looks like this: (standalone) 1234567localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-master-namenode-master-virtual-machine.outlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-master-datanode-master-virtual-machine.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-master-secondarynamenode-master-virtual-machine.outstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-master-resourcemanager-master-virtual-machine.outlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-master-nodemanager-master-virtual-machine.out The output looks like this: (standalone) 1234567master: starting namenode, logging to /usr/local/hadoop/logs/hadoop-master-namenode-master.outslave01: starting datanode, logging to /usr/local/hadoop/logs/hadoop-master-datanode-slave01.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-master-secondarynamenode-master.outstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-master-resourcemanager-master.outslave01: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-master-nodemanager-slave01.out check whether the server starts by connecting the local server. 12345# for standalonefirefox http:\\localhost:50070firefox http:\\localhost:50090# for multi-node hadoophdfs dfsadmin -report Run a example in the folder 1hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar pi 10 100 The last two line will show the following informations: 12Job Finished in 167.153 secondsEstimated value of Pi is 3.14800000000000000000 Run second example run wordaccout 123456789101112131415161718cd ~/Downloads &amp;&amp; mkdir testData &amp;&amp; cd testData# download data for testwget http://www.gutenberg.org/ebooks/5000.txt.utf-8cd ..# upload to the hadoop serverhdfs dfs -copyFromLocal testData/ /user/celest/# check that the file is in the hadoop serverhdfs dfs -ls /user/celest/testData/# run wordcount on hadoophadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/celest/testData /user/celest/testData-output# check the whether it successeshdfs dfs -ls /user/celest/testData-output/# view the resulthdfs dfs -cat /user/celest/testData-output/part-r-00000# clean the test file (optional)hdfs dfs -rm -r /user/celest/testDatahdfs dfs -rm -r /user/celest/testData-output Stopping hadoopstop-all.sh or stop-dfs.sh &amp;&amp; stop-yarn.sh compiling hadoop library by yourself (optional) To avoid the warning WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable, you can build the 64 bit hadoop lib by yourself. Here is the bash script to compile: 123456789101112131415161718192021222324252627282930313233343536373839# install the necessary packagessudo apt-get -y install build-essential protobuf-compiler autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev git subversioncd ~/Downloadswget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gztar zxvf protobuf-2.5.0.tar.gzcd protobuf-2.5.0./configuremakesudo make install# get antcd ..wget http://apache.stu.edu.tw/ant/binaries/apache-ant-1.9.4-bin.tar.gztar zxvf apache-ant-1.9.4-bin.tar.gzsudo mv apache-ant-1.9.4 /usr/local/antsudo chown -R celest /usr/local/ant# get mavencd ..wget http://apache.stu.edu.tw/maven/maven-3/3.3.1/binaries/apache-maven-3.3.1-bin.tar.gztar zxvf apache-maven-3.3.1-bin.tar.gzsudo mv apache-maven-3.3.1 /usr/local/mavensudo chown -R celest /usr/local/mavensudo subl /etc/bash.bashrc# add following 5 lines into file# export MAVEN_HOME=/usr/local/maven# export PATH=$PATH:$MAVEN_HOME/bin# export ANT_HOME=/usr/local/ant# export PATH=$PATH:$ANT_HOME/bin# export MAVEN_OPTS="-Xms256m -Xmx512m"source /etc/bash.bashrc# get the source code of hadoopcd ..wget http://apache.stu.edu.tw/hadoop/common/stable2/hadoop-2.7.2-src.tar.gztar zxvf hadoop-2.7.2-src.tar.gzcd hadoop-2.7.2-srcmvn clean package -Pdist -Dtar -Dmaven.javadoc.skip=true -DskipTests -fail-at-end -Pnativesudo cp -r hadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2 /usr/local/hadoopsudo mv /usr/local/hadoop-2.7.2 /usr/local/hadoopsudo chown -R celest hadoop]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Mint</tag>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installation of HBase and Hive in mint 17]]></title>
    <url>%2Fposts%2F201504%2F2015-04-06-installation-of-hbase-and-hive-in-mint-17.html</url>
    <content type="text"><![CDATA[The second post for series of building hadoop environment in mint 17. HBase support the storage of big table in HDFS. Hive support Hadoop to query data with sql commands. Based on hadoop, we install the hbase. Get hbase 1234wget http://apache.stu.edu.tw/hbase/stable/hbase-1.0.0-bin.tar.gztar zxvf hbase-1.0.0-bin.tar.gzsudo mv hbase-1.0.0 /usr/local/hbasecd /usr/local/hbase Put the following into the file by command: subl conf/hbase-env.sh. 12345export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport HBASE_HOME=/usr/local/hbaseexport HADOOP_INSTALL=/usr/local/hadoopexport HBASE_CLASSPATH=/usr/local/hbase/confexport HBASE_MANAGES_ZK=true Set up the hbase by command: subl conf/hbase-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:54310/hbase&lt;/value&gt; &lt;!-- &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; for the multi-node case --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;!-- &lt;value&gt;master&lt;/value&gt; for the multi-node case --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Copy the setting of hdfs into hbase folder by command:cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/hbase/conf start hbase 123456sudo subl /etc/bash.bashrc# add following 2 lines into file# export HBASE_INSTALL=/usr/local/hbase# export PATH=$PATH:$HBASE_INSTALL/binsource /etc/bash.bashrc# in ubuntu, is &gt;&gt; etc/bash.bashrc 12start-dfs.sh &amp;&amp; start-yarn.sh # or start-all.shstart-hbase.sh test hbasetype hbase shell in the terminal, then it will show: 12345HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Type "exit&lt;RETURN&gt;" to leave the HBase ShellVersion 1.0.0, r6c98bff7b719efdb16f71606f3b7d8229445eb81, Sat Feb 14 19:49:22 PST 2015hbase(main):001:0&gt; type list and it return12TABLE0 row(s) in 4.4510 seconds The other tests remain in the rhbase. Stop the server: 12stop-hbase.shstop-dfs.sh &amp;&amp; stop-yarn.sh # or stop-all.sh hive installation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293wget http://apache.stu.edu.tw/hive/stable/apache-hive-1.1.0-bin.tar.gztar zxvf apache-hive-1.1.0-bin.tar.gzsudo mv apache-hive-1.1.0-bin /usr/local/hivesudo subl /etc/bash.bashrc# add following 7 lines to file# export HIVE_HOME="/usr/local/hive"# export PATH=$PATH:$HIVE_HOME/bin# export HADOOP_USER_CLASSPATH_FIRST=truesource /etc/bash.bashrc# install mysqlsudo apt-get install mysql-server# it need to set up a password to access mysql servermysql -u root -p # enter the password you just set# create usermysql &gt; create user 'celest'@'%' identified by 'celest';mysql &gt; grant all privileges on *.* to 'celest'@'%' with grant option;mysql &gt; flush privileges;mysql &gt; exitcd /usr/local/hivecp conf/hive-env.sh.template conf/hive-env.shsubl conf/hive-env.sh# add this line to file# export HADOOP_HOME=/usr/local/hadoopcp conf/hive-default.xml.template conf/hive-site.xmlsubl hive-site.xml# modify file like below# &lt;property&gt;# &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;# &lt;value&gt;/user/celest/warehouse&lt;/value&gt;# &lt;description&gt;location of default database for the warehouse &lt;/ description&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;# &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;# &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;# &lt;value&gt;celest&lt;/value&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;# &lt;value&gt;celest&lt;/value&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;hive.querylog.location&lt;/name&gt;# &lt;value&gt;/usr/local/hive/log&lt;/value&gt;# &lt;description&gt;# Location of Hive run time structured log file# &lt;/description&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;# &lt;value&gt;/usr/local/hive/log/operation_logs&lt;/#alue&gt;# &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;# &lt;value&gt;/usr/local/hive/tmpdir/tmp&lt;/value&gt;# &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;# &lt;/property&gt;# &lt;property&gt;# &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;# &lt;value&gt;/usr/local/hive/tmpdir/tmp_resources&lt;/value&gt;# &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;# &lt;/property&gt;cd ~/Downloadswget http://ftp.kaist.ac.kr/mysql/Downloads/Connector-J/mysql-connector-java-5.1.35.tar.gztar zxvf mysql-connector-java-5.1.35.tar.gzcp -R mysql-connector-java-5.1.35//home/celest/Downloads/mysql-connector-java-5.1.35/mysql-connector-java-5.1.35-bin.jar /usr/local/hive/lib/## test hivemkdir /usr/local/hive/tmpdirmkdir /usr/local/hive/tmpdir/logsmkdir /usr/local/hive/tmpdir/operation_logsmkdir /usr/local/hive/tmpdir/tmpmkdir /usr/local/hive/tmpdir/tmp_resourceshdfs dfs -mkdir /user/celest/warehousehivehive &gt; show tables;hive &gt; exit;mysql -u celest -pmysql &gt; show databases;mysql &gt; use hivemysql &gt; show tables;mysql &gt; exit# There shows that the hive had connected to mysql server.]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Mint</tag>
        <tag>BigData</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Combinations of protein in Rcpp]]></title>
    <url>%2Fposts%2F201503%2F2015-03-30-combinations-of-protein-in-Rcpp.html</url>
    <content type="text"><![CDATA[There is a sequence of protein like A B1/B2 C1/C2 K/D E F1/F2, K does not connect to next point, so it is cut at K. Therefore, the combinations of protein is in the following: 12345678910111213A B1 C1 KA B2 C1 KA B1 C2 KA B2 C2 KA B1 C1 D E F1A B2 C1 D E F1A B1 C2 D E F1A B2 C2 D E F1A B1 C1 D E F2A B2 C1 D E F2A B1 C2 D E F2A B2 C2 D E F2 The code to generate sequence of protein and list the all combinations is below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#define ARMA_64BIT_WORD#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]List combns_f2(IntegerVector comnbs_for_each_loc, IntegerVector exist_k, List names_eliminate_k)&#123; Col&lt;int&gt; X(comnbs_for_each_loc.begin(), comnbs_for_each_loc.size()); Col&lt;int&gt; Y(exist_k.begin(), exist_k.size(), false); uvec loc_Y = find(Y &gt;= 1); X(loc_Y) -= 1; loc_Y.insert_rows(loc_Y.n_elem, X.n_elem*ones&lt;uvec&gt;(1)-1); List result_list(loc_Y.n_elem); CharacterVector char_k = CharacterVector::create("K"); for (int i = 0; i &lt; loc_Y.n_elem; i++) &#123; uword size_mat = 1; for (int k = 0; k &lt;= loc_Y(i); k++) size_mat *= (uword) X(k); CharacterMatrix tmp_result(size_mat, loc_Y(i)+1); for (int j = 0; j &lt; (int) loc_Y(i)+1; j++) &#123; if ( i &lt; loc_Y.n_elem-1 &amp;&amp; j == (int) loc_Y(i)) tmp_result(_, j) = rep(char_k, size_mat); else tmp_result(_, j) = rep(as&lt;CharacterVector&gt;(names_eliminate_k[j]), size_mat / X(j)); &#125; result_list[i] = tmp_result; &#125; return result_list;&#125;// [[Rcpp::export]]List combns_f(IntegerVector comnbs_for_each_loc, IntegerVector exist_k)&#123; Col&lt;int&gt; X(comnbs_for_each_loc.begin(), comnbs_for_each_loc.size()); Col&lt;int&gt; Y(exist_k.begin(), exist_k.size(), false); uvec loc_Y = find(Y &gt;= 1); X(loc_Y) -= 1; loc_Y.insert_rows(loc_Y.n_elem, X.n_elem*ones&lt;uvec&gt;(1)-1); List result_list(loc_Y.n_elem); for (int i = 0; i &lt; loc_Y.n_elem; i++) &#123; uword size_mat = 1; for (int k = 0; k &lt;= loc_Y(i); k++) size_mat *= (uword) X(k); Mat&lt;int&gt; tmp_result(size_mat, loc_Y(i)+1); for (int j = 0; j &lt; (int) loc_Y(i)+1; j++) &#123; if ( i &lt; loc_Y.n_elem-1 &amp;&amp; j == (int) loc_Y(i)) tmp_result.col(j) = repmat( Y(loc_Y(i)) * ones&lt;Col&lt;int&gt; &gt;(1), size_mat, 1); else tmp_result.col(j) = repmat( linspace&lt;Col&lt;int&gt; &gt;(1, X(j), X(j)), size_mat / X(j), 1); &#125; result_list[i] = wrap(tmp_result); &#125; return result_list;&#125;')# x_test = list("A", c("B1", "B2"), c("C1", "C2"), c("K", "D"), "E",# c("F1", "F2"))set.seed(77)x = lapply(setdiff(LETTERS[1:14], "K"), function(a) paste0(a, 1:sample(1:5, 1)))x = lapply(x, function(y) if(runif(1) &lt; 0.4)&#123;c(y, "K")&#125; else&#123;y&#125;)t1 = proc.time()size_x = sapply(x, length)exist_k = as.integer(sapply(x, function(x) which(x=="K")))exist_k[which(is.na(exist_k))] = 0result = combns_f(size_x, exist_k)result_transform = vector('list', length(result))tmp_x = xfor (j in 1:length(result))&#123; result_transform[[j]] = sapply(1:ncol(result[[j]]), function(i) tmp_x[[i]][result[[j]][,i]]) if (j &lt; length(result)) tmp_x[[which(exist_k&gt;=1)[j]]] = setdiff(tmp_x[[which(exist_k&gt;=1)[j]]], "K")&#125;proc.time() - t1# user system elapsed# 9.31 1.74 11.90object.size(result) # 704257576 bytesobject.size(result_transform) # 1408520016 bytest2 = proc.time()size_x = sapply(x, length)exist_k = as.integer(sapply(x, function(x) which(x=="K")))exist_k[which(is.na(exist_k))] = 0result2 = combns_f2(size_x, exist_k, lapply(x, setdiff, y = "K"))proc.time() - t2# user system elapsed# 1.86 0.15 2.03object.size(result2) # 1408520016 bytesall.equal(result_transform, result2)# TRUE This code alert me that I should go to the destination directly, not windingly. It saves a lot of time.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The length of runs of equal values in a vector]]></title>
    <url>%2Fposts%2F201503%2F2015-03-28-the-length-of-runs-of-equal-values-in-a-vector.html</url>
    <content type="text"><![CDATA[We usually encounter the problem for counting the length of equal values repeatedly. rle is the build-in command in R to solve this problem which is consist of diff and which. But it is not so fast, I write another version of rle in Rcpp. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;template &lt;int RTYPE&gt;List rle_template(const Vector&lt;RTYPE&gt;&amp; x) &#123; IntegerVector tmp = seq_len(x.size()-1); LogicalVector loc = head(x, x.size()-1) != tail(x, x.size()-1); IntegerVector y = tmp[loc | is_na(loc)]; y.push_back(x.size()); Col&lt;int&gt; y2(y.begin(), y.size()); y2.insert_rows(0, zeros&lt; Col&lt;int&gt; &gt;(1)); IntegerVector y3 = wrap(y2); return List::create(Named("lengths") = diff(y3), Named("values") = x[y-1]);&#125;// [[Rcpp::export]]List rle_cpp(SEXP x)&#123; switch( TYPEOF(x) ) &#123; case INTSXP: return rle_template&lt;INTSXP&gt;(x); case REALSXP: return rle_template&lt;REALSXP&gt;(x); case STRSXP: return rle_template&lt;STRSXP&gt;(x); &#125; return R_NilValue;&#125;')x &lt;- rev(rep(6:10, 1:5))all.equal(rle(x), rle_cpp(x), check.attributes = FALSE)# TRUEN = 100000testVector = rep(sample(1:150, round(N/10), TRUE), sample(1:25, round(N/10), TRUE))all.equal(rle(testVector), rle_cpp(testVector), check.attributes = FALSE)# TRUElibrary(rbenchmark)benchmark(rle(testVector), rle_cpp(testVector), columns = c("test", "replications","elapsed", "relative"), replications=100, order="relative")# test replications elapsed relative# 2 rle_cpp(x) 100 0.34 1.000# 1 rle(x) 100 2.53 7.441]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>Running Length</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Drawing planes in R]]></title>
    <url>%2Fposts%2F201503%2F2015-03-27-drawing-planes-in-R.html</url>
    <content type="text"><![CDATA[A simple log for drawing 2 planes in 3D plot. 12345678910111213141516171819202122library(dplyr)library(data.table)library(magrittr)library(tidyr)library(lattice)xygrid = expand.grid(x = -30:30, y = -30:30)dat = xygrid %&gt;% mutate(z1 = x+3*y, z2 = 0)dat_piled = dat %&gt;% gather(group, z_value, z1:z2) %&gt;% tbl_dt(FALSE)cols = c("cyan", "yellow")panel.custom = function(x, y, z, xlim, ylim, zlim, xlim.scaled, ylim.scaled, zlim.scaled, ...) &#123; panel.3dwire(x = x, y = y, z = z, xlim = xlim, ylim = ylim, zlim = zlim,xlim.scaled = xlim.scaled,ylim.scaled = ylim.scaled, zlim.scaled = zlim.scaled, col = "transparent", ...) &#125;wireframe(z_value ~ x + y, dat_piled, groups=group, par.settings = simpleTheme(col = cols), auto.key = list(columns = 2, text = paste("z", 1:2), rectangles = TRUE, points = FALSE), panel.3d.wireframe = panel.custom )]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>3D plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Regular expression in R]]></title>
    <url>%2Fposts%2F201503%2F2015-03-26-regular-expression-in-R.html</url>
    <content type="text"><![CDATA[Several examples of regular expression in R. Regular expression is a powerful tool to process strings. First example, we have strings &#39;q123er&#39;, &#39;a2334bc&#39;, &#39;b78gg&#39; and we want to let it be &#39;qer123qer&#39;, &#39;abc2334abc&#39;, &#39;bgg78bgg&#39;. 1234a = c('q123er', 'a2334bc', 'b78gg')a_out = gsub('([a-z])(\\d*)([a-z]&#123;2&#125;)', '\\1\\3\\2\\1\\3', a)all.equal(c('qer123qer', 'abc2334abc', 'bgg78bgg'), a_out)# TRUE Second example is that we want add some string in front of the some characters. We want to search parenthesis in string, but parenthesis need escape or it cannot be found. 12345b = c('p(478)cer', 'ba(2334)bdc', 'ber(728)gigi')b2 = gsub("(\\(|\\))", "\\\\\\1", b)b_out = gsub("(\\(|\\))", "", b)b_out# [1] "p478cer" "ba2334bdc" "ber728gigi" Third example is that we take the information from strings. 12345d = c('p478cer', 'ba2334bdc', 'ber728gigi')d_pattern = regexec('[a-z]*(\\d*)[a-z]*', d)d_out = sapply(regmatches(d, d_pattern), function(x) x[2])d_out# [1] "478" "2334" "728"]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Processing string in R (Rcpp and rJava)]]></title>
    <url>%2Fposts%2F201503%2F2015-03-20-processing-string-in-R.html</url>
    <content type="text"><![CDATA[There is a example for processing string in R. Before we use rJava, we need a class file first. The regex_java.java is shown in below and compilation can be done with command javac regex_java.java. 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.regex.Matcher;import java.util.regex.Pattern;public class regex_java &#123; public static String[] string_split_1d(String[] string_array, int[] loc) &#123; String[] out = new String[string_array.length*(loc.length - 1)]; for (int j = 0; j &lt; loc.length - 1; j++) for (int i = 0; i &lt; string_array.length; i++) out[i+j*string_array.length] = string_array[i].subSequence(loc[j], loc[j+1]).toString(); return out; &#125; public static String[][] string_split(String[] string_array, int[] loc) &#123; String[][] out = new String[string_array.length][loc.length - 1]; for (int i = 0; i &lt; string_array.length; i++) for (int j = 0; j &lt; loc.length - 1; j++) out[i][j] = string_array[i].subSequence(loc[j], loc[j+1]).toString(); return out; &#125; public static String[] regex_java(String[] string_array, String patternStr, int pattern_reconize) &#123; String[] out = new String[string_array.length*pattern_reconize]; Pattern pattern = Pattern.compile(patternStr); for (int j = 0; j &lt; string_array.length; j++) &#123; Matcher matcher = pattern.matcher(string_array[j]); boolean matchFound = matcher.find(); while(matchFound) &#123; for(int i = 1; i &lt;= matcher.groupCount(); i++) out[j*pattern_reconize+i-1] = matcher.group(i); if(matcher.end() + 1 &lt;= string_array[j].length()) matchFound = matcher.find(matcher.end()); else break; &#125; &#125; return out; &#125; public static void main(String[] args) &#123; &#125;&#125; Next, we demonstrate the performance between R, Rcpp and rJava. However, Rcpp cannot process string in regular expression because the standard of C++ does not support regular expression before C++14. Note that g++ have regexp.h to support regular expression in linux. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384library(data.table)library(plyr)library(dplyr)library(magrittr)dat = fread(paste0(rep("001female2019920404\n002male 3019920505\n003male 4019920606\n004female5019920707\n", 100000), collapse=""), sep="\n", sep2="",header=FALSE)## using Rtt = proc.time()dat_regex = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% regexec("([0-9]&#123;3&#125;)(female|male\\s&#123;2&#125;)([0-9]&#123;2&#125;)([0-9]&#123;8&#125;)", text = .)dat_split2 = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% regmatches(dat_regex) %&gt;% do.call(rbind, .) %&gt;% data.table() %&gt;% select(2:ncol(.)) %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt# user system elapsed# 10.19 0.14 10.96## using Rcpplibrary(Rcpp)library(inline)sourceCpp(code = '#include &lt;Rcpp.h&gt;using namespace Rcpp;// [[Rcpp::export]]CharacterMatrix dat_split_f( std::vector&lt; std::string &gt; strings,NumericVector loc) &#123; int loc_len = loc.size(), num_strings = strings.size(); CharacterMatrix output(num_strings, loc_len); for( int j=0; j &lt; num_strings; j++ ) &#123; for (int i=0; i &lt; loc_len-1; i++) output(j, i) = strings[j].substr(loc[i], loc[i+1] - loc[i]); &#125; return output;&#125;')tt = proc.time()dat_split = dat_split_f(dat[[1]], c(0, 3, 9, 11, 19)) %&gt;% data.table() %&gt;% select(1:4) %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt# user system elapsed# 1.12 0.08 1.20## using rJavalibrary(rJava).jinit().jaddClassPath( "D:\\Program\\R\\some_code\\regex_java")regex_java = .jnew("regex_java")tt = proc.time()output = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% .jcall(regex_java, "[S", "string_split_1d", ., as.integer(c(0, 3, 9, 11, 19))) %&gt;% matrix(ncol=4) %&gt;% data.table() %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt# user system elapsed# 3.24 0.04 2.55tt = proc.time()output2 = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% .jcall(regex_java, "[[Ljava/lang/String;", "string_split", ., as.integer(c(0, 3, 9, 11, 19)), simplify = TRUE) %&gt;% data.table() %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt# user system elapsed# 3.46 0.05 2.92tt = proc.time()pattern = "(\\d&#123;3&#125;)(female|male\\s&#123;2&#125;)(\\d&#123;2&#125;)(\\d&#123;8&#125;)"size_recognize = "(\\((?&gt;[^()]+|(?R))*\\))" %&gt;% gregexpr(pattern, perl = TRUE) %&gt;% extract2(1) %&gt;% length() %&gt;% as.integer()output3 = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% .jcall(regex_java, "[S", "regex_java", ., pattern, size_recognize) %&gt;% matrix(ncol = 4, byrow=TRUE) %&gt;% data.table() %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt# user system elapsed# 4.66 0.13 3.38all.equal(output, output2) # TRUEall.equal(output, dat_split) # TRUEall.equal(output, dat_split2) # TRUEall.equal(output, output3) # TRUE]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>split</tag>
        <tag>regex</tag>
        <tag>rJava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Splitting characters in Rcpp]]></title>
    <url>%2Fposts%2F201503%2F2015-03-18-splitting-characters-in-R.html</url>
    <content type="text"><![CDATA[We usually need to process the raw data by ourself, the character type of data is the most common type of raw data. I demonstrate a example to simply split the characters. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455library(data.table)library(magrittr)library(Rcpp)library(inline)sourceCpp(code = '#include &lt;Rcpp.h&gt;using namespace Rcpp;// [[Rcpp::export]]CharacterMatrix dat_split_f( std::vector&lt; std::string &gt; strings,NumericVector loc) &#123; int loc_len = loc.size(), num_strings = strings.size(); CharacterMatrix output(num_strings, loc_len); for( int j=0; j &lt; num_strings; j++ ) &#123; for (int i=0; i &lt; loc_len-1; i++) output(j, i) = strings[j].substr(loc[i], loc[i+1] - loc[i]); &#125; return output;&#125;')# size is 400000dat = fread(paste0(rep("001female2019920404\n002male 3019920505\n003male 4019920606\n004female5019920707\n", 100000), collapse=""), sep="\n", sep2="", header=FALSE)tt = proc.time()dat_split = dat_split_f(dat[[1]], c(0, 3, 9, 11, 19)) %&gt;% data.table()dat_split[, ':='(V1 = as.numeric(V1), V3 = as.numeric(V3))]proc.time() - tt# user system elapsed# 0.97 0.06 1.03# size is 4000000dat = fread(paste0(rep("001female2019920404\n002male 3019920505\n003male 4019920606\n004female5019920707\n", 1000000), collapse=""), sep="\n", sep2="", header=FALSE)tt = proc.time()dat_split = dat_split_f(dat[[1]], c(0, 3, 9, 11, 19)) %&gt;% data.table()dat_split[, ':='(V1 = as.numeric(V1), V3 = as.numeric(V3))]proc.time() - tt# user system elapsed# 7.73 0.21 7.98## by using regular expressiondat = fread(paste0(rep("001female2019920404\n002male 3019920505\n003male4019920606\n004female5019920707\n", 100000), collapse=""),sep="\n",sep2="",header=FALSE)tt = proc.time()dat_regex = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% regexec("([0-9]&#123;3&#125;)(female|male\\s&#123;2&#125;)([0-9]&#123;2&#125;)([0-9]&#123;8&#125;)", text = .)dat_split = dat %&gt;% select(V1) %&gt;% extract2(1) %&gt;% regmatches(dat_regex) %&gt;% do.call(rbind, .) %&gt;% data.table() %&gt;% select(2:ncol(.)) %&gt;% setnames(c("id", "gender", "age", "birthday"))proc.time() - tt]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>split</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computing the transition matrix for multi-state individual]]></title>
    <url>%2Fposts%2F201503%2F2015-03-14-Computing-the-transition-matrix-for-multi-state-id.html</url>
    <content type="text"><![CDATA[We have a repeated-measuring data. We want to take average every 3 periods. Here is code to do it. 1234567891011121314151617181920212223242526272829303132333435363738## A simple way to compute transition matrix if every individual does not have multiple state.library(data.table)library(dplyr)library(magrittr)# data generationN_patient = 1000dat = sample(48, N_patient, replace = TRUE) %&gt;% &#123; cbind(rep(1:N_patient, times=.), sapply(., seq, from = 1) %&gt;% unlist()) &#125; %&gt;% cbind(sample(6, nrow(.), replace = TRUE)) %&gt;% data.table() %&gt;% setnames(c("id", "obs_time", "dose"))dat_transform = dat$dose %&gt;% spMatrix(length(.), 6, 1:length(.), ., rep(1,length(.))) %&gt;% as.matrix() %&gt;% data.table() %&gt;% setnames(paste0("dose_", 1:6)) %&gt;% cbind(select(dat, 1:2), .)## inverse transformation of dat_transform# dat = dat_transform %&gt;%# mutate(dose = dose_1 + dose_2 * 2 + dose_3 * 3 + dose_4 * 4 +# dose_5 * 5 + dose_6 * 6) %&gt;% select(c(1,2, 9))st = proc.time()dat_count = dat %&gt;% group_by(id) %&gt;% mutate(previous_dose = c(0, dose[-length(dose)])) %&gt;% filter(obs_time &gt; 1) %&gt;% group_by(obs_time, dose, previous_dose) %&gt;% summarise(count = n())transitMatrix = dat_count %&gt;% group_by(obs_time, previous_dose) %&gt;% mutate(transitP = count / sum(count)) %&gt;% ungroup() %&gt;% split(.$obs_time) %&gt;% lapply(function(x) spMatrix(6, 6, x$previous_dose, x$dose, x$transitP))proc.time() - st# user system elapsed# 0.72 0.05 0.78 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190## A method to compute transition matrix if every individual does have multiple state.library(data.table)library(dplyr)library(magrittr)# data generationN_patient = 3000dat = sample(48, N_patient, replace = TRUE) %&gt;% &#123; cbind(rep(1:N_patient, times=.), unlist(sapply(., seq, from = 1))) &#125; %&gt;% cbind(replicate(6, sample(0:1, nrow(.), TRUE))) %&gt;% tbl_dt() %&gt;% setnames(c("id", "duration", paste0("M_", 1:6))) %&gt;% arrange(id, duration)st = proc.time()transitMatrix_eachTime = dat %&gt;% split(.$id) %&gt;% lapply(function(dd)&#123; out = lapply(1:47, function(x) matrix(0, 6, 6)) if (nrow(dd) &gt; 1) &#123; tmp = dd %&gt;% select(3:8) %&gt;% as.matrix() for(i in 2:nrow(dd)) &#123; transitMatrix = matrix(0, 6, 6); if (sum(tmp[i-1,]) &gt; 0) transitMatrix[tmp[i-1,]==1,] = t(replicate(sum(tmp[i-1,]), tmp[i,])) out[[i-1]] = transitMatrix &#125; &#125; out &#125;) %&gt;% Reduce(function(x, y) lapply(1:length(x), function(v) x[[v]]+y[[v]]), .) %&gt;% lapply(function(x) x / ifelse(rowSums(x)&gt; 0, rowSums(x), 1))proc.time() - st# user system elapsed# 32.10 0.19 33.26library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]List transitMatrix_f(umat M, uword maxDuration)&#123; uword maxDuration_id = M.n_rows; List transitMatrixList(maxDuration-1); umat transitMatrix(M.n_cols, M.n_cols); urowvec previous_M(M.n_cols); for (uword i = 1; i &lt; maxDuration; i++) &#123; transitMatrix.zeros(); if ( i &lt; maxDuration_id) &#123; previous_M = M.row(i-1); if (any(previous_M==1)) transitMatrix.rows(find(previous_M==1)) = repmat(M.row(i), sum(previous_M), 1); &#125; transitMatrixList[i-1] = wrap(transitMatrix); &#125; return transitMatrixList;&#125;')library(RcppEigen)sourceCpp(code = '// [[Rcpp::depends(RcppEigen)]]#include &lt;RcppEigen.h&gt;using namespace Rcpp;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;// [[Rcpp::export]]void list_sum_f(List Xr, List Yr) &#123; for(int i = 0; i &lt; Xr.size(); i++) Yr[i] = as&lt; Map&lt;MatrixXd&gt; &gt;(Xr[i]) + as&lt; Map&lt;MatrixXd&gt; &gt;(Yr[i]);&#125;// [[Rcpp::export]]List listAddition(List Xr) &#123; int n = Xr.size(); List list_sum = Xr[0]; for(int j = 1; j &lt; n; j++) list_sum_f(Xr[j], list_sum); return list_sum;&#125;')st = proc.time()maxDuration = max(dat$duration)transitMatrix_eachTime2 = dat %&gt;% split(.$id) %&gt;% lapply(function(x)&#123; transitMatrix_f(x %&gt;% select(-id, -duration) %&gt;% as.matrix(), maxDuration) &#125;) %&gt;% listAddition() %&gt;% lapply(function(x) x / ifelse(rowSums(x)&gt; 0, rowSums(x), 1))proc.time() - st# user system elapsed# 14.68 0.20 15.27library(reshape2)library(Matrix)st = proc.time()dat_previous = dat %&gt;% group_by(id) %&gt;% mutate_(.dots = paste0("c(0, M_", 1:6, "[-length(M_1)])")) %&gt;% setnames(old = tail(names(.), 6), new = paste0("M_", 1:6, "p"))dat_transform_1 = dat_previous %&gt;% melt(id = c("id", "duration"), measure = paste0("M_", 1:6)) %&gt;% filter(value == 1, duration &gt; 1) %&gt;% select(-value) %&gt;% transform(variable = as.numeric(substr(variable, 3, 3))) %&gt;% setnames("variable", "M") %&gt;% setkey(id, duration)dat_transform_2 = dat_previous %&gt;% melt(id = c("id","duration"), measure = paste0("M_", 1:6, "p")) %&gt;% filter(value == 1) %&gt;% select(-value) %&gt;% mutate(variable = as.numeric(substr(variable, 3, 3))) %&gt;% setnames("variable", "M_p") %&gt;% setkey(id, duration)dat_combined = dat_transform_1[dat_transform_2, allow.cartesian=TRUE] %&gt;% filter(!is.na(M), !is.na(M_p))transitMatrix_eachTime3 = dat_combined %&gt;% group_by(duration, M, M_p) %&gt;% summarise(count = n()) %&gt;% group_by(duration, M_p) %&gt;% mutate(transitProb = count / sum(count)) %&gt;% ungroup() %&gt;% split(.$duration) %&gt;% lapply(function(x) spMatrix(6, 6, x$M_p, x$M, x$transitProb))proc.time() - st# user system elapsed# 2.15 0.31 2.62st = proc.time()dat_transform = dat %&gt;% melt(id = c("id", "duration"), measure = paste0("M_", 1:6)) %&gt;% filter(value == 1) %&gt;% select(-value) %&gt;% transform(variable = as.numeric(substr(variable, 3, 3))) %&gt;% setnames("variable", "M") %&gt;% setkey(id, duration)dat_combined = dat_transform %&gt;% filter(duration &gt; 1) %&gt;% inner_join(dat_transform %&gt;% transform(duration = duration + 1), by = c("id", "duration"))transitMatrix_eachTime5 = dat_combined %&gt;% group_by(duration, M.x, M.y) %&gt;% summarise(count = n()) %&gt;% group_by(duration, M.y) %&gt;% mutate(transitProb = count / sum(count)) %&gt;% ungroup() %&gt;% split(.$duration) %&gt;% lapply(function(x) spMatrix(6, 6, x$M.y, x$M.x, x$transitProb))proc.time() - st# user system elapsed# 1.25 0.19 1.45library(tidyr)st = proc.time()dat_transform = dat %&gt;% gather(M, value, M_1:M_6) %&gt;% filter(value == 1) %&gt;% select(-value) %&gt;% transform(M = as.numeric(substr(M, 3, 3)))dat_combined = dat_transform %&gt;% filter(duration &gt; 1) %&gt;% inner_join(dat_transform %&gt;% transform(duration = duration + 1), by = c("id", "duration"))transitMatrix_eachTime5 = dat_combined %&gt;% group_by(duration, M.x, M.y) %&gt;% summarise(count = n()) %&gt;% group_by(duration, M.y) %&gt;% mutate(transitProb = count / sum(count)) %&gt;% ungroup() %&gt;% split(.$duration) %&gt;% lapply(function(x) spMatrix(6, 6, x$M.y, x$M.x, x$transitProb))proc.time() - st# user system elapsed# 1.28 0.12 1.40all.equal(transitMatrix_eachTime, transitMatrix_eachTime2)# TRUEall.equal(transitMatrix_eachTime, transitMatrix_eachTime3 %&gt;% lapply(as.matrix) %&gt;% set_names(NULL))# TRUEall.equal(transitMatrix_eachTime, transitMatrix_eachTime4 %&gt;% lapply(as.matrix) %&gt;% set_names(NULL))# TRUEall.equal(transitMatrix_eachTime, transitMatrix_eachTime5 %&gt;% lapply(as.matrix) %&gt;% set_names(NULL))# TRUE# &gt; transitMatrix_eachTime[[1]] # the transition matrix at period 2# [,1] [,2] [,3] [,4] [,5] [,6]# [1,] 0.1579643 0.1652346 0.1625909 0.1711831 0.1784534 0.1645737# [2,] 0.1698612 0.1692003 0.1639128 0.1625909 0.1771315 0.1573034# [3,] 0.1635638 0.1775266 0.1569149 0.1675532 0.1682181 0.1662234# [4,] 0.1661085 0.1654387 0.1634293 0.1654387 0.1694575 0.1701273# [5,] 0.1722746 0.1648721 0.1561238 0.1641992 0.1709287 0.1716016# [6,] 0.1675862 0.1620690 0.1648276 0.1655172 0.1765517 0.1634483]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>dplyr</tag>
        <tag>magrittr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Taking average every k periods]]></title>
    <url>%2Fposts%2F201503%2F2015-03-04-Taking-average-%20every-k-periods.html</url>
    <content type="text"><![CDATA[We have a repeated-measuring data. We want to take average every 3 periods. Here is code to do it. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162library(plyr)library(dplyr)library(data.table)library(magrittr)# data generationn = 200dat = data.table(id = 1:n, len = sample(2:15, n, replace = TRUE)) %&gt;% mdply(function(id, len) data.table(id = rep(id, len), values = rnorm(len)))dat = select(dat, c(id, values))# meank = 3start_time = Sys.time()result = dat %&gt;% group_by(id) %&gt;% mutate(newgroup = rep(1:ceiling(length(values)/k), each = k, length = length(values))) %&gt;% group_by(id, newgroup) %&gt;% summarise(mean(values))Sys.time() - start_timelibrary(dplyr)library(data.table)library(magrittr)# data generationdat_gen_f = function(N_patient, max_obs_time, n_vars)&#123; dat = sample(max_obs_time, N_patient, replace = TRUE) %&gt;% &#123; cbind(rep(1:N_patient, times=.), sapply(., seq, from = 1) %&gt;% unlist()) &#125; %&gt;% cbind(matrix(rnorm(nrow(.)*n_vars),, n_vars)) %&gt;% data.table() setnames(dat, c("id", "obs_times", paste0("V", 1:n_vars)))&#125;mean_dat_f = function(dat, k)&#123; result = dat %&gt;% group_by(id) %&gt;% mutate(newgroup = rep(1:ceiling(length(obs_times)/k), each = k, length=length(obs_times)), n_combine = (length(obs_times) %/% k) %&gt;% &#123;c(rep(k, . * k), rep(length(obs_times) - . * k, length(obs_times) - . * k))&#125;) %&gt;% ungroup() %&gt;% mutate(times_combine = paste((newgroup-1)*3+1, (newgroup-1)*3 + n_combine, sep="-")) result = result %&gt;% select(match(c(names(dat)[names(dat)!="obs_times"], "times_combine"), names(result))) %&gt;% extract(, lapply(.SD, mean), by = "id,times_combine") result&#125;start_time = Sys.time()dat = dat_gen_f(30000, 20, 15)Sys.time() - start_time# Time difference of 1.503086 secsstart_time = Sys.time()result = mean_dat_f(dat, 3)Sys.time() - start_time# Time difference of 4.236243 secsstart_time = Sys.time()dat = dat_gen_f(13820, 15, 1)Sys.time() - start_time# Time difference of 0.4750271 secsstart_time = Sys.time()result = mean_dat_f(dat, 3)Sys.time() - start_time# Time difference of 1.848106 secs]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>dplyr</tag>
        <tag>magrittr</tag>
        <tag>plyr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hotkeys in sublime text]]></title>
    <url>%2Fposts%2F201502%2F2015-02-28-hotkeys-in-sublime-text.html</url>
    <content type="text"><![CDATA[一些sublime text 3好用的快捷鍵。 CTRL + D: 選擇當前的字串(兩個空白字元(包含\n, \t等)之間)CTRL + L: 選擇當前行CTRL+ SHIFT + SPACE: 選當前的scope(在兩個括號之間)CTRL + ALT + UP / DOWN: 做多行編輯 (筆電要先關掉旋轉的快捷鍵)CTRL + PAGE UP / DOWN: 切換同一個layout的文件ALT+ SHIFT + 2: 切換成雙欄 (1是單欄，3則是3欄，最多4欄，89是2,3欄)CTRL + number: 切換不同的layoutCTRL + SHIFT + D: 可以複製同一行]]></content>
      <categories>
        <category>Sublime Text</category>
      </categories>
      <tags>
        <tag>Sublime Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用windows batch快速複製excel檔案(逐月)，並在excel依檔名變更日期]]></title>
    <url>%2Fposts%2F201502%2F2015-02-20-excel-and-winbash.html</url>
    <content type="text"><![CDATA[寫了一個windows batch 去複製檔案~~~ 緣由：這個單位每天都要用excel做日報表一般作法是複製改裡面的日期，一次做大概半年用這種事情相當耗時… 改進方式： EXCEL可以根據呼叫該檔檔名，根據該檔檔名去變動日期EX: 1040216日報表.xls 裡面日期會顯示 104年02月16日EXCEL指令： 123456=CONCATENATE( LEFT(MID(CELL("filename"), SEARCH("[",CELL("filename"))+1, SEARCH("]",CELL("filename")) - SEARCH("[",CELL("filename"))-1), 3), "年", RIGHT(LEFT(MID(CELL("filename"), SEARCH("[",CELL("filename")) + 1, SEARCH("]",CELL("filename"))-SEARCH("[",CELL("filename"))-1), 5),2), "月", RIGHT(LEFT(MID(CELL("filename"),SEARCH("[",CELL("filename")) + 1, SEARCH("]",CELL("filename"))-SEARCH("[",CELL("filename"))-1), 7),2), "日") BAT檔案做COPY，自動根據月份做判斷複製相對應的份數，並且改成適當的檔名母檔：YYYMMDD.xls複製成 1040201.xls 到 1040228***.xls 12345678910111213141516171819202122232425262728293031323334353637383940414243@Echo Offset "year=1990"set "month=01"if %month% == 01 goto d31if %month% == 02 goto d28if %month% == 03 goto d31if %month% == 04 goto d30if %month% == 05 goto d31if %month% == 06 goto d30if %month% == 07 goto d31if %month% == 08 goto d31if %month% == 09 goto d30if %month% == 10 goto d31if %month% == 11 goto d30if %month% == 12 goto d31:d31for /d %%i IN (0 1 2) DO ( for %%j IN (0 1 2 3 4 5 6 7 8 9) DO ( copy "YYYYMMDD測試檔案.xls" "%year%%month%%%i%%j測試檔案.xls" ))del "%year%%month%00測試檔案.xls"copy "YYYYMMDD測試檔案.xls" "%year%%month%30測試檔案.xls"copy "YYYYMMDD測試檔案.xls" "%year%%month%31測試檔案.xls":d30for /d %%i IN (0 1 2) DO ( for %%j IN (0 1 2 3 4 5 6 7 8 9) DO ( copy "YYYYMMDD測試檔案.xls" "%year%%month%%%i%%j測試檔案.xls" ))del "%year%%month%00測試檔案.xls"copy "YYYYMMDD測試檔案.xls" "%year%%month%30測試檔案.xls":d28for /d %%i IN (0 1 2) DO ( for %%j IN (0 1 2 3 4 5 6 7 8 9) DO ( copy "YYYYMMDD測試檔案.xls" "%year%%month%%%i%%j測試檔案.xls" ))del "%year%%month%00測試檔案.xls"del "%year%%month%29測試檔案.xls"]]></content>
      <categories>
        <category>windows bat</category>
      </categories>
      <tags>
        <tag>windows bat</tag>
        <tag>excel</tag>
        <tag>copy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parsing html page with javascript]]></title>
    <url>%2Fposts%2F201502%2F2015-02-15-pasrsing-html-with-javascript.html</url>
    <content type="text"><![CDATA[Here is a demonstration to grab lottery number by using RSelenium. Since RCurl cannot parse the table produced by javascript, we need to construct a server to interpret javascript and grab the information we need. The tool of building server is RSelenium. It is simple to generate the html page by javascript, then we can access the table we want. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546library(RCurl)library(XML)library(RSelenium)# RSelenium::checkForServer()library(data.table)library(dplyr)library(magrittr)## Access DatareadLotteryHistory = function(year)&#123; lottery_url = paste0("http://www.cpzhan.com/lotto649/all-results?year=", year) remDr$navigate(lottery_url) webElem &lt;- remDr$findElement(using = 'class name', value = "mytable") tblSource = webElem$getElementAttribute("outerHTML")[[1]] tables = readHTMLTable(tblSource) tables&#125;year_today = year(Sys.Date())RSelenium::startServer()remDr = RSelenium::remoteDriver()remDr$open()lottery = 2004:year_today %&gt;% sapply(function(year) readLotteryHistory(year)) %&gt;% rbindlist(.)remDr$close()remDr$closeServer()names_lottery = c("year", "number", "date", "ball_1", "ball_2", "ball_3", "ball_4", "ball_5", "ball_6", "ball_s")date_name = c("year", "month", "day")lottery = lottery %&gt;% setnames(old = names_lottery) %&gt;% .$date %&gt;% as.POSIXct(.) %&gt;% sapply(function(x) c(year(x), month(x), mday(x))) %&gt;% t() %&gt;% data.table() %&gt;% setnames(old = date_name) %&gt;% cbind(lottery) %&gt;% select(c(1:3, 7:13)) %&gt;% mutate(number = 1:nrow(lottery)) %&gt;% setcolorder(c(names_lottery[2], date_name, names_lottery[4:10]))lottery_HT = lottery[, lapply(.SD, as.numeric)] # history table# number year month day ball_1 ball_2 ball_3 ball_4 ball_5 ball_6 ball_s# 1: 1 2004 1 5 24 4 39 43 29 5 13# 2: 2 2004 1 8 21 36 18 23 7 14 26# 3: 3 2004 1 12 25 16 29 11 16 33 33# 4: 4 2004 1 15 21 19 9 25 9 20 4# 5: 5 2004 1 19 23 7 5 20 29 13 35# ---# 1163: 1163 2015 1 30 26 6 15 47 20 34 18# 1164: 1164 2015 2 3 21 5 7 9 40 7 45# 1165: 1165 2015 2 6 3 31 49 20 36 27 40# 1166: 1166 2015 2 10 19 6 33 17 6 26 46# 1167: 1167 2015 2 13 15 1 19 43 6 26 29 The encoding in windows is trouble, I have spent 2 hour on it, but it is still unsolved. Therefore, if you want to parse the html involving non-UTF8 characters, just aware it.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>crawler</tag>
        <tag>RSelenium</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster R in windows]]></title>
    <url>%2Fposts%2F201502%2F2015-02-12-Faster-R-in-windows.html</url>
    <content type="text"><![CDATA[R的BLAS庫效率不彰，在linux上可以透過更換成openBLAS來加速，或是compiled with intel MKL，在windows上compile R是一個痛苦的過程。 因此，有人提供這方面的資源，最有名的就是Revolution，Revolution是compiled with intel MKL，但是要錢。但是天無絕人之路，總是有其他辦法的。 用Revolution R Open，你可以在官方網站下載到。 1-2. 如果討厭RRO的猴子圖案，可以把RRO/bin/x64中的libiomp5md.dll, RBlas.dll, Rlapack.dll這三個檔案複製到R/bin/x64取代原本的檔案。 更換BLAS庫，網路上有人提供GotoBLAS2編譯的RBlas.dll，連結在此，win32的部分，可以參考CRAN，下載相對應CPU的RBlas.dll然後替換掉R/bin/x64 (or i386)的RBlas.dll即可享受快速的BLAS帶來的效能。 至於OpenBLAS的部分則參考此連結，這個方法比較複雜一點。 總結：個人測試這三個BLAS都差不多快，不會差太多，自己選擇喜歡的使用即可。但是有人回報用第三個方案常常會導致R崩潰，可能是因為沒有經過R的編譯之緣故。]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>BLAS</tag>
        <tag>Revolution R Open</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Implementation for summing a list of matrices]]></title>
    <url>%2Fposts%2F201501%2F2015-01-18-Implementation-for-summing-a-list-of-matrices.html</url>
    <content type="text"><![CDATA[Usually, we have to sum a list of matrices, I introduce several ways to do that. In my test, Reduce is the most easy and fast to do this, but it need a lot of memory. A function written in Rcpp is the fastest way to do. But there is a requirement that you have learned C++. Besides above two methods, the naive method by using for is not so slow, it is also a good option. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# Aware that this setting require your memory to be large than 6GB. Otherwise, your computer will be out of memory and maybe shut down. If this happens or not enough memory, please change the `mat_size` or `length_list`.length_list = 30mat_size = c(3000, 3000)mat = matrix(rnorm(prod(mat_size)), mat_size[1], mat_size[2])mat.list = lapply(1:length_list, function(i) mat)res1 = function()&#123; mat_sum = matrix(rep(0, prod(mat_size)), mat_size[1], mat_size[2]) for (i in 1:length_list)&#123; mat_sum = mat_sum + mat.list[[i]] &#125; mat_sum&#125;res2 = function() Reduce('+', mat.list)res3 = function() matrix(rowSums(matrix(unlist(mat.list), nrow=prod(mat_size))), nrow=mat_size[1])res4 = function() matrix(colSums(do.call(rbind, lapply(mat.list, as.vector))), nrow=mat_size[1])res5 = function() matrix(rowSums(do.call(cbind, lapply(mat.list, as.vector))), nrow=mat_size[1])library(Rcpp)library(RcppArmadillo)library(inline)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;stdio.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]mat list_mat_sum_f(List data_list)&#123; int n = data_list.size(); SEXP mat1 = data_list[0]; NumericMatrix mat1_rmat(mat1); mat result_mat(mat1_rmat.begin(), mat1_rmat.nrow(), mat1_rmat.ncol()); for(int i = 1; i &lt; n; i++) &#123; SEXP tmp_m = data_list[i]; NumericMatrix data_m(tmp_m); mat working_m(data_m.begin(), data_m.nrow(), data_m.ncol(), false); result_mat += working_m; &#125; return result_mat;&#125;')res6 = function() list_mat_sum_f(mat.list)# cmpfunlibrary(compiler)res1_cmp = cmpfun(res1)res2_cmp = cmpfun(res2)res3_cmp = cmpfun(res3)res4_cmp = cmpfun(res4)res5_cmp = cmpfun(res5)all.equal(tmp &lt;- res1(), res2())# TRUEall.equal(tmp, res3())# TRUEall.equal(tmp, res4())# TRUEall.equal(tmp, res5())# TRUEall.equal(tmp, res6())# TRUElibrary(rbenchmark)benchmark(res1(), res2(), res3(), res4(), res5(), res6(), res1_cmp(), res2_cmp(), res3_cmp(), res4_cmp(), res5_cmp(), replications = 20, order='relative', columns=c('test', 'replications', 'elapsed','relative', 'user.self'))# test replications elapsed relative user.self# 6 res6() 20 6.49 1.000 6.13# 2 res2() 20 15.58 2.401 8.45# 8 res2_cmp() 20 16.19 2.495 8.61# 1 res1() 20 18.44 2.841 10.61# 7 res1_cmp() 20 19.33 2.978 11.15# 5 res5() 20 56.60 8.721 40.24# 11 res5_cmp() 20 56.79 8.750 40.36# 4 res4() 20 70.48 10.860 54.63# 10 res4_cmp() 20 70.53 10.867 55.17# 3 res3() 20 92.87 14.310 76.21# 9 res3_cmp() 20 93.74 14.444 77.19# length_list = 60# mat_size = c(3000, 3000)# benchmark(res2(), res6(), replications = 20, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')## test replications elapsed relative user.self# 2 res6() 20 12.02 1.000 11.73# 1 res2() 20 31.66 2.634 16.44]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>Reduce</tag>
        <tag>do.call</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Finding unique rows of a matrix in mex]]></title>
    <url>%2Fposts%2F201501%2F2015-01-14-unique-rows-of-matrix-in-mex.html</url>
    <content type="text"><![CDATA[This code is to find the unique rows of a matrix. It is based on the quick sort algorithm. Thanks to this, I can use MatLab to implement faster sorting of rows. I compiled this code with Microsoft Visual Studio 2015 and Intel Parallel Studio XE 2017 and linking to armadillo library. I put files in my github (Link). My version of MatLab is 2016a. The test script: 12345678A = [9 2 9 5; 9 2 9 0; 9 2 9 5; 9 2 9 0; 9 2 9 5];unique_rows(A)% 9 2 9 0% 9 2 9 5unique(A, 'rows')% 9 2 9 0% 9 2 9 5 The compile script: 12345678MKLROOT = 'C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2017.1.143\windows\mkl';ICLLIB = 'C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2017.1.143\windows\compiler\lib\intel64_win';OPTIMFLAGS = 'OPTIMFLAGS= /O3 /DNDEBUG /QxHost /Qparallel /Qopenmp /Qprec-div- /Qipo /Qinline-calloc';LINKER = 'LINKER=xilink';mex('-f', 'intel_cpp_17_vs2015.xml', '-v', '-largeArrayDims', '-IC:\armadillo-7.800.2\include', ... ['-I', MKLROOT, '\include'], ['-L', MKLROOT, '\lib\intel64'], ['-L', ICLLIB], ... 'test_mkl_blas.cpp', OPTIMFLAGS, LINKER, '-lmkl_intel_lp64', '-lmkl_intel_thread', ... '-lmkl_core', '-llibiomp5md') My mex setup file (intel_cpp_17_vs2015.xml): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;config Name="Intel C++ Composer XE 2017 with Microsoft Visual Studio 2015" ShortName="IntelCPP15MSVCPP140" Manufacturer="Intel" Version="17.0" Language="C++" Priority="A" Location="$CPPROOT" &gt; &lt;Details CompilerExecutable="$COMPILER" CompilerDefines="$COMPDEFINES" CompilerFlags="$COMPFLAGS" OptimizationFlags="$OPTIMFLAGS" DebugFlags="$DEBUGFLAGS" IncludeFlags="$INCLUDE" LinkerExecutable="$LINKER" LinkerFlags="$LINKFLAGS" LinkerLibraries="$LINKLIBS" LinkerDebugFlags="$LINKDEBUGFLAGS" LinkerOptimizationFlags="$LINKOPTIMFLAGS" CommandLineShell="$CPPROOT\bin\iclvars.bat " CommandLineShellArg="intel64 vs2015" CompilerDefineFormatter="/D%s" LinkerLibrarySwitchFormatter="lib%s.lib;%s.lib" LinkerPathFormatter="/LIBPATH:%s" LibrarySearchPath="$$LIB;$$LIBPATH;$$PATH;$$INCLUDE;$MATLABROOT\extern\lib\$ARCH\microsoft" /&gt; &lt;!-- Switch guide: http://msdn.microsoft.com/en-us/library/fwkeyyhe(v=vs.71).aspx --&gt; &lt;vars CMDLINE100="$COMPILER /c $COMPFLAGS $OPTIM $COMPDEFINES $INCLUDE $SRC /Fo$OBJ" CMDLINE200="$LINKER $LINKFLAGS $LINKTYPE $LINKOPTIM $LINKEXPORT $OBJS $LINKLIBS /out:$EXE" CMDLINE250="mt -outputresource:$EXE;2 -manifest $MANIFEST" CMDLINE300="del $EXP $LIB $MANIFEST $ILK" COMPILER="icl" COMPFLAGS="/Zp8 /GR /W3 /EHs /nologo /MD" COMPDEFINES="/D_CRT_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_DEPRECATE /D_SECURE_SCL=0 $MATLABMEX" MATLABMEX=" /DMATLAB_MEX_FILE" OPTIMFLAGS="/O2 /DNDEBUG" INCLUDE="-I&amp;quot;$MATLABROOT\extern\include&amp;quot; -I&amp;quot;$MATLABROOT\simulink\include&amp;quot;" DEBUGFLAGS="/Z7" LINKER="link" LINKFLAGS="/nologo /manifest " LINKTYPE="/DLL" LINKEXPORT="/EXPORT:mexFunction" LINKLIBS="/LIBPATH:&amp;quot;$MATLABROOT\extern\lib\$ARCH\microsoft&amp;quot; libmx.lib libmex.lib libmat.lib kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib" LINKDEBUGFLAGS="/debug /PDB:&amp;quot;$TEMPNAME$LDEXT.pdb&amp;quot;" LINKOPTIMFLAGS="" OBJEXT=".obj" LDEXT=".mexw64" SETENV="set COMPILER=$COMPILER set COMPFLAGS=/c $COMPFLAGS $COMPDEFINES $MATLABMEX set OPTIMFLAGS=$OPTIMFLAGS set DEBUGFLAGS=$DEBUGFLAGS set LINKER=$LINKER set LINKFLAGS=$LINKFLAGS /export:%ENTRYPOINT% $LINKTYPE $LINKLIBS $LINKEXPORT set LINKDEBUGFLAGS=/debug /PDB:&amp;quot;%OUTDIR%%MEX_NAME%$LDEXT.pdb&amp;quot; set NAME_OUTPUT=/out:&amp;quot;%OUTDIR%%MEX_NAME%%MEX_EXT%&amp;quot;" /&gt; &lt;client&gt; &lt;engine LINKLIBS="$LINKLIBS libeng.lib" LINKEXPORT="" LDEXT=".exe" LINKTYPE="" MATLABMEX="" /&gt; &lt;/client&gt; &lt;locationFinder&gt; &lt;!-- CPPROOT=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries\windows\ --&gt; &lt;CPPROOT&gt; &lt;and&gt; &lt;envVarExists name="ICPP_COMPILER17" /&gt; &lt;fileExists name="$$\Bin\intel64\icl.exe" /&gt; &lt;dirExists name="$$\..\.." /&gt; &lt;/and&gt; &lt;/CPPROOT&gt; &lt;!-- VCROOT=C:\Program Files (x86)\Microsoft Visual Studio 14.0\ --&gt; &lt;VCROOT&gt; &lt;and&gt; &lt;or&gt; &lt;hklmExists path="SOFTWARE\Microsoft\VisualStudio\SxS\VC7" name="14.0" /&gt; &lt;hkcuExists path="SOFTWARE\Microsoft\VisualStudio\SxS\VC7" name="14.0" /&gt; &lt;hklmExists path="SOFTWARE\Wow6432Node\Microsoft\VisualStudio\SxS\VC7" name="14.0" /&gt; &lt;hkcuExists path="SOFTWARE\Wow6432Node\Microsoft\VisualStudio\SxS\VC7" name="14.0" /&gt; &lt;/or&gt; &lt;fileExists name="$$\bin\amd64\cl.exe" /&gt; &lt;dirExists name="$$\..\.." /&gt; &lt;/and&gt; &lt;/VCROOT&gt; &lt;!-- SDKROOT=C:\Program Files (x86)\Windows Kits\8.1\ --&gt; &lt;SDKROOT&gt; &lt;or&gt; &lt;hklmExists path="SOFTWARE\Microsoft\Microsoft SDKs\Windows\v8.1" name="InstallationFolder" /&gt; &lt;hkcuExists path="SOFTWARE\Microsoft\Microsoft SDKs\Windows\v8.1" name="InstallationFolder" /&gt; &lt;hklmExists path="SOFTWARE\Wow6432Node\Microsoft\Microsoft SDKs\Windows\v8.1" name="InstallationFolder" /&gt; &lt;hkcuExists path="SOFTWARE\Wow6432Node\Microsoft\Microsoft SDKs\Windows\v8.1" name="InstallationFolder" /&gt; &lt;/or&gt; &lt;/SDKROOT&gt; &lt;!-- KITSROOT=C:\Program Files (x86)\Windows Kits\10 --&gt; &lt;KITSROOT&gt; &lt;or&gt; &lt;hklmExists path="SOFTWARE\Microsoft\Windows Kits\Installed Roots" name="KitsRoot10" /&gt; &lt;hkcuExists path="SOFTWARE\Microsoft\Windows Kits\Installed Roots" name="KitsRoot10" /&gt; &lt;hklmExists path="SOFTWARE\Wow6432Node\Microsoft\Windows Kits\Installed Roots" name="KitsRoot10" /&gt; &lt;hkcuExists path="SOFTWARE\Wow6432Node\Microsoft\Windows Kits\Installed Roots" name="KitsRoot10" /&gt; &lt;/or&gt; &lt;/KITSROOT&gt; &lt;/locationFinder&gt; &lt;env PATH="$CPPROOT\bin\intel64;$VCROOT\bin\amd64;$VCROOT\vcpackages;$VCROOT\..\Common7\IDE;$VCROOT\..\Common7\Tools;$SDKROOT\bin\x64;" INCLUDE="$CPPROOT\compiler\include;$VCROOT\include;$VCROOT\ATLMFC\INCLUDE;$KITSROOT\include\10.0.10240.0\ucrt;$SDKROOT\Include\shared;$SDKROOT\Include\um;$SDKROOT\Include\winrt;$MATLABROOT\extern\include;" LIB="$CPPROOT\compiler\lib\intel64;$VCROOT\lib\amd64;$VCROOT\ATLMFC\Lib\amd64;$KITSROOT\Lib\10.0.10240.0\ucrt\x64;$SDKROOT\Lib\winv6.3\um\x64;$MATLABROOT\lib\win64;" LIBPATH="$CPPROOT\compiler\lib\intel64;$VCROOT\lib\amd64;$VCROOT\atlmfc\lib\amd64;$SDKROOT\Lib\winv6.3\um\x64;$MATLABROOT\extern\lib\win64;" /&gt;&lt;/config&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// import header files#include &lt;omp.h&gt;// use armadillo#define ARMA_USE_MKL_ALLOC#include &lt;armadillo&gt;// include matlab mex header files#include "mex.h"#include "matrix.h"using namespace arma;void armaSetPr(mxArray *matlabMatrix, const Mat&lt;double&gt;&amp; armaMatrix)&#123; double *dst_pointer = mxGetPr(matlabMatrix); const double *src_pointer = armaMatrix.memptr(); std::memcpy(dst_pointer, src_pointer, sizeof(double)*armaMatrix.n_elem);&#125;int compare_vec(const rowvec&amp; mat_row, const rowvec&amp; pivot_row)&#123; int v = 0; for (uword i = 0; i &lt; mat_row.n_elem; i++) &#123; if (mat_row(i) &lt; pivot_row(i)) v = 1; else if (mat_row(i) &gt; pivot_row(i)) v = -1; if (v != 0) break; &#125; return v;&#125;void sortrows_f(mat&amp; M, const int left, const int right)&#123; if (left &lt; right) &#123; int i = left, j = right; uword mid_loc = (uword) (left+right)/2, pivot_loc = mid_loc; if (right - left &gt; 5) &#123; uvec sortIndex = stable_sort_index(M.col(0).subvec(mid_loc-2, mid_loc+2)); pivot_loc = as_scalar(find(sortIndex == 2)) + mid_loc - 1; &#125; rowvec pivot_row = M.row(pivot_loc); while (i &lt;= j) &#123; while (compare_vec(M.row( (uword) i), pivot_row) == 1) i++; while (compare_vec(M.row( (uword) j), pivot_row) == -1) j--; if (i &lt;= j) &#123; M.swap_rows((uword) i, (uword) j); i++; j--; &#125; &#125; if (j &gt; 0) sortrows_f(M, left, j); if (i &lt; (int) M.n_rows - 1) sortrows_f(M, i, right); &#125;&#125;void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[])&#123; arma_rng::set_seed_random(); int max_threads_mkl = mkl_get_max_threads(); mkl_set_num_threads(max_threads_mkl); mat x = Mat&lt;double&gt;(mxGetPr(prhs[0]), (uword) mxGetM(prhs[0]), (uword) mxGetN(prhs[0]), false, true); sortrows_f(x, 0, x.n_rows - 1); uvec unique_v = join_cols(ones&lt;uvec&gt;(1), any(x.rows(0, x.n_rows-2) != x.rows(1, x.n_rows-1), 1)); mat output = x.rows(find(unique_v)); plhs[0] = mxCreateDoubleMatrix(output.n_rows, output.n_cols, mxREAL); armaSetPr(plhs[0], output);&#125;]]></content>
      <categories>
        <category>MatLab</category>
      </categories>
      <tags>
        <tag>Intel C++</tag>
        <tag>MatLab</tag>
        <tag>mex</tag>
        <tag>unique row</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tables combining]]></title>
    <url>%2Fposts%2F201408%2F2014-08-20-tables-combining.html</url>
    <content type="text"><![CDATA[A benchmark for combine the two data.frames in R. There are two tables: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112library(data.table)library(plyr)library(dplyr)library(magrittr)library(reshape2)ref_m = data.table( "gene_a" = c("A", "B", "C"), "Chromosome" = c("1", "X", "2"), "gene_start" = c(25000, 1000, 0), "gene_end" = c(50000, 2000, 800)) %&gt;% tbl_dt(FALSE)ref_m# gene_a Chromosome gene_start gene_end# 1 A 1 25000 50000# 2 B X 1000 2000# 3 C 2 0 800dat = data.table( "Probe_b" = c("a1", "a2", "a3", "a4", "a5"), "Chromosome" = c("2", "4", "1", "X", "1"), "Chr_s" = as.integer(c(175, 600, 23575, 1010, 30000)), "Chr_e" = as.integer(c(200, 625, 23600, 1035, 30025)), stringsAsFactors = FALSE) %&gt;% tbl_dt(FALSE)dat# Probe_b Chromosome Chr_s Chr_e# 1 a1 2 175 200# 2 a2 4 600 625# 3 a3 1 23575 23600# 4 a4 X 1010 1035# 5 a5 1 30000 30025# we want combine these two tables like following by referring the Chromosome and checking that the range of Chr_s and Chr_e is fallen into range of gene_start and gene_end .# gene_a match# 1 A a5# 2 B a4# 3 C a1## first methodcombine_f = function(dat, ref_m)&#123; check_f = function(v, ref_m)&#123; loc &lt;- match(v$Chromosome, ref_m$Chromosome, nomatch = 0) if(length(loc) != 0 &amp;&amp; loc &gt; 0) # 避免loc出現integer(0)的情況 &#123; if(v$Chr_s &gt;= ref_m[loc,]$gene_s &amp;&amp; v$Chr_e &lt;= ref_m[loc,]$gene_e) return(as.character(ref_m[loc,]$gene_a)) else return(NA) &#125; else return(NA) &#125; result = rep(NA, nrow(dat)) aaply(1:nrow(dat), 1, function(v) check_f(dat[v,], ref_m)) %&gt;% &#123;tapply(as.character(dat$Probe_b), ., c)&#125;&#125;combine_f(dat, ref_m)# A B C# "a5" "a4" "a1"## second methodcombine_f2 = function(dat, ref_m)&#123; loc_ref_v = match(dat$Chromosome, ref_m$Chromosome) loc_dat_v = loc_ref_v &gt; 0 loc_dat_v = loc_dat_v[!is.na(loc_dat_v)] loc_ref_v = loc_ref_v[loc_dat_v] loc = dat$Chr_s[loc_dat_v] &gt;= ref_m$gene_start[loc_ref_v] &amp; dat$Chr_e[loc_dat_v] &lt;= ref_m$gene_end[loc_ref_v] tapply(dat$Probe_b[loc_dat_v][loc], ref_m$gene_a[loc_ref_v[loc]], c)&#125;combine_f2(dat, ref_m)# A B C# "a5" "a4" "a1"## third methodcombine_f3 = function(dat, ref_m)&#123; merge(dat, ref_m, by="Chromosome") %&gt;% filter((Chr_s &gt; gene_start) &amp; (Chr_e &lt; gene_end)) %&gt;% select(one_of(c("gene_a", "Probe_b"))) %&gt;% dlply(.(gene_a), function(x) x$Probe_b)&#125;combine_f3(dat, ref_m)## test the performanceN = 5000; k = 20ref_m = data.table( gene_a = LETTERS[1:k], Chromosome = sample(c(as.character(1:25), "X"), k), gene_start = sample(seq(0, 50000, 200), k)) %&gt;% mutate(gene_end = gene_start+sample(seq(600, 15000, 200), k)) %&gt;% tbl_dt(FALSE)dat = data.table( Probe_b = paste0("a", 1:N), Chromosome = sample(c(as.character(1:25), "X"), N, replace=TRUE), Chr_s = as.integer(sample(seq(0, 50000, 5), N, replace=TRUE))) %&gt;% mutate(Chr_e = Chr_s + sample(seq(1000, 10000, 100), N, replace=TRUE)) %&gt;% tbl_dt(FALSE)all.equal( combine_f(dat, ref_m), combine_f2(dat, ref_m))# TRUEall.equal( combine_f2(dat, ref_m), combine_f3(dat, ref_m), check.attributes =FALSE)# TRUElibrary(rbenchmark)benchmark(combine_f(dat, ref_m), combine_f2(dat, ref_m), combine_f3(dat, ref_m), replications = 20, columns = c("test", "replications", "elapsed", "relative"), order = "relative")# test replications elapsed relative# 2 combine_f2(dat, ref_m) 20 0.11 1.000# 3 combine_f3(dat, ref_m) 20 0.38 3.455# 1 combine_f(dat, ref_m) 20 406.35 3694.091 The second method has the best performance, but it is a hard coding. Therefore, I recommend the third method.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>dplyr</tag>
        <tag>reshape2</tag>
        <tag>magrittr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grouping in a fixed time interval]]></title>
    <url>%2Fposts%2F201408%2F2014-09-02-grouping-in-a-fixed-time-interval.html</url>
    <content type="text"><![CDATA[There is a data give the Time and ID to find the Session. The value of variable session is given by the Time variable. The session of first time in ID is 1, session is 2 if the different time between its time and the first time of session time is longer than 1 hour, and otherwise session is 1 until next time is out of range of 1 hour. For example, the data looks like: 1234567891011ID Time Session1 2014-08-28 00:00:00 11 2014-08-28 00:23:33 11 2014-08-28 00:59:59 11 2014-08-28 01:02:17 21 2014-08-28 02:30:22 31 2014-08-28 03:29:59 32 2014-08-28 00:00:01 12 2014-08-28 03:25:49 22 2014-08-28 03:49:13 22 2014-08-28 04:29:15 3 We generate ID and time to test the performance. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778library(data.table)library(dplyr)library(magrittr)library(reshape2)set.seed(100)n = 2100start_time = strptime("2014-08-01 00:00:00", "%Y-%m-%d %H:%M:%S")dat = data.table(ID = rep(1:n, ceiling(runif(n) * 10) + 1)) %&gt;% mutate(tmp_v = 1) %&gt;% group_by(ID) %&gt;% mutate(Time = as.POSIXct(sort(start_time + round(86400 * runif(length(tmp_v)))))) %&gt;% select(one_of(c("ID", "Time"))) %&gt;% tbl_dt(FALSE)# first methodsplit_session_f = function(dat)&#123;Session &lt;- rep(0, nrow(dat)) id = dat$ID[1] session.start = dat$Time[1] Session[1] = 1 for (row in 2:nrow(dat)) &#123; if (id != dat$ID[row]) &#123; session.start = dat$Time[row] Session[row] = 1 id = dat$ID[row] &#125; else &#123; if (as.numeric(dat$Time[row]-session.start, unit='hours') &gt;= 1) &#123; Session[row] = Session[row-1] + 1 session.start = dat$Time[row] &#125; else &#123; Session[row] = Session[row-1] &#125; &#125; &#125; Session&#125;# second methodsplit_session_sub_f = function(time)&#123; output = rep(0, length(time)) start_time_index = 1; session_num = 1 repeat &#123; loc = difftime(time, time[start_time_index], unit='hours') &lt; 1 &amp; output == 0 output[loc] = session_num session_num = session_num + 1 start_time_index = start_time_index + sum(loc) if(start_time_index &gt; length(time)) break &#125; output&#125;split_session_f2 = function(dat)&#123; tapply(dat$Time, dat$ID, split_session_sub_f) %&gt;% unlist() %&gt;% set_names(NULL)&#125;# third methodsplit_session_f3 = function(dat)&#123; dat %&gt;% group_by(ID) %&gt;% mutate(Session = split_session_sub_f(Time)) %&gt;% use_series("Session")&#125;all.equal(split_session_f(dat), split_session_f2(dat))# TRUEall.equal(split_session_f2(dat), split_session_f3(dat))# TRUE## test the performancelibrary(rbenchmark)benchmark(split_session_f(dat), split_session_f2(dat), split_session_f3(dat), replications = 20, columns = c("test", "replications", "elapsed", "relative"), order = "relative")# test replications elapsed relative# 3 split_session_f3(dat) 20 43.43 1.000# 2 split_session_f2(dat) 20 44.99 1.036# 1 split_session_f(dat) 20 121.34 2.794]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
        <tag>dplyr</tag>
        <tag>reshape2</tag>
        <tag>magrittr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R with GPU performance of HiPLARM]]></title>
    <url>%2Fposts%2F201408%2F2014-08-03-R-with-GPU-performance-of-HiPLARM.html</url>
    <content type="text"><![CDATA[This post is to benchmark the performance of HiPLARM and to introduce how to install HiPLARM. The performance and test code are showed in following: 123456789101112131415161718192021222324252627282930313233343536373839library(Matrix)p = 6000X = Matrix(rnorm(p**2), p)Y = Matrix(rnorm(p**2), p)s = proc.time(); Z = X %*% Y; proc.time() - s# user system elapsed# 20.646 0.137 5.193s = proc.time(); Z = solve(X, Y); proc.time() - s# user system elapsed# 40.418 0.342 10.741s = proc.time(); Z = crossprod(X, Y); proc.time() - s# user system elapsed# 22.706 0.128 5.986s = proc.time(); Z = chol(X %*% t(X)); proc.time() - s# user system elapsed# 25.774 0.242 6.934s = proc.time(); Z = solve(t(X) %*% X, X %*% Y); proc.time() - s# user system elapsed# 86.842 0.755 23.521library(HiPLARM)p = 6000X = Matrix(rnorm(p**2), p)Y = Matrix(rnorm(p**2), p)s = proc.time(); Z = X %*% Y; proc.time() - s# user system elapsed# 2.826 1.031 3.858s = proc.time(); Z = solve(X, Y); proc.time() - s# user system elapsed# 4.295 1.343 5.642s = proc.time(); Z = crossprod(X, Y); proc.time() - s# user system elapsed# 2.899 1.029 3.931s = proc.time(); Z = chol(X %*% t(X)); proc.time() - s# user system elapsed# 4.165 1.333 5.504s = proc.time(); Z = solve(t(X) %*% X, X %*% Y); proc.time() - s# user system elapsed# 10.440 3.377 13.825 HiPLARM is 1.6 times faster than the R without HiPLARM. I think that it will be much faster if the size of matrix is bigger. Since the memory of my gpu is 2GB, I can’t test bigger size matrix. The tests for smaller size matrix are almost the same because the data movement between host (CPU) and device (GPU). PS: I have ran the benchmark script(found in Simon Urbanek’s), but it is so slow that I do not put it on this post. I simply introduce how to install HiPLARM. Since my R is compiled by icc and MKL, so I have some trouble in installing it. You can download the auto-installer in here. I ran the auto-installer with ALTAS (I can’t compile OpenBLAS and I don’t know why.) and it stopped at installing the R package caused by the environment variable. I solve this problem by add following line in the file .bashrc: 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/clarence/Downloads/LALibs/lib where /home/clarence/Downloads/LALibs/lib is the installation directory of magma, plasma and hwloc. After installation, you should add export R_PLASMA_NUM_THREADS=4 into .bashrc. Then you can use it!! I have tried to compile plasma and magma with icc and MKL, the install script is present in following: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121#!/bin/bashmkdir LALibsexport BLDDIR=/home/clarence/Downloads/LALibscd $BLDDIRmkdir libmkdir includeLIBDIR=$BLDDIR/libINCDIR=$BLDDIR/includeNUM_PHYS_CPU=`cat /proc/cpuinfo | grep &apos;physical id&apos; | sort | uniq | wc -l`NUM_CORES=`cat /proc/cpuinfo | grep &apos;cpu cores&apos; | uniq | sed &apos;s/[a-z]//g&apos; | sed &apos;s/://g&apos;`let TOTAL_CORES=$[NUM_PHYS_CPU * NUM_CORES]## start install hwlocwget http://www.open-mpi.org/software/hwloc/v1.9/downloads/hwloc-1.9.tar.gztar -xf hwloc-1.9.tar.gzcd hwloc-1.9./configure --prefix=&quot;$BLDDIR&quot;make -j $NUM_CORESmake installexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$LIBDIR/pkgconfigexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$LIBDIRcd $BLDDIR## start install PLASMAexport OMP_NUM_THREADS=1export MKL_NUM_THREADS=1wget http://icl.cs.utk.edu/projectsfiles/plasma/pubs/plasma-installer_2.6.0.tar.gztar -xf plasma-installer_2.6.0.tar.gzcd plasma-installer_2.6.0./setup.py --prefix=&quot;$BLDDIR&quot; --cc=icc --fc=ifort \ --blaslib=&quot;-L/opt/intel/mkl/lib/lib64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot; \ --cflags=&quot;-O3 -fPIC -I$BLDDIR/include&quot; \ --fflags=&quot;-O3 -fPIC&quot; --noopt=&quot;-fPIC&quot; \ --ldflags_c=&quot;-I$BLDDIR/include&quot;\ --notesting --downlapc# compile shared librariescd $LIBDIRicc -shared -o libplasma.so -Wl,-whole-archive libplasma.a -Wl,-no-whole-archive -L. -lhwloc -llapackeicc -shared -o libcoreblas.so -Wl,-whole-archive libcoreblas.a -Wl,-no-whole-archive -L. -llapackeicc -shared -o libquark.so -Wl,-whole-archive libquark.a -Wl,-no-whole-archiveicc -shared -o libcoreblasqw.so -Wl,-whole-archive libcoreblasqw.a -Wl,-no-whole-archive -L. -llapackeexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$LIBDIR/pkgconfigexport PLASMA_NUM_THREADS=$TOTAL_CORESexport R_PLASMA_NUM_THREADS=$TOTAL_COREScd $BLDDIR## start install MGMGAexport CUDADIR=/usr/local/cuda-6.0export CUDALIB=$CUDADIR/lib64wget http://icl.cs.utk.edu/projectsfiles/magma/downloads/magma-1.4.1.tar.gztar -xf magma-1.4.1.tar.gzcd magma-1.4.1echo &quot;#include&lt;cuda.h&gt;#include&lt;cuda_runtime_api.h&gt;#include&lt;stdio.h&gt;int main() &#123; int deviceCount = 0; cudaError_t error_id = cudaGetDeviceCount(&amp;deviceCount); int dev, driverVersion = 0, runtimeVersion = 0; struct cudaDeviceProp deviceProp; int prev = 0; for (dev = 0; dev &lt; deviceCount; dev++) &#123; cudaGetDeviceProperties(&amp;deviceProp, dev); if(deviceProp.major &gt; prev) prev = deviceProp.major; &#125; if(prev &gt;= 2 &amp;&amp; prev &lt; 3) printf(\&quot;GPU_TARGET = Fermi\&quot;); else if(prev &gt;= 3) printf(\&quot;GPU_TARGET = Kepler\&quot;); else printf(\&quot;GPU_TARGET = Tesla\&quot;); return 0;&#125;&quot; &gt; getDevice.cexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDALIBgcc getDevice.c -I$CUDADIR/include -o getDevice -L$CUDALIB -lcudart./getDevice &gt; make.incecho &quot;CC = gccNVCC = nvccFORT = gfortranARCH = arARCHFLAGS = crRANLIB = ranlibOPTS = -fPIC -O3 -DADD_ -Wall -fno-strict-aliasing -fopenmp -DMAGMA_WITH_MKL -DMAGMA_SETAFFINITYF77OPTS = -fPIC -O3 -DADD_ -WallFOPTS = -fPIC -O3 -DADD_ -Wall -x f95-cpp-inputNVOPTS = -O3 -DADD_ -Xcompiler -fno-strict-aliasing,-fPICLDOPTS = -fPIC -fopenmpLIB = -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lpthread -lcublas -lcudart -lstdc++ -lm -liomp5 -lgfortran-include make.check-mkl-include make.check-cudaLIBDIR = -L$LIBDIR -L$MKLROOT/lib/intel64 -L$CUDADIR/lib64INC = -I$CUDADIR/include -I$MKLROOT/include&quot; &gt;&gt; make.incmake -j $NUM_CORES sharedcd libcp *.so $LIBDIRcd ../cp include/*.h $BLDDIR/include/cd $BLDDIR## install HiPLARMwget http://www.hiplar.org/downloads/HiPLARM_0.1.1.tar.gzR CMD INSTALL --configure-args=&quot;--with-lapack=-L$MKLROOT/lib/intel64 --with-plasma-lib=/home/clarence/Downloads/LALibs --with-cuda-home=/usr/local/cuda-6.0 --with-magma-lib=/home/clarence/Downloads/LALibs&quot; HiPLARM_0.1.1.tar.gz I have two troubles in compiling magma. First one is failure on compiling magma with icc and success with switching the compiler to gcc. Another trouble is the function lapack_const which is defined in both plasma and magma, so I can’t compile and I use the suggestion on MAGMA forum to disable the magma-with-plasma routines. To disable the magma-with-plasma routines, you need to comment the line PLASMA = ... in the file Makefile.internal like this: 12# Use Plasma to compile zgetfl and ztstrf# PLASMA = $(shell pkg-config --libs plasma 2&gt; /dev/null ) My environment is ubuntu 14.04. My CPU is 3770K@4.3GHz and GPU is GTX 670. If you have some questions, you can reference following urls:]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>GPU</tag>
        <tag>ubuntu</tag>
        <tag>cuda</tag>
        <tag>HiPLARM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R with GPU performance of gputools]]></title>
    <url>%2Fposts%2F201408%2F2014-08-03-R-with-GPU-performance-of-gputools.html</url>
    <content type="text"><![CDATA[I can’t wait to test the performance of R with GPU. I test the performance of gputools and OpenCL. The test code is showed in following: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151library(Rcpp)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#define ARMA_USE_BLAS#define ARMA_USE_ARPACK#define ARMA_USE_MKL_ALLOC#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]List fastLm_RcppArma_cpp_f(NumericVector yr, S4 Xr) &#123; uvec dim(as&lt;uvec&gt;(Xr.slot("Dim"))); NumericVector Xr_elem = Xr.slot("x"); mat X(Xr_elem.begin(), dim(0), dim(1), false); colvec y(yr.begin(), yr.size(), false); colvec coef = solve(X, y); colvec resid = y - X*coef; double sig2 = as_scalar(resid.t()*resid/(dim(0) - dim(1))); colvec stderrest = sqrt(sig2 * diagvec( inv(X.t()*X))); return List::create(Named("coefficients") = coef, Named("stderr") = stderrest);&#125;')sourceCpp(code = '// [[Rcpp::depends(RcppEigen)]]#define EIGEN_USE_MKL_ALL#include &lt;RcppEigen.h&gt;using namespace Rcpp;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;// [[Rcpp::export]]List fastLm_RcppEigen_cpp_f(NumericVector yr, NumericMatrix Xr) &#123; const Map&lt;MatrixXd&gt; X(as&lt;Map&lt;MatrixXd&gt; &gt;(Xr)); const Map&lt;VectorXd&gt; y(as&lt;Map&lt;VectorXd&gt; &gt;(yr)); int n = Xr.nrow(), k = Xr.ncol(); VectorXd coef = (X.transpose() * X).llt().solve(X.transpose() * y.col(0)); VectorXd resid = y - X*coef; double sig2 = resid.squaredNorm() / (n - k); VectorXd stderrest = (sig2 * ((X.transpose() * X).inverse()).diagonal()).array().sqrt(); return List::create(Named("coefficients") = coef, Named("stderr") = stderrest);&#125;')library(Matrix)library(MatrixModels)fastLm_RcppArma = function(formula, data)&#123; inputs = model.Matrix(formula, data, sparse = FALSE) output = data[,as.character(formula[[2]]), with = FALSE][[1]] return(fastLm_RcppArma_cpp_f(output, inputs))&#125;fastLm_RcppEigen = function(formula, data)&#123; inputs = model.matrix(formula, data) output = data[,as.character(formula[[2]]), with = FALSE][[1]] return(fastLm_RcppEigen_cpp_f(output, inputs))&#125;library(data.table)library(dplyr)N = 25000p = 1200X = as.matrix(Matrix(rnorm(N*p), ncol = p))Beta = (10**(sample(seq(-5, 1, length = N+p), p)))y = X %*% Beta + rnorm(N)dat = data.table(X)set(dat, j = "y", value = y)library(gputools)library(rbenchmark)benchmark(result_RcppArma = fastLm_RcppArma(y ~ ., data = dat), result_RcppEigen = fastLm_RcppEigen(y ~ ., data = dat), result_gpuLM = gpuLm(y ~ ., data = dat), result_Rlm = lm(y~., data = dat), columns=c("test", "replications","elapsed", "relative"), replications=10, order="relative")# test replications elapsed relative# 3 result_gpuLM 10 35.137 1.000# 2 result_RcppEigen 10 37.511 1.068# 1 result_RcppArma 10 40.170 1.143# 4 result_Rlm 10 146.959 4.182sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#define ARMA_USE_BLAS#define ARMA_USE_ARPACK#define ARMA_USE_MKL_ALLOC#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]NumericMatrix RcppMatMult_Arma(NumericMatrix Xr, NumericMatrix Yr) &#123; int m = Xr.nrow(), n = Xr.ncol(), k = Yr.ncol(); if(n != Yr.ncol()) exit(1); mat X(Xr.begin(), m, n, false); mat Y(Xr.begin(), n, k, false); return wrap(X*Y);&#125;')sourceCpp(code = '// [[Rcpp::depends(RcppEigen)]]#define EIGEN_USE_MKL_ALL#include &lt;RcppEigen.h&gt;using namespace Rcpp;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;// [[Rcpp::export]]List RcppMatMult_Eigen(NumericMatrix Xr, NumericMatrix Yr) &#123; const Map&lt;MatrixXd&gt; X(as&lt;Map&lt;MatrixXd&gt; &gt;(Xr)); const Map&lt;MatrixXd&gt; Y(as&lt;Map&lt;MatrixXd&gt; &gt;(Yr)); return wrap(X*Y);&#125;')m = 10000n = 5000k = 10000matA = matrix(runif(m*n), m)matB = matrix(runif(n*k), n)s = proc.time()a = matA %*% matBproc.time() - s# user system elapsed# 57.513 0.330 15.525s = proc.time()a = cpuMatMult(matA, matB) # same as matA %*% matBproc.time() - s# user system elapsed# 56.287 0.252 14.573s = proc.time()a = gpuMatMult(matA, matB)proc.time() - s# user system elapsed# 6.587 2.381 8.958s = proc.time()a = RcppMatMult_Arma(matA, matB)proc.time() - s# same BLAS, so the result is the same.# user system elapsed# 56.763 0.308 14.876s = proc.time()a = RcppMatMult_Eigen(matA, matB)proc.time() - s# same BLAS, so the result is the same.# user system elapsed# 123.486 1.766 88.519 R with GPU is 2 times faster than R with CPU!! My environment is ubuntu 14.04. My CPU is 3770K@4.3GHz and GPU is GTX 670. If you have some questions, you can reference following urls:]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>GPU</tag>
        <tag>ubuntu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R with GPU]]></title>
    <url>%2Fposts%2F201408%2F2014-08-03-R-with-GPU.html</url>
    <content type="text"><![CDATA[The GPU computing is very popular since its capability of parallel computing. The environment of ubuntu is adequate to try GPU computing and its setting is easier than in window. I introduce the installation of CUDA, nVidia driver and R packages of GPU. First, we go over the installation of CUDA 6.0. Download the CUDA 6.0 installer: CUDA Download. In my case, I download the RUN file of Ubuntu 13.04. Remove the old drive of nVidia: 12sudo apt-get purge nvidiasudo apt-get autoremove nvidia-current Install compatible GCC compiler. If the version of GCC compiler is not below 4.7.3, please install GCC 4.7.3. You can do that in this way: 123456sudo add-apt-repository ppa:ubuntu-toolchain-r/testsudo apt-get updatesudo apt-get install gcc-4.7 g++-4.7sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 50sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.7 10sudo update-alternatives --config gcc Choose gcc-4.7 for installing CUDA. Install CUDA 6.0: Press alt+ctrl+F1 login tty1 and input following commands: 12sudo stop lightdmsudo bash cuda_6.0.37_linux_64.run -driver -toolkit -silent At begin, I do not install the driver, then it cause I can’t implement the compiled program. Add some environment variables: Use your text editor to edit .bashrc (In my case, it is subl ~/.bashrc.) and add following lines into the files: (Notice whether your installation dictionary is the same as mine.) 1234567export PATH=/usr/local/cuda-6.0/bin:$PATHexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-6.0/lib:/usr/local/cuda-6.0/lib64export CUDA_INC_PATH=/usr/local/cuda-6.0/includeexport CUDA_LIB_PATH=/usr/local/cuda-6.0/lib64export CUDA_BIN=/usr/local/cuda-6.0/binexport CUDA_HOME=/usr/local/cuda-6.0export R_INC_PATH=/usr/local/lib/R/include Then you can use CUDA and start to install R package gputools. Besides, if you want opencl, then you need to install opencl-headers (use apt-get) and you can install R package OpenCL. However, I did not install R package WideLM successfully. (Caused by unknown option Wl) The R package rpud is updated, but it did not update in CRAN. You need go to its website to download. Follow its installation note, you can install it successfully. Next time I will try to install the R package HiPLARM. My environment is ubuntu 14.04. My CPU is 3770K@4.3GHz and GPU is GTX 670. If you have some questions, you can reference following urls: R and GPUh Installing CUDA Toolkit 6.0 in Ubuntu 12.04 Linux How do I install gcc 4.7? R gputools rpud]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>GPU</tag>
        <tag>ubuntu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Search Maximum Point by Point]]></title>
    <url>%2Fposts%2F201408%2F2014-08-02-Search-Maximum-Point-by-Point.html</url>
    <content type="text"><![CDATA[We have an image data, it is discrete and I want to find the local maximum. I write a Rcpp function for searching maximum point by point. I have two strategies to search, one is in circle, another one is in square. Besides, I add a radius parameter to control the search region. Figure 1 shows that there are 34 by 34 points which each point contains a value of height, it searches maximum point by point with an unit circle. Figure 2, 3 and 4 show that searching maximum in circle with radius 2, square with radius 1 and square with radius 2, respectively. Figure 1: Searching in circle with radius 1Figure 2: Searching in circle with radius 2Figure 3: Searching in square with radius 1Figure 4: Searching in square with radius 2 I have three versions for searching maximum point by point. First one rely on Rcpp function completely, second one use sapply and Rcpp function which searching one point and third one use parallel and R function. Code is present as following: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138x.range.v = 1:300y.range.v = 1:300data.m = expand.grid(x.range.v, y.range.v)data.m[,3] = rnorm(nrow(data.m))# square searchsq_search_f = function(i, data.m, loc.v, obj, mar)&#123; x = data.m[i,loc.v[1]]; y = data.m[i,loc.v[2]] near_int = data.m[abs(data.m[,loc.v[1]] - x) &lt;= mar &amp; abs(data.m[,loc.v[2]] - y) &lt;= mar,obj] if( length(near_int) &gt;= 1)&#123; all(near_int &lt;= data.m[i,obj]) &#125; else&#123; FALSE &#125;&#125;library(Rcpp)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]uword sq_search_f_cpp_i_f(int i, NumericVector int_mr, NumericVector Xr, NumericVector Yr, double mar) &#123; int n = Xr.size(); colvec X(Xr.begin(), n, false), Y(Yr.begin(), n, false), int_m(int_mr.begin(), n, false); double x_loc = X( (uword) i), y_loc = Y( (uword) i); colvec temp_y = Y(find(abs(X - x_loc) &lt;= mar)), temp_int = int_m(find(abs(X - x_loc) &lt;= mar)), int_near = temp_int(find(abs(temp_y - y_loc) &lt;= mar)); uword value = (uword) all(int_near &lt;= int_m((uword) i)); return value;&#125;// [[Rcpp::export]]IntegerVector sq_search_cpp_f(NumericVector int_mr, NumericVector Xr, NumericVector Yr, double mar) &#123; int n = Xr.size(); ucolvec location_v(n); for(int i = 0; i &lt; n; i++) location_v(i) = sq_search_f_cpp_i_f(i, int_mr, Xr, Yr, mar); return wrap(location_v);&#125;')mar = 1library(snowfall)start.time = Sys.time()loc1 = sq_search_cpp_f(data.m[,3], data.m[,1], data.m[,2], mar)Sys.time() - start.time# Time difference of 20.32596 secsstart.time = Sys.time()loc2 = sapply(1:nrow(data.m), function(i) sq_search_f_cpp_i_f(i-1, data.m[,3], data.m[,1], data.m[,2], mar))Sys.time() - start.time# Time difference of 1.229692 minssfInit(parallel = TRUE, cpus = 8)sfExport("data.m", "sq_search_f", "mar")start.time = Sys.time()loc3 = sfSapply(1:nrow(data.m), function(i) sq_search_f(i, data.m, c(1,2), 3, mar))Sys.time() - start.timesfStop()# Time difference of 1.530629 minsall.equal(as.vector(loc1), loc2)# TRUEall.equal(as.vector(loc1), as.numeric(loc3))# TRUE# circle searchcr_search_f = function(i, data.m, loc.v, obj, mar)&#123; x = data.m[i,loc.v[1]]; y = data.m[i,loc.v[2]] near_int = data.m[(data.m[,loc.v[1]] - x)^2+ (data.m[,loc.v[2]] - y)^2 &lt;= mar,obj] if( length(near_int) &gt;= 1)&#123; all(near_int &lt;= data.m[i,obj]) &#125; else&#123; FALSE &#125;&#125;library(Rcpp)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]uword cr_search_f_cpp_i_f(int i, NumericVector int_mr, NumericVector Xr, NumericVector Yr, double mar) &#123; int max_threads_mkl = mkl_get_max_threads(); mkl_set_num_threads(max_threads_mkl); int n = Xr.size(); colvec X(Xr.begin(), n, false), Y(Yr.begin(), n, false), int_m(int_mr.begin(), n, false); double x_loc = X( (uword) i), y_loc = Y( (uword) i); colvec int_near = int_m(find(square(X - x_loc) + square(Y - y_loc) &lt;= mar)); uword value = (uword) all(int_near &lt;= int_m((uword) i)); return value;&#125;// [[Rcpp::export]]IntegerVector cr_search_cpp_f(NumericVector int_mr, NumericVector Xr, NumericVector Yr, double mar) &#123; int n = Xr.size(); ucolvec location_v(n); for(int i = 0; i &lt; n; i++) location_v(i) = cr_search_f_cpp_i_f(i, int_mr, Xr, Yr, mar); return wrap(location_v);&#125;')mar = 1library(snowfall)start.time = Sys.time()loc1 = cr_search_cpp_f(data.m[,3], data.m[,1], data.m[,2], mar)Sys.time() - start.time# Time difference of 19.21388 secsstart.time = Sys.time()loc2 = sapply(1:nrow(data.m), function(i) cr_search_f_cpp_i_f(i-1, data.m[,3], data.m[,1], data.m[,2], mar))Sys.time() - start.time# Time difference of 47.72535 secssfInit(parallel = TRUE, cpus = 8)sfExport("data.m", "cr_search_f", "mar")start.time = Sys.time()loc3 = sfSapply(1:nrow(data.m), function(i) cr_search_f(i, data.m, c(1,2), 3, mar))Sys.time() - start.timesfStop()# Time difference of 1.055895 minsall.equal(as.vector(loc1), loc2)# TRUEall.equal(as.vector(loc1), as.numeric(loc3))# TRUE The above result shows that sapply is not fast enough, write Rcpp function if you can! The speed of paralleled R function (8 threads) is close to Rcpp function with sapply (one threads), the for loop in R is so slow. My environment is ubuntu 14.04, R 3.1.1 compiled by Intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Attributes for Sparse Matrix]]></title>
    <url>%2Fposts%2F201407%2F2014-07-30-Rcpp-Attributes-for-Sparse-Matrix.html</url>
    <content type="text"><![CDATA[Thanks to the R package Matrix, we can use the sparse matrix in R. But to use the sparse matrix in the Rcpp, we need create a S4 object to get sparse matrix in Rcpp. However, RcppArmadillo provides a convenient API for interfacing the armadillo class SpMat and R class dgCMatrix. I provide several methods to use the sparse matrix in R. Code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364library(Rcpp)library(Matrix)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(Matrix, RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]mat SparseDenseMatrixMulti(sp_mat m, mat n) &#123; mat R = m* n; return R;&#125;// [[Rcpp::export]]mat SparseDenseMatrixMulti2(sp_mat m, S4 n) &#123; uvec dim(as&lt;uvec&gt;(n.slot("Dim"))); NumericVector n_elem = n.slot("x"); mat n_mat(n_elem.begin(), dim(0), dim(1), false); mat R = m * n_mat; return R;&#125;// [[Rcpp::export]]mat SparseDenseMatrixMulti3(sp_mat m, NumericMatrix n) &#123; mat n_mat(n.begin(), n.nrow(), n.ncol(), false); mat R = m* n_mat; return R;&#125;// [[Rcpp::export]]mat SparseDenseMatrixMulti4(S4 m, S4 n) &#123; uvec i(as&lt;uvec&gt;(m.slot("i"))), p(as&lt;uvec&gt;(m.slot("p"))), dim_m(as&lt;uvec&gt;(m.slot("Dim"))), dim_n(as&lt;uvec&gt;(n.slot("Dim"))); NumericVector n_elem = n.slot("x"); colvec m_elem = as&lt;colvec&gt;(m.slot("x")); sp_mat m_mat(i, p, m_elem, dim_m(0), dim_m(1)); mat n_mat(n_elem.begin(), dim_n(0), dim_n(1), false); mat R = m_mat * n_mat; return R;&#125;')i = c(1:196,3:800, 1005:2107, 5009:7011, 2001:8000)j = c(2:200, 9:160, 11:9700, 9942:10000)x = runif(length(i))A = sparseMatrix(i, j, x = x)B_S4 = Matrix(rnorm(3e7), 10000)B = as.matrix(B_S4)library(rbenchmark)benchmark(CC1 = SparseDenseMatrixMulti(A, B), CC2 = SparseDenseMatrixMulti2(A, B_S4), CC3 = SparseDenseMatrixMulti3(A, B), CC4 = SparseDenseMatrixMulti4(A, B_S4), columns=c("test", "replications","elapsed", "relative"), replications=20, order="relative")# test replications elapsed relative# 4 CC4 20 5.892 1.000# 3 CC3 20 5.922 1.005# 2 CC2 20 5.933 1.007# 1 CC1 20 6.820 1.158 The above result show that if you use the interface of RcppArmadillo or Rcpp, then you need suffer from delay of memory copy. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>Rcpp Attributes</tag>
        <tag>Sparse Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R data.table - melt and cast]]></title>
    <url>%2Fposts%2F201407%2F2014-07-29-R-data-table-melt-and-cast.html</url>
    <content type="text"><![CDATA[data.table is a powerful tool for exploring data.Here we provides a example for melting and casting data. Code: 12345678910111213141516171819202122232425262728293031323334353637library(data.table)library(reshape2)library(dplyr)dat = list(); length(dat) = 3dat[[1]] = data.table(name=c("A","B","D"),value=c(23,45,100),day="day1")dat[[2]] = data.table(name=c("A","C","D"),value=c(77,11,35),day="day2")dat[[3]] = data.table(name=c("B","D","E"),value=c(11,44,55),day="day3")dat2 = rbindlist(dat)# name value day# 1: A 23 day1# 2: B 45 day1# 3: D 100 day1# 4: A 77 day2# 5: C 11 day2# 6: D 35 day2# 7: B 11 day3# 8: D 44 day3# 9: E 55 day3dat3 = dcast.data.table(dat2, name ~ day)# name day1 day2 day3# 1: A 23 77 NA# 2: B 45 NA 11# 3: C NA 11 NA# 4: D 100 35 44# 5: E NA NA 55dat4 = filter(melt(dat3, "name"), !is.na(value))# name variable value# 1: A day1 23# 2: B day1 45# 3: D day1 100# 4: A day2 77# 5: C day2 11# 6: D day2 35# 7: B day3 11# 8: D day3 44# 9: E day3 55 cast can change raw data into a wide table and melt can change data in wide table format back to raw data. I think that it is faster and easier than other functions on data.frame. (Some functions like reshape, cast and melt in reshape.)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R data.table - collating data]]></title>
    <url>%2Fposts%2F201407%2F2014-07-29-R-data-table-collating-data.html</url>
    <content type="text"><![CDATA[data.table is a powerful tool for collating data. Here we provides a example. Recently, I join a team and participate a competition of R data analysis. It is an estate data which contains housing price and some variables. I put data in my [Google drive].(https://drive.google.com/open?id=0B1UBN4lCLHrVeWRzQ1FHQUoyem8). Code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138library(data.table)library(dplyr)dat = lapply(paste0("List_", LETTERS[LETTERS!="L" &amp; LETTERS!="R" &amp; LETTERS!="S" &amp; LETTERS!="Y"], ".csv"), read.csv)dat2 = rbindlist(dat)region_v = c("臺北市","臺中市","基隆市","臺南市","高雄市","新北市","宜蘭縣","桃園縣","嘉義市","新竹縣","苗栗縣","南投縣","彰化縣","新竹市","雲林縣","嘉義縣","屏東縣","花蓮縣","臺東縣","金門縣","澎湖縣","連江縣")region = unlist(lapply(1:length(dat), function(i) rep(region_v[i], nrow(dat[[i]]))))dat2 = data.table(dat2, region = region)cols = c(1:3, 5:7, 9:15, 20:21, 24)dat2[, (cols):=lapply(.SD, as.character),.SDcols=cols]setnames(dat2, "交易標的", "trading_target")dat2 = filter(dat2, grepl("建物", trading_target))setnames(dat2, "交易年月", "trading_ym")dat2 = mutate(dat2, year.trading = substr(trading_ym, 0, nchar(trading_ym)-2))dat2 = mutate(dat2, month.trading = substr(trading_ym, nchar(as.character(trading_ym))-1, nchar(trading_ym)))set(dat2, i = which(dat2$month.trading &gt;= 13), j = "month.trading", value = NA)set(dat2, i = which(dat2$month.trading == 0), j = "month.trading", value = NA)set(dat2, i = which(is.na(dat2$month.trading)), j = "month.trading", value = "6")set(dat2, j = "year.trading", value = as.integer(dat2$year.trading))set(dat2, j = "month.trading", value = as.integer(dat2$month.trading))setnames(dat2, "交易筆棟數", "trading_detail")dat2 = mutate(dat2, n.land = as.integer(substr(trading_detail,3,3)))dat2 = mutate(dat2, n.building = as.integer(substr(trading_detail,6,6)))dat2 = mutate(dat2, n.parking_lot = as.integer(substr(trading_detail,9,9)))setnames(dat2, "主要建材", "building_materials")dat2 = mutate(dat2, building_materials = as.character(building_materials))set(dat2, i = which(dat2$building_materials==""), j = "building_materials", value = NA)dat2 = mutate(dat2, CRC = grepl("混凝土", building_materials))setnames(dat2, "建築完成年月", "construction_ym")dat2 = mutate(dat2, year.construction = as.numeric(rep(NA, nrow(dat2))))dat2 = mutate(dat2, month.construction = as.numeric(rep(NA, nrow(dat2))))dat2[nchar(construction_ym)==2 &amp; !is.na(construction_ym), year.construction := as.integer(construction_ym)]dat2[nchar(construction_ym)==4 &amp; substr(construction_ym, 1, 2) != 10 &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 2))]dat2[nchar(construction_ym)==4 &amp; substr(construction_ym, 1, 2) != 10 &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 3, 4))]dat2[nchar(construction_ym)==4 &amp; substr(construction_ym, 1, 2) == 10 &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 3))]dat2[nchar(construction_ym)==4 &amp; substr(construction_ym, 1, 2) == 10 &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 4, 4))]dat2[nchar(construction_ym)==5 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 3))]dat2[nchar(construction_ym)==5 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 4, 5))]dat2[nchar(construction_ym)==5 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 2))]dat2[nchar(construction_ym)==5 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 3, 3))]dat2[nchar(construction_ym)==6 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 3))]dat2[nchar(construction_ym)==6 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 4, 5))]dat2[nchar(construction_ym)==6 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 2))]dat2[nchar(construction_ym)==6 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 3, 4))]dat2[nchar(construction_ym)==7 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 3))]dat2[nchar(construction_ym)==7 &amp; (substr(dat2$construction_ym, 1, 1) == 1 | substr(dat2$construction_ym, 1, 1) == 0) &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 4, 5))]dat2[nchar(construction_ym)==7 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), year.construction := as.integer(substr(construction_ym, 1, 2))]dat2[nchar(construction_ym)==7 &amp; substr(dat2$construction_ym, 1, 1) != 1 &amp; substr(dat2$construction_ym, 1, 1) != 0 &amp; !is.na(construction_ym), month.construction := as.integer(substr(construction_ym, 4, 5))]set(dat2, i = which(dat2$year.construction &gt;= 104), j = "year.construction", value = NA)set(dat2, i = which(dat2$month.construction &gt;= 13), j = "month.construction", value = NA)set(dat2, i = which(dat2$month.construction == 0), j = "month.construction", value = NA)set(dat2, i = which(is.na(dat2$month.construction)), j = "month.construction", value = 6)dat2 = mutate(dat2, trading = year.trading *12 + month.trading)dat2 = mutate(dat2, construction = year.construction *12 + month.construction)dat2 = mutate(dat2, age_house = trading - construction)### outputdat3 = as.data.frame(select(dat2, c(23,29,4,5,39,11,12,40,16:21,25,32:35)))dat4 = dat3[setdiff(1:nrow(dat3), which(is.na(dat3), arr.ind = TRUE)[,1]),]names(dat4) = c("Y", paste0("V", 1:(ncol(dat4)-1)))dat4$V1 = as.factor(dat4$V1)dat4$V3 = as.factor(dat4$V3)dat4$V5 = as.numeric(dat4$V5)dat4$V6 = as.factor(dat4$V6)dat4$V12 = as.factor(dat4$V12)dat4$V13 = as.factor(dat4$V13)dat4$V18 = as.factor(dat4$V18)dat4 = dat4[dat4$Y!=0,]dat4 = dat4[setdiff(1:nrow(dat4), which(is.na(dat4), arr.ind = TRUE)[,1]),]### modelinglm.fit &lt;- lm(log(Y+0.1)~.,data=dat4)summary(lm.fit)dat4.sub&lt;-dat4[-which(rownames(dat4)%in%c(323747, 80936, 222834)),]lm.fit &lt;- lm(log(Y+0.1)~.,data=dat4.sub)summary(lm.fit)library(MASS)lm.AIC &lt;- stepAIC(lm.fit)fold = 10cv_index_f = function(n, fold = 10)&#123; fold_n = floor(n / fold) rem = n - fold_n * fold size = rep(fold_n, fold) if(rem &gt; 0) size[1:rem] = fold_n + 1 cv_index = unlist(sapply(1:fold, function(i) rep(i, size[i]))) cv_index = sample(cv_index, length(cv_index)) return(cv_index)&#125;index = cv_index_f(nrow(dat4.sub), fold)library(snowfall)n_rep = 10sfInit(TRUE, 8)sfExport("dat4.sub")pred.error = sapply(1:n_rep , function(i)&#123; index = cv_index_f(nrow(dat4.sub), fold) sfExport("index") lm.CV &lt;- sfSapply(1:fold, function(v)&#123; dat4.train = dat4.sub[index != v,] dat4.test = dat4.sub[index == v,] lm.fit.train = lm(log(Y+0.1)~.,data = dat4.train) sum((log(dat4.test$Y+0.1) - predict(lm.fit.train, dat4.test))^2)/nrow(dat4.test) &#125;) mean(lm.CV)&#125;)sfStop()mean(pred.error)# 0.1912725pred.error# [1] 0.1913776 0.1912849 0.1912692 0.1913275 0.1914682 0.1914266 0.1911751 0.1911378 0.1911330 0.1911254library(grpreg)X.m &lt;- model.matrix(log(Y+0.1)~. , data=dat4.sub)[,-1]group.v &lt;- substring(colnames(X.m),1,2)group.v[41:49] &lt;- substring(colnames(X.m),1,3)[41:49]group.v &lt;- as.numeric(factor(group.v, levels=paste("V",c(1:18),sep="")))out.glasso &lt;- cv.grpreg(X.m, log(dat4.sub$Y+0.1), group=group.v)plot(out.glasso)out.glasso$lambda.min# 0.0001378642out.glasso$cve[out.glasso$min]# 0.1911332coef(out.glasso) Thanks to this competition, I have an interesting data to practice the use of data.table. In addition, data.table and dplyr are so fast that I drop data.frame. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R data.table - sum by groups]]></title>
    <url>%2Fposts%2F201407%2F2014-07-29-R-data-table-sum-by-groups.html</url>
    <content type="text"><![CDATA[data.table is a powerful tool for exploring data. However, how is it fast? Here we provides a performance test for summing by groups. Code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253N = 1e4x = data.frame(Freq=runif(N,0,1),Category=c("T","F"))library(data.table)library(plyr)library(dplyr)x_dt = data.table(x)setkey(x_dt, Category)Cate_group_dt = group_by(x_dt, Category)Cate_group_df = group_by(x_dt, Category)library(rbenchmark)benchmark(data_table = x_dt[, sum(Freq),by = Category], tapply = tapply(x$Freq, x$Category, FUN=sum), plyr_dt = aggregate(Freq ~ Category, data = x_dt, FUN=sum), plyr_dt2 = ddply(x_dt, .(Category), colwise(sum)), plyr_df = aggregate(Freq ~ Category, data = x, FUN=sum), plyr_df2 = ddply(x, .(Category), colwise(sum)), dplyr_dt = summarise(Cate_group_dt, sum(Freq)), dplyr_df = summarise(Cate_group_df, sum(Freq)), replications = 20, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')# Result for N = 1e4:# test replications elapsed relative user.self# 2 tapply 100 0.082 1.000 0.082# 1 data_table 100 0.143 1.744 0.142# 8 dplyr_df 100 0.186 2.268 0.186# 7 dplyr_dt 100 0.188 2.293 0.187# 6 plyr_df2 100 0.312 3.805 0.312# 4 plyr_dt2 100 0.320 3.902 0.320# 3 plyr_dt 100 2.738 33.390 2.739# 5 plyr_df 100 3.887 47.402 3.888# Result for N = 1e6:# test replications elapsed relative user.self# 1 data_table 20 0.613 1.000 0.608# 7 dplyr_dt 20 0.631 1.029 0.626# 8 dplyr_df 20 0.636 1.038 0.630# 2 tapply 20 1.179 1.923 1.168# 6 plyr_df2 20 1.445 2.357 1.419# 4 plyr_dt2 20 1.475 2.406 1.451# 3 plyr_dt 20 66.165 107.936 66.162# 5 plyr_df 20 98.394 160.512 98.416# Result for N = 1e7:# test replications elapsed relative user.self# 6 dplyr_df 20 5.939 1.000 5.823# 5 dplyr_dt 20 5.954 1.003 5.840# 1 data_table 20 5.980 1.007 5.847# 2 tapply 20 12.080 2.034 11.649# 3 plyr_dt2 20 14.759 2.485 13.813# 4 plyr_df2 20 14.857 2.502 13.809 In the case with small sample size, tapply is the most efficient tool for summing by groups. In the case with large sample size, data.table and summarise in dplyr are more efficient. Next, we benchmark the performance of summing by two groups. Code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748N = 1e4set.seed(100)x &lt;- data.frame(Freq=runif(N,0,1),Category=c("T","F"),Category2=sample(c("T","F"), N, replace = TRUE))library(data.table)library(plyr)library(dplyr)x_dt = data.table(x)setkey(x_dt, Category, Category2)Cate_group_dt = group_by(x_dt, Category, Category2)Cate_group_df = group_by(x_dt, Category, Category2)library(rbenchmark)benchmark(data_table = x_dt[, sum(Freq),by = key(x_dt)], tapply = tapply(x$Freq, list(x$Category, x$Category2), FUN=sum), plyr_dt2 = ddply(x_dt, .(Category, Category2), colwise(sum)), plyr_df2 = ddply(x, .(Category, Category2), colwise(sum)), dplyr_dt = summarise(Cate_group_dt, sum(Freq)), dplyr_df = summarise(Cate_group_df, sum(Freq)), replications = 100, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')# Result for N = 1e4:# test replications elapsed relative user.self# 2 tapply 100 0.093 1.000 0.359# 1 data_table 100 0.161 1.731 0.642# 5 dplyr_dt 100 0.217 2.333 0.833# 6 dplyr_df 100 0.219 2.355 0.219# 4 plyr_df2 100 0.487 5.237 1.870# 3 plyr_dt2 100 0.498 5.355 1.914# Result for N = 1e6:# test replications elapsed relative user.self# 1 data_table 20 0.476 1.000 0.960# 6 dplyr_df 20 0.491 1.032 0.491# 5 dplyr_dt 20 0.502 1.055 1.137# 2 tapply 20 1.416 2.975 1.417# 3 plyr_dt2 20 3.285 6.901 12.437# 4 plyr_df2 20 3.292 6.916 12.673# Result for N = 1e7:# test replications elapsed relative user.self# 1 data_table 20 4.472 1.000 4.427# 5 dplyr_dt 20 4.520 1.011 4.454# 6 dplyr_df 20 4.526 1.012 4.460# 2 tapply 20 14.732 3.294 14.245# 4 plyr_df2 20 31.481 7.040 48.578# 3 plyr_dt2 20 31.625 7.072 48.127 In the case of summing by two groups, data.table is much more efficient in large size. We also try a Rcpp in summing by groups. Code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263library(Rcpp)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]NumericVector group_sum_f(NumericVector Xr, IntegerVector Groupr) &#123; int n = Xr.size(); colvec X(Xr.begin(), n, false); ivec Group(Groupr.begin(), n, false); ivec Group_unique = unique(Group); int k = Group_unique.size(); colvec result(k); for(int i = 0; i &lt; k; i++) result(i) = sum(X.elem(find(Group == Group_unique(i)))); return wrap(result);&#125;')N = 1e4; k = 2x = data.frame(Freq=runif(N,0,1),Category=sample(LETTERS[1:k], N, replace = TRUE))library(data.table)library(dplyr)x_dt = data.table(x)setkey(x_dt, Category)Cate_group_dt = group_by(x_dt, Category)Cate_group_df = group_by(x_dt, Category)library(rbenchmark)benchmark(data_table = x_dt[, sum(Freq),by = Category], tapply = tapply(x$Freq, x$Category, FUN=sum), Rcpp = group_sum_f(x$Freq, x$Category), dplyr_dt = summarise(Cate_group_dt, sum(Freq)), dplyr_df = summarise(Cate_group_df, sum(Freq)), replications = 100, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')# Result for N = 1e4:# test replications elapsed relative user.self# 3 Rcpp 20 0.005 1.0 0.005# 2 tapply 20 0.016 3.2 0.016# 1 data_table 20 0.027 5.4 0.027# 5 dplyr_df 20 0.035 7.0 0.035# 4 dplyr_dt 20 0.037 7.4 0.037# Result for N = 1e6:# test replications elapsed relative user.self# 3 Rcpp 20 0.502 1.000 0.502# 1 data_table 20 0.616 1.227 0.616# 5 dplyr_df 20 0.620 1.235 0.620# 4 dplyr_dt 20 0.628 1.251 0.628# 2 tapply 20 1.141 2.273 1.142# Result for N = 1e7:# # test replications elapsed relative user.self# 3 Rcpp 20 5.449 1.000 5.365# 4 dplyr_dt 20 5.883 1.080 5.789# 5 dplyr_df 20 5.916 1.086 5.821# 1 data_table 20 5.967 1.095 5.841# 2 tapply 20 12.030 2.208 11.603 We can see that data.table is compatible with Rcpp and more convenient than Rcpp. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R data.table - subsets]]></title>
    <url>%2Fposts%2F201407%2F2014-07-29-R-data-table-subsets.html</url>
    <content type="text"><![CDATA[data.table is a powerful tool for exploring data. However, how is it fast? Here we provides a performance test for subsetting data. Code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798library(data.table)library(dplyr)library(fastmatch)library(Rcpp)library(rbenchmark)perf_test = function(N)&#123; tmp &lt;- list() for(i in 1:N) tmp[[i]] &lt;- iris m &lt;- do.call(rbind, tmp) m2 = data.table(m) setkey(m2, "Sepal.Width") m3 = as.matrix(m[,1:4]) benchmark(replications=100, order = "relative", data.frame = m[m$Sepal.Width == 3.5,], subset = subset(m, Sepal.Width == 3.5), dt1 = m2[J(3.5)], filter_dt = filter(m, Sepal.Width == 3.5), filter_df = filter(m2, Sepal.Width == 3.5), dt2 = m2[list(3.5)], fmatch = m2[fmatch(m2$Sepal.Width, 3.5, nomatch = 0L),], matrix = m3[m3[,2]==3.5,], columns = c("test", "replications", "elapsed", "relative") )&#125;# iris的大小object.size(iris)# 7088 bytes# 200倍的資料量perf_test(200)# test replications elapsed relative# 4 filter_dt 100 0.038 1.000# 5 filter_df 100 0.088 2.316# 6 dt2 100 0.119 3.132# 3 dt1 100 0.131 3.447# 8 matrix 100 0.134 3.526# 7 fmatch 100 0.222 5.842# 1 data.frame 100 0.407 10.711# 2 subset 100 0.490 12.895# 500倍的資料量perf_test(500)# test replications elapsed relative# 4 filter_dt 100 0.083 1.000# 5 filter_df 100 0.119 1.434# 6 dt2 100 0.126 1.518# 3 dt1 100 0.127 1.530# 8 matrix 100 0.371 4.470# 7 fmatch 100 0.517 6.229# 1 data.frame 100 1.056 12.723# 2 subset 100 1.224 14.747# 1000倍的資料量perf_test(1000)# test replications elapsed relative# 3 dt1 100 0.136 1.000# 6 dt2 100 0.139 1.022# 4 filter_dt 100 0.159 1.169# 5 filter_df 100 0.194 1.426# 8 matrix 100 0.809 5.949# 7 fmatch 100 1.128 8.294# 1 data.frame 100 2.157 15.860# 2 subset 100 2.541 18.684# 1500倍的資料量perf_test(1500)# test replications elapsed relative# 3 dt1 100 0.144 1.000# 6 dt2 100 0.148 1.028# 4 filter_dt 100 0.259 1.799# 5 filter_df 100 0.287 1.993# 8 matrix 100 1.204 8.361# 7 fmatch 100 1.543 10.715# 1 data.frame 100 3.242 22.514# 2 subset 100 3.729 25.896# 3000倍的資料量perf_test(3000)# test replications elapsed relative# 3 dt1 100 0.174 1.000# 6 dt2 100 0.174 1.000# 5 filter_df 100 0.405 2.328# 4 filter_dt 100 0.509 2.925# 8 matrix 100 2.441 14.029# 7 fmatch 100 2.993 17.201# 1 data.frame 100 6.428 36.943# 2 subset 100 7.458 42.862# 5000倍的資料量perf_test(5000)# test replications elapsed relative# 6 dt2 100 0.224 1.000# 3 dt1 100 0.225 1.004# 5 filter_df 100 0.632 2.821# 4 filter_dt 100 0.869 3.879# 8 matrix 100 4.027 17.978# 7 fmatch 100 4.797 21.415# 1 data.frame 100 10.578 47.223# 2 subset 100 12.177 54.362 After above benchmarks, we can see that filter in dplyr is fast when data size is low (lower than 10 MB), but data.table searching by key is faster when data size is larger. Fastmatch is not fast. HAHA!! Even data.frame is slower than matrix. data.table is so worth to learn! My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>data.table</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R Performance for calculating the maximum of two normal random variable]]></title>
    <url>%2Fposts%2F201407%2F2014-07-22-R-performance.html</url>
    <content type="text"><![CDATA[There is a performance test between R functions, R functions with byte compiler, Rcpp and RcppArmadillo. It is about the generating the normal random variables and the vectorized computations. Code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223# 1a = function()&#123; sum = 0 nreps = 1e7 for(i in 1:nreps)&#123; xy = rnorm(2) sum = sum + max(xy) &#125; print(sum/nreps)&#125;# 2b = function()&#123; nreps = 1e7 xymat = matrix(rnorm(2*nreps),nreps) maxs = pmax(xymat[,1],xymat[,2]) print(mean(maxs))&#125;# 3d = function()&#123; nreps = 1e7 print(mean(sapply(1:nreps, function(i) max(rnorm(2)))))&#125;# 4e = function()&#123; nreps = 1e7 xymat = matrix(rnorm(2*nreps),nreps) maxs = apply(xymat, 1, max) print(mean(maxs))&#125;# 5f = function()&#123; nreps = 1e7 print(Reduce('+', lapply(1:nreps, function(i) max(rnorm(2)))) / nreps)&#125;# 6g = function()&#123; nreps = 1e7 library(snowfall) sfInit(TRUE, 8) sfExport('nreps') maxs = sfLapply(1:nreps, function(i) max(rnorm(2))) sfStop() print(Reduce('+', maxs) / nreps)&#125;# cmpfunlibrary(compiler)a_cmp = cmpfun(a)b_cmp = cmpfun(b)d_cmp = cmpfun(d)e_cmp = cmpfun(e)f_cmp = cmpfun(f)# Rcpplibrary(Rcpp)## For windows user# library(inline)# settings &lt;- getPlugin("Rcpp")# settings$env$PKG_CXXFLAGS &lt;- paste('-fopenmp', settings$env$PKG_CXXFLAGS)# settings$env$PKG_LIBS &lt;- paste('-fopenmp -lgomp', settings$env$PKG_LIBS)# do.call(Sys.setenv, settings$env)sourceCpp(code = '#include &lt;Rcpp.h&gt;#include &lt;omp.h&gt;using namespace Rcpp;// [[Rcpp::export]]double mm(NumericVector x, NumericVector y) &#123; int n = x.size(); double sum_maxs = 0; for(int i = 0; i &lt; n; ++i) &#123; if(x[i] &gt; y[i]) sum_maxs += x[i]; else sum_maxs += y[i]; &#125; return sum_maxs / n;&#125;// [[Rcpp::export]]double mm2(NumericMatrix x) &#123; int n = x.nrow(); double sum_maxs = 0; for(int i = 0; i &lt; n; ++i) &#123; if(x[i] &gt; x[i+n]) sum_maxs += x[i]; else sum_maxs += x[i+n]; &#125; return sum_maxs / n;&#125;// [[Rcpp::export]]double mm3(NumericVector x, NumericVector y) &#123; omp_set_num_threads(omp_get_max_threads()); int n = x.size(); double sum_maxs = 0; #pragma omp parallel &#123; #pragma omp for reduction( +:sum_maxs) for(int i = 0; i &lt; n; ++i) &#123; if(x[i] &gt; y[i]) sum_maxs += x[i]; else sum_maxs += y[i]; &#125; &#125; return sum_maxs / n;&#125;// [[Rcpp::export]]double mm4(NumericMatrix x) &#123; omp_set_num_threads(omp_get_max_threads()); int n = x.nrow(); double sum_maxs = 0; #pragma omp parallel &#123; #pragma omp for reduction( +:sum_maxs) for(int i = 0; i &lt; n; ++i) &#123; if(x[i] &gt; x[i+n]) sum_maxs += x[i]; else sum_maxs += x[i+n]; &#125; &#125; return sum_maxs / n;&#125;// [[Rcpp::export]]double mm5(int n) &#123; omp_set_num_threads(omp_get_max_threads()); Rcpp::NumericVector x(n); Rcpp::NumericVector y(n); RNGScope scope; x = rnorm(n, 0.0, 1.0); y = rnorm(n, 0.0, 1.0); double sum_maxs = 0; #pragma omp parallel &#123; #pragma omp for reduction( +:sum_maxs) for(int i = 0; i &lt; n; ++i) &#123; if(x[i] &gt; y[i]) sum_maxs += x[i]; else sum_maxs += y[i]; &#125; &#125; return sum_maxs / n;&#125;')h = function()&#123; nreps = 1e7 x = rnorm(nreps) y = rnorm(nreps) print(mm(x, y))&#125;i = function()&#123; nreps = 1e7 x = matrix(rnorm(2*nreps),nreps) print(mm2(x))&#125;j = function()&#123; nreps = 1e7 x = rnorm(nreps) y = rnorm(nreps) print(mm3(x, y))&#125;k = function()&#123; nreps = 1e7 x = matrix(rnorm(2*nreps),nreps) print(mm4(x))&#125;l = function()&#123; nreps = 1e7 print(mm5(nreps))&#125;library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;Rcpp.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]double mm6(int n) &#123; arma_rng::set_seed_random(); colvec x = randn(n), y = randn(n); return as_scalar(mean(arma::max(x,y)));&#125;// [[Rcpp::export]]double mm7(int n) &#123; arma_rng::set_seed_random(); mat x = randn(n, 2); return as_scalar(mean(arma::max(x,1)));&#125;')m = function()&#123; nreps = 1e7 print(mm6(nreps))&#125;n = function()&#123; nreps = 1e7 print(mm7(nreps))&#125;library(rbenchmark)benchmark(a(),b(),d(),e(),f(),g(),h(),i(),j(),k(),l(),m(),n(), a_cmp(),b_cmp(),d_cmp(),e_cmp(),f_cmp(), replications = 10, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')## linux# test replications elapsed relative user.self# 12 m() 10 6.909 1.000 8.790# 13 n() 10 7.161 1.036 7.091# 11 l() 10 11.123 1.610 29.872# 7 h() 10 17.842 2.582 17.826# 9 j() 10 17.873 2.587 34.783# 10 k() 10 20.029 2.899 37.757# 8 i() 10 20.925 3.029 20.811# 15 b_cmp() 10 24.484 3.544 24.282# 2 b() 10 24.514 3.548 24.290# 6 g() 10 242.593 35.113 114.829# 4 e() 10 362.433 52.458 361.964# 17 e_cmp() 10 370.546 53.632 370.284# 14 a_cmp() 10 381.048 55.152 381.387# 1 a() 10 434.574 62.900 434.929# 5 f() 10 579.186 83.831 579.427# 18 f_cmp() 10 621.092 89.896 621.314# 3 d() 10 634.046 91.771 634.205# 16 d_cmp() 10 686.426 99.352 686.650 Number 1, 2, 3, 4, 5 and 6 are the R functions. Number 14, 15, 16, 17 and 18 are R functions with byte compiler. Number 7, 8, 9, 10 and 11 are the Rcpp functions. Number 12 and 13 are RcppArmadillo functions. Number 1 function is using for-loop in R. Number 2 function is using the function pmax (R vectorized maximum function). Number 3, 4 and 5 functions are the application of sapply, apply and lapply function, respectively. Number 6 function is using the parallel computing package snowfall in R. Number 7 and 8 function is to locate the two vectors of normal r.v. and to use Rcpp function to calculate. The difference between two functions are the input element, number 7 function input two vectors and number 8 function input one matrix. Number 9 and 10 functions is the parallel version of number 7 and 8 functions, respectively. Number 11 function is to use the c++ for locate the two vectors of normal r.v. and to calculate in the way of number 9 function. Number 12 and 13 functions are to use armadillo for locating the two vectors of normal r.v. and calculating with vector or matrix manipulation. The results shows that generating normal r.v. in c++ is fast than generating in R, the vectorized function pmax is fast than other ?apply functions and calculation of two vectors are fast than a matrix (I think that the reason is searching address.). My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz. Remark:I think that the reason why the calculation with matrix is slow is that c++ is a row-major language, so I try another example: 123456789101112131415161718192021222324252627282930313233343536373839404142434445library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;Rcpp.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]double mm6(int n) &#123;arma_rng::set_seed_random();colvec x = randn(n), y = randn(n);return as_scalar(mean(arma::max(x,y)));&#125;// [[Rcpp::export]]double mm7(int n) &#123;arma_rng::set_seed_random();mat x = randn(n, 2);return as_scalar(mean(arma::max(x,1)));&#125;// [[Rcpp::export]]double mm9(int n) &#123;arma_rng::set_seed_random();mat x = randn(2, n);return as_scalar(mean(arma::max(x,0), 1));&#125;')m = function()&#123; nreps = 1e7 print(mm6(nreps))&#125;n = function()&#123; nreps = 1e7 print(mm7(nreps))&#125;o = function()&#123; nreps = 1e7 print(mm9(nreps))&#125;library(rbenchmark)benchmark(m(),n(),o(), replications = 10, columns=c('test', 'replications', 'elapsed','relative', 'user.self'), order='relative')# test replications elapsed relative user.self# 1 m() 10 7.001 1.000 6.875# 3 o() 10 7.090 1.013 6.977# 2 n() 10 7.247 1.035 7.124 The results show that calculation in row and calculation in column is big difference. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>openmp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Attributes for kernelMatrix with openmp]]></title>
    <url>%2Fposts%2F201407%2F2014-07-20-Rcpp-Attributes-for-kernelMatrix-with-openmp.html</url>
    <content type="text"><![CDATA[I use Rcpp Attributes to compute the kernel matrix and show how to link omp to speedup your Rcpp code. I also present that the speed of Rcpp Attributes is competitive with the function kernelMatrix written in C in the R package kernlab. Code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455library(kernlab)library(Rcpp)library(RcppArmadillo)## For windows user# library(inline)# settings &lt;- getPlugin("Rcpp")# settings$env$PKG_CXXFLAGS &lt;- paste('-fopenmp', settings$env$PKG_CXXFLAGS)# settings$env$PKG_LIBS &lt;- paste('-fopenmp -lgomp', settings$env$PKG_LIBS)# do.call(Sys.setenv, settings$env)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;omp.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]NumericMatrix kernelMatrix_cpp(NumericMatrix Xr, NumericMatrix Centerr, double sigma) &#123; omp_set_num_threads(omp_get_max_threads()); uword n = Xr.nrow(), b = Centerr.nrow(), row_index, col_index; mat X(Xr.begin(), n, Xr.ncol(), false); mat Center(Centerr.begin(), b, Centerr.ncol(), false); mat KerX(n, b); #pragma omp parallel private(row_index, col_index) for (row_index = 0; row_index &lt; n; row_index++) &#123; #pragma omp for nowait for (col_index = 0; col_index &lt; b; col_index++) &#123; KerX(row_index, col_index) = exp(sum(square(X.row(row_index) - Center.row(col_index))) / (-2.0 * sigma * sigma)); &#125; &#125; return wrap(KerX);&#125;')N = 1000p = 100b = 300X = matrix(rnorm(N*p), ncol = p)center = X[sample(1:N, b),]sigma = 3kernel_X &lt;- kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)kernel_X_cpp = kernelMatrix_cpp(X, center, sigma)## testall.equal(kernel_X@.Data, kernel_X_cpp)# TRUElibrary(rbenchmark)benchmark(cpp = kernelMatrix_cpp(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), columns=c("test", "replications","elapsed", "relative"), replications=10, order="relative")# test replications elapsed relative# 1 cpp 10 0.131 1.000# 2 kernlab 10 0.199 1.519 With openmp, it is faster than kernlab with small input matrix size, however, the efficiency is too low when the size increase. Another efficient method is listed below: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172library(kernlab)library(Rcpp)library(RcppArmadillo)## For windows user# library(inline)# settings &lt;- getPlugin("Rcpp")# settings$env$PKG_CXXFLAGS &lt;- paste('-fopenmp', settings$env$PKG_CXXFLAGS)# settings$env$PKG_LIBS &lt;- paste('-fopenmp -lgomp', settings$env$PKG_LIBS)# do.call(Sys.setenv, settings$env)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]NumericMatrix kernelMatrix_cpp2(NumericMatrix Xr, NumericMatrix Centerr, double sigma) &#123; uword n = Xr.nrow(), b = Centerr.nrow(), row_index, col_index; mat X(Xr.begin(), n, Xr.ncol(), false), Center(Centerr.begin(), b, Centerr.ncol(), false), KerX(X*Center.t()); colvec X_sq = sum(square(X), 1) / 2; rowvec Center_sq = (sum(square(Center), 1)).t() / 2; KerX.each_row() -= Center_sq; KerX.each_col() -= X_sq; KerX *= 1 / (sigma * sigma); KerX = exp(KerX); return wrap(KerX);&#125;')N = 10000p = 1000b = 3000X = matrix(rnorm(N*p), ncol = p)center = X[sample(1:N, b),]sigma = 3t1 = Sys.time()kernel_X_cpp = kernelMatrix_cpp2(X, center, sigma) Sys.time() - t1 t1 = Sys.time()kernel_X &lt;- kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center)Sys.time() - t1## Testall.equal(kernel_X@.Data, kernel_X_cpp)# TRUElibrary(rbenchmark)benchmark(cpp = kernelMatrix_cpp(X, center, sigma), cpp2 = kernelMatrix_cpp2(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), columns=c("test", "replications","elapsed", "relative"), replications=10, order="relative")# test replications elapsed relative# 2 cpp2 10 13.810 1.000# 3 kernlab 10 24.978 1.809# 1 cpp 10 207.192 15.003N = 1000p = 100b = 300X = matrix(rnorm(N*p), ncol = p)center = X[sample(1:N, b),]sigma = 3benchmark(cpp = kernelMatrix_cpp(X, center, sigma), cpp2 = kernelMatrix_cpp2(X, center, sigma), kernlab = kernelMatrix(rbfdot(sigma=1/(2*sigma^2)), X, center), columns=c("test", "replications","elapsed", "relative"), replications=10, order="relative")# test replications elapsed relative# 2 cpp2 10 0.059 1.000# 1 cpp 10 0.179 3.034# 3 kernlab 10 0.230 3.898 The above result show that there is a trick you should know to speedup the code: if you can directly use matrix manipulation to finish the calculation, then please do it! Or you get low efficiency if you use for loop. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>openmp</tag>
        <tag>kernel matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Attributes]]></title>
    <url>%2Fposts%2F201407%2F2014-07-19-Rcpp-Attributes.html</url>
    <content type="text"><![CDATA[Recently, I went to the 23th STSC, I got some information about the new API of Rcpp, Rcpp attributes. I had tried some examples and it worked well. Here I demonstrate some examples. First example: call the pnorm function in Rcpp: 1234567891011121314151617181920require(Rcpp)sourceCpp(code = '#include &lt;Rcpp.h&gt;using namespace Rcpp;// [[Rcpp::export]]DataFrame mypnorm(NumericVector x)&#123; int n = x.size(); NumericVector y1(n), y2(n), y3(n); for (int i=0; i&lt;n; i++)&#123; y1[i] = ::Rf_pnorm5(x[i], 0.0, 1.0, 1, 0); y2[i] = R::pnorm(x[i], 0.0, 1.0, 1, 0); &#125; y3 = pnorm(x); return DataFrame::create( Named("R") = y1, Named("Rf_") = y2, Named("sugar") = y3);&#125;')mypnorm(runif(10, -3, 3)) Rcpp attributes allows user to write Rcpp in a simple way. User does not need to learn about how to write R extension. Just write a cpp script and add the line // [[Rcpp::export]], then user can use the function in R. Next two example is about the two extension packages of Rcpp, RcppArmadillo and RcppEigen. The two packages provide Rcpp to link the C++ linear algebra libraries, armadillo and Eigen. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657library(Rcpp)library(RcppArmadillo)library(RcppEigen)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;using namespace Rcpp;// [[Rcpp::export]]List fastLm_RcppArma(NumericVector yr, NumericMatrix Xr) &#123; int n = Xr.nrow(), k = Xr.ncol(); arma::mat X(Xr.begin(), n, k, false); arma::colvec y(yr.begin(), yr.size(), false); arma::colvec coef = arma::solve(X, y); arma::colvec resid = y - X*coef; double sig2 = arma::as_scalar(arma::trans(resid)*resid/(n-k)); arma::colvec stderrest = arma::sqrt(sig2 * arma::diagvec( arma::inv(arma::trans(X)*X))); return List::create( Named("coefficients") = coef, Named("stderr") = stderrest);&#125;')sourceCpp(code = '// [[Rcpp::depends(RcppEigen)]]#include &lt;RcppEigen.h&gt;using namespace Rcpp;using Eigen::Map;using Eigen::MatrixXd;using Eigen::VectorXd;// [[Rcpp::export]]List fastLm_RcppEigen(NumericVector yr, NumericMatrix Xr) &#123; const Map&lt;MatrixXd&gt; X(as&lt;Map&lt;MatrixXd&gt; &gt;(Xr)); const Map&lt;VectorXd&gt; y(as&lt;Map&lt;VectorXd&gt; &gt;(yr)); int n = Xr.nrow(), k = Xr.ncol(); VectorXd coef = (X.transpose() * X).llt().solve(X.transpose() * y.col(0)); VectorXd resid = y - X*coef; double sig2 = resid.squaredNorm() / (n - k); VectorXd stderrest = (sig2 * ((X.transpose() * X).inverse()).diagonal()).array().sqrt(); return List::create( Named("coefficients") = coef, Named("stderr") = stderrest);&#125;')N = 20000p = 1000X = matrix(rnorm(N*p), ncol = p)y = X %*% 10**(sample(seq(-5, 3, length = N+p), p)) + rnorm(N)system.time(fastLm_RcppArma(y, X))# user system elapsed# 5.49 0.06 1.46system.time(fastLm_RcppEigen(y, X))# user system elapsed# 9.32 0.06 8.96system.time(lm(y~X - 1))# user system elapsed# 145.13 14.76 91.84 The cpp functions are faster 63 times than R function lm. My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz. I think that Rcpp is the package which is the worthiest to learn if you want to use R to do statistical computing or machine learning. Rcpp attributes had changed the way to source C++ code in R, it let Rcpp is more convenient and more powerful.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>Rcpp Attributes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rcpp Attributes with lasso]]></title>
    <url>%2Fposts%2F201407%2F2014-07-19-Rcpp-Attributes-with-lasso.html</url>
    <content type="text"><![CDATA[I try to write a lasso algorithm by Rcpp attributes. Reference:Friedman, J., Hastie, T. and Tibshirani, R. (2008) Regularization Paths for Generalized Linear Models via Coordinate Descent, http://www.stanford.edu/~hastie/Papers/glmnet.pdf, Journal of Statistical Software, Vol. 33(1), 1-22 Feb 2010, http://www.jstatsoft.org/v33/i01/ Code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960library(Rcpp)library(RcppArmadillo)sourceCpp(code = '// [[Rcpp::depends(RcppArmadillo)]]#include &lt;RcppArmadillo.h&gt;#include &lt;ctime&gt;using namespace Rcpp;using namespace arma;// [[Rcpp::export]]List lasso_fit_f(NumericMatrix Xr, NumericVector yr, int n_penalty = 100)&#123; int n = Xr.nrow(), p = Xr.ncol(); mat X(Xr.begin(), n, p, false); colvec y(yr.begin(), n, false); double z0 = mean(y), penalty; colvec xydot = (y.t() * X).t(); mat xxdot(p, p); xxdot = X.t() * X; double penalty_max_log = log(max(xydot) / n); colvec penalties = exp(linspace&lt;colvec&gt;(penalty_max_log, penalty_max_log+log(0.05), n_penalty)); mat coef_m(p, n_penalty); int MaxIter = 1e5; colvec coef(p), coef_new(p); for(int k = 1; k &lt; n_penalty; k++) &#123; coef = coef_m.col(k-1); penalty = penalties(k); for(int i = 0; i &lt; MaxIter; i++) &#123; coef_new = (xydot - xxdot * coef) / n + coef; coef_new(find(abs(coef_new) &lt; penalty)).zeros(); coef_new(find(coef_new &gt; penalty)) -= penalty; coef_new(find(coef_new &lt; -penalty)) += penalty; if( as_scalar((coef - coef_new).t() * (coef - coef_new)) / k &lt; 1e-7) break; coef = coef_new; &#125; coef_m.col(k) = coef; &#125; return List::create(Named("intercept") = z0, Named("coefficients") = coef_m, Named("penalties") = penalties);&#125;')d = 1000; N = 10000x = matrix(rnorm(N*d), N)x[,3] = x[,1] - 2*x[,2] + rnorm(N)x[,30] = x[,10] - 2*x[,20] + rnorm(N)y = cbind(1, x) %*% c(-1, 2, -0.3, 0.7, sample(10**seq(-10, 1, length = N), d-3)) + rnorm(N, 0, 2)x = scale(x)t1 = Sys.time()a = lasso_fit_f(x, y)t_cpp = Sys.time() - t1library(glmnet)t1 = Sys.time()fit = glmnet(x,y, lambda = a$penalties)t_glmnet = Sys.time() - t1c(t_glmnet, t_cpp)# [1] 0.7171087 0.2032089 It works well!! My environment is ubuntu 14.04, R 3.1.1 compiled by intel c++, fortran compiler with MKL. My CPU is 3770K@4.3GHz.]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Rcpp</tag>
        <tag>RcppArmadillo</tag>
        <tag>LASSO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some tips on Linux]]></title>
    <url>%2Fposts%2F201407%2F2014-07-10-Some-tips-on-linux.html</url>
    <content type="text"><![CDATA[This post is used to record some tips I can’t categorize in ubuntu. i. automatically load some shell scriptsIn my system ubuntu 14.04, I can find the file .bashrc in my home directory.Since I want ubuntu to load intel complier and mkl parameter automatically, all I need to do is to add the two lines in the end of that file: (mint 17: gedit /etc/bash.bashrc) 12source /opt/intel/composer_xe_2015/mkl/bin/mklvars.sh intel64source /opt/intel/composer_xe_2015/bin/compilervars.sh intel64 Then I success!! ii. cannot install ubuntu or MintWith the options - acpi=off nolapic noapic, I finally install ubuntu successfully. iii. cannot boot without nolapic, however, it only recognize one cpu with nolapicI solved this problem by Dual core recognized as single core because of nolapic?.I edited the grub file with following commands: 12sudo bashgedit /etc/default/grub And replace nolapic with pci=assign-busses apicmaintimer idle=poll reboot=cold,hard, the grub file would be contain this two lines: 12GRUB_CMDLINE_LINUX_DEFAULT="quiet splash acpi_osi=linux"GRUB_CMDLINE_LINUX="noapic pci=assign-busses apicmaintimer idle=poll reboot=cold,hard" Then use following command to update grub. And the problem is fixed. 1sudo update-grub iv. to get the permission of ntfs disks, you can edit the fstab in /etc as following: 1sudo gedit /etc/fstab And you can find the uuid by using the command ls -l /dev/disk/by-uuid. To add the disk and set the permission in the file fstab like this: 1234UUID=1c712d26-7f9d-4efc-b796-65bee366c8aa / ext4 noatime,nodiratime,discard,errors=remount-ro 0 1UUID=9298D0AB98D08EDB /media/Windows ntfs defaults,uid=1000,gid=1000,umask=002 0 0UUID=08C2997EC29970A4 /media/Download ntfs defaults,uid=1000,gid=1000,umask=002 0 0UUID=01CD524F3352C990 /media/Files ntfs defaults,uid=1000,gid=1000,umask=002 0 0 Then you can access your ntfs disk and set an alias for each disk. v. use grub comstomer to edit the boot order. Installation: 123sudo add-apt-repository ppa:danielrichter2007/grub-customizersudo apt-get updatesudo apt-get install grub-customizer vi. Install font InconsolataDownload Here and unity tweak tool (sudo apt-get install unity-tweak-tool). vii. Install the chinese input fcitx and language Chinese Traditional. 1sudo apt-get install fcitx fcitx-chewing fcitx-config-gtk fcitx-frontend-all fcitx-module-cloudpinyin fcitx-ui-classic fcitx-frontend-qt4 fcitx-frontend-qt5 fcitx-frontend-gtk2 fcitx-frontend-gtk3 viii. Install ruby, jekyll and git. 123456sudo apt-get install software-properties-commonsudo apt-add-repository ppa:brightbox/ruby-ngsudo apt-get updatesudo apt-get install ruby2.2 ruby2.2-dev git python-pip python3-pipsudo gem install jekyllsudo pip install pygments (To be continued.)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>Mint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何compile numpy and scipy with Intel C++ compiler and Intel MKL]]></title>
    <url>%2Fposts%2F201405%2F2014-05-20-how-to-build-numpy-and-scipy-with-MKL.html</url>
    <content type="text"><![CDATA[這篇要來敘述怎麼在linux中，利用Intel C++ compiler以及Intel MKL編譯numpy以及scipy這兩個python的套件，以下是參考連結： https://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl http://songuke.blogspot.tw/2012/02/compile-numpy-and-scipy-with-intel-math.html Numpy使用MKL库提升计算性能 Numpy fails with python-dbg 首先，先取得編譯環境以及root權限以方便進行編譯的工作，另外還有一些需要的套件要安裝，命令如下： 123sudo apt-get install python-setuptools python-pip python-dev cython# python 3sudo apt-get install python3-setuptools python3-pip python3-dev cython3 接著切換到Downloads目錄(這你可以自己調整)並下載numpy以及scipy的原始碼，命令如下： 123cd; cd Downloadsgit clone https://github.com/numpy/numpy.gitgit clone https://github.com/scipy/scipy.git 接著在numpy資料夾中新增一個site.cfg的檔案(此處以sublime text做編輯器)，命令如下： 123456sudo -ssource /opt/intel/composer_xe_2013_sp1.3.174/mkl/bin/mklvars.sh intel64source /opt/intel/composer_xe_2013_sp1.3.174/bin/compilervars.sh intel64cd numpy# rm -rf build # if there is a build foldersubl site.cfg 並且添加內容： 123456[mkl]library_dirs = /opt/intel/composer_xe_2013_sp1.3.174/compiler/lib/intel64:/opt/intel/composer_xe_2013_sp1.3.174/mkl/lib/intel64include_dirs = /opt/intel/composer_xe_2013_sp1.3.174/compiler/include:/opt/intel/composer_xe_2013_sp1.3.174/mkl/includemkl_libs = mkl_def, mkl_intel_lp64, mkl_intel_thread, mkl_corelapack_libs = mkl_lapack95_lp64libraries = iomp5 接著修改編譯的參數， 1subl numpy/distutils/intelccompiler.py 以下方文字分別取代取代文件中self.cc_exe=&#39;icc -fPIC&#39;以及self.cc_exe=&#39;icc -m64 -fPIC&#39;： 12self.cc_exe = 'icc -O3 -g -fPIC -fp-model strict -fomit-frame-pointer -openmp -xhost'self.cc_exe = 'icc -m64 -O3 -g -fPIC -fp-model strict -fomit-frame-pointer -openmp -xhost' 最後運行這個指令就可以進行安裝了。 123python setup.py config --compiler=intelem build_clib --compiler=intelem build_ext --compiler=intelem install# python 3python3 setup.py config --compiler=intelem build_clib --compiler=intelem build_ext --compiler=intelem install 如果出現No module named msvc9compiler，就把numpy/distutil/intelccompiler.py裡面有關msvc9compiler的code都註解掉就好了。 請先測試numpy是否正常，先安裝nose這個套件： 123easy_install nose# python3easy_install3 nose 開啟python並運行(注意環境還是要source上方兩個檔案)： 12import numpynumpy.test() 接著編譯scipy，把site.cfg從numpy複製到scipy的資料夾中： 12345cp site.cfg ../scipy/site.cfgcd ../scipypython setup.py config --compiler=intelem --fcompiler=intelem build_clib --compiler=intelem --fcompiler=intelem build_ext --compiler=intelem --fcompiler=intelem install# python 3python3 setup.py config --compiler=intelem --fcompiler=intelem build_clib --compiler=intelem --fcompiler=intelem build_ext --compiler=intelem --fcompiler=intelem install 開啟python測試scipy(注意環境還是要source上方兩個檔案)： 12import scipyscipy.test() 我跑scipy的測試會失敗三個，看了一下別人的問答，他們認為應該不是太嚴重的錯，我也沒有再裡他了。最後如果中間有出錯，請記得移除掉你安裝套件的位置，假設你使用的python是2.7版就是執行下方指令，在加上tab補全剩下的檔名： 12345sudo rm -r /usr/local/lib/python2.7/dist-packages/numpysudo rm -r /usr/local/lib/python2.7/dist-packages/scipy# python 3.4sudo rm -r /usr/local/lib/python3.4/dist-packages/numpysudo rm -r /usr/local/lib/python3.4/dist-packages/scipy Add $LD_LIBRARY_PATH in environment by using subl ~/.bashrc. 1export LD_LIBRARY_PATH=/opt/intel/composer_xe_2013_sp1.3.174/compiler/lib/intel64:/opt/intel/composer_xe_2013_sp1.3.174/mkl/lib/intel64:$LD_LIBRARY_PATH]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>numpy</tag>
        <tag>scipy</tag>
        <tag>MKL</tag>
        <tag>Intel C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Syntax highlight in Jekyll]]></title>
    <url>%2Fposts%2F201405%2F2014-05-19-syntax-highlight-in-jekyll.html</url>
    <content type="text"><![CDATA[從上週就一直在嘗試如何把我部落格的程式碼都上色，弄了好多天才發現主要的癥結。中間參考太多網站，列幾個重要的。參考網站如下： Do I need to generate a css file … Add code highlight with Pygments 你要有syntax highlight的功能，要先安裝幾個重要的工具，第一個是要有Python，並且安裝其套件Pygments，Ruby要安裝Pygments.rb，版本0.5.4可能會出錯，我裝的是0.5.0，這個版本大多人都可以成功。接著需要調整_config.yml中的選項： 1highlighter: pygments 另外，還有要生成highlight所需的css檔案，於cmd中鍵入 1pygmentize -S default -f html &gt; pygments.css 然後在themes中的default.html中加上下面這一行： 1&lt;link rel="stylesheet" type="text/css" href="/path/to/pygments.css"&gt; 這樣就成功了。]]></content>
      <categories>
        <category>Jekyll</category>
      </categories>
      <tags>
        <tag>Jekyll</tag>
        <tag>Syntax Highlight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 in Linux]]></title>
    <url>%2Fposts%2F201405%2F2014-05-18-sublime-text-in-linux.html</url>
    <content type="text"><![CDATA[以下文章參考下列四個網址： 完美解决 Linux 下 Sublime Text 中文输入 How do I make Sublime Text 3 the default text editor linux上，中文輸入法一直是難題，應用程式沒辦法支援中文輸入是非常常見的事情，連sublime text也是。還不只如此，還有輸入法的戰爭，我在進入linux時，最一開使用的輸入法是gcin，然後去用hime，最後因為sublime text的解決方法只能只用fcitx，最後用了這個輸入法。 我看了不少文章提供各種sublime text輸入的問題，我覺得下列方法是最方便的： 把下列的程式碼存為sublime_imfix.c: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*sublime-imfix.cUse LD_PRELOAD to interpose some function to fix sublime input method support for linux.By Cjacker Huanggcc -shared -o libsublime-imfix.so sublime-imfix.c `pkg-config --libs --cflags gtk+-2.0` -fPICLD_PRELOAD=./libsublime-imfix.so subl*/#include &lt;gtk/gtk.h&gt;#include &lt;gdk/gdkx.h&gt;typedef GdkSegment GdkRegionBox;struct _GdkRegion&#123; long size; long numRects; GdkRegionBox *rects; GdkRegionBox extents;&#125;;GtkIMContext *local_context;voidgdk_region_get_clipbox (const GdkRegion *region, GdkRectangle *rectangle)&#123; g_return_if_fail (region != NULL); g_return_if_fail (rectangle != NULL); rectangle-&gt;x = region-&gt;extents.x1; rectangle-&gt;y = region-&gt;extents.y1; rectangle-&gt;width = region-&gt;extents.x2 - region-&gt;extents.x1; rectangle-&gt;height = region-&gt;extents.y2 - region-&gt;extents.y1; GdkRectangle rect; rect.x = rectangle-&gt;x; rect.y = rectangle-&gt;y; rect.width = 0; rect.height = rectangle-&gt;height; //The caret width is 2; //Maybe sometimes we will make a mistake, but for most of the time, it should be the caret. if(rectangle-&gt;width == 2 &amp;&amp; GTK_IS_IM_CONTEXT(local_context)) &#123; gtk_im_context_set_cursor_location(local_context, rectangle); &#125;&#125;//this is needed, for example, if you input something in file dialog and return back the edit area//context will lost, so here we set it again.static GdkFilterReturn event_filter (GdkXEvent *xevent, GdkEvent *event, gpointer im_context)&#123; XEvent *xev = (XEvent *)xevent; if(xev-&gt;type == KeyRelease &amp;&amp; GTK_IS_IM_CONTEXT(im_context)) &#123; GdkWindow * win = g_object_get_data(G_OBJECT(im_context),"window"); if(GDK_IS_WINDOW(win)) gtk_im_context_set_client_window(im_context, win); &#125; return GDK_FILTER_CONTINUE;&#125;void gtk_im_context_set_client_window (GtkIMContext *context, GdkWindow *window)&#123; GtkIMContextClass *klass; g_return_if_fail (GTK_IS_IM_CONTEXT (context)); klass = GTK_IM_CONTEXT_GET_CLASS (context); if (klass-&gt;set_client_window) klass-&gt;set_client_window (context, window); if(!GDK_IS_WINDOW (window)) return; g_object_set_data(G_OBJECT(context),"window",window); int width = gdk_window_get_width(window); int height = gdk_window_get_height(window); if(width != 0 &amp;&amp; height !=0) &#123; gtk_im_context_focus_in(context); local_context = context; &#125; gdk_window_add_filter (window, event_filter, context);&#125; Ctrl+Alt+T打開你的Terminal視窗到你儲存上面檔案的地方，鍵入： 123sudo apt-get install build-essential libgtk2.0-devgcc -shared -o libsublime-imfix.so sublime-imfix.c `pkg-config --libs --cflags gtk+-2.0` -fPICsudo mv libsublime-imfix.so /opt/sublime_text/ 這樣就完成編譯，並且將檔案放置到安裝目錄了。 修改啟動部份 1sudo subl /usr/share/applications/sublime-text.desktop 在每一個Exec=後面都加上下面的指令： 1env LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so 然後輸入 1sudo subl /usr/bin/subl 更動內容為 123#!/bin/shexport LD_PRELOAD=/opt/sublime_text/libsublime-imfix.soexec /opt/sublime_text/sublime_text "$@" 如果想要把sublime text更動為預設編輯器，先使用下列指令確定是否有安裝成功： 1ls /usr/share/applications/sublime-text.desktop 接著打開linux的default列表： 1sudo subl /usr/share/applications/defaults.list 按下Ctrl+H replace gedit with sublime-text。接著打開user的設定列表： 1subl ~/.local/share/applications/mimeapps.list 修改或添加下列下列文字： 12345678[Added Associations]text/plain=ubuntu-software-center.desktop;shotwell.desktop;sublime-text.desktop;text/x-chdr=shotwell-viewer.desktop;[Default Applications]text/plain=sublime-text.desktoptext/x-c++src=sublime-text.desktoptext/x-chdr=sublime-text.desktop]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Sublime Text 3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何compile R with Intel C++ compiler and Intel MKL]]></title>
    <url>%2Fposts%2F201405%2F2014-05-16-how-to-compile-R-with-mkl.html</url>
    <content type="text"><![CDATA[以下文章參考下列四個網址： Using Intel MKL with R Build R-3.0.1 with Intel C++ Compiler and Intel MKL on Linux Compiling R 3.0.1 with MKL support R Installation and Administraction 開始之前，先用Default R and R with Openblas來測試看看，I use testing script found in Simon Urbanek’s，Openblas部份參考這個網站For faster R use OpenBLAS instead: better than ATLAS, trivial to switch to on Ubuntu。 12345678910111213141516# to install package in /usr/lib/R/librarysudo chmod -R 774 /usr/lib/Rsudo chown -R celest.celest /usr/lib/Rsudo chmod -R 774 /usr/local/lib/Rsudo chown -R celest.celest /usr/local/lib/R# install required packageR -e "install.packages('SuppDists', repos = 'http://cran.rstudio.com/')"# run benchmarkR -e "source('http://r.research.att.com/benchmarks/R-benchmark-25.R')"# install OpenBLASsudo apt-get install libopenblas-base libatlas3gf-base# check the BLAS is replaced with OpenBLASsudo update-alternatives --config libblas.so.3sudo update-alternatives --config liblapack.so.3# run benchmark againR -e "source('http://r.research.att.com/benchmarks/R-benchmark-25.R')" 測試結果如下：Default R： 1234567891011121314151617181920212223242526272829303132333435363738 R Benchmark 2.5 ===============Number of times each test is run__________________________: 3 I. Matrix calculation ---------------------Creation, transp., deformation of a 2500x2500 matrix (sec): 0.8123333333333342400x2400 normal distributed random matrix ^1000____ (sec): 0.474666666666667Sorting of 7,000,000 random values__________________ (sec): 0.5633333333333332800x2800 cross-product matrix (b = a' * a)_________ (sec): 8.99466666666667Linear regr. over a 3000x3000 matrix (c = a \ b')___ (sec): 4.42166666666667 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 1.26481956425649 II. Matrix functions --------------------FFT over 2,400,000 random values____________________ (sec): 0.396000000000001Eigenvalues of a 640x640 random matrix______________ (sec): 0.718000000000001Determinant of a 2500x2500 random matrix____________ (sec): 3.03633333333334Cholesky decomposition of a 3000x3000 matrix________ (sec): 3.42433333333333Inverse of a 1600x1600 random matrix________________ (sec): 2.56266666666666 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 1.77441555997029 III. Programmation ------------------3,500,000 Fibonacci numbers calculation (vector calc)(sec): 0.543999999999992Creation of a 3000x3000 Hilbert matrix (matrix calc) (sec): 0.294333333333337Grand common divisors of 400,000 pairs (recursion)__ (sec): 0.686666666666658Creation of a 500x500 Toeplitz matrix (loops)_______ (sec): 0.49266666666666Escoufier's method on a 45x45 matrix (mixed)________ (sec): 0.378999999999991 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.466584631384852Total time for all 15 tests_________________________ (sec): 27.8006666666666Overall mean (sum of I, II and III trimmed means/3)_ (sec): 1.01548017027814 --- End of test --- R with Openblas: 1234567891011121314151617181920212223242526272829303132333435363738 R Benchmark 2.5 ===============Number of times each test is run__________________________: 3 I. Matrix calculation ---------------------Creation, transp., deformation of a 2500x2500 matrix (sec): 0.7556666666666672400x2400 normal distributed random matrix ^1000____ (sec): 0.473Sorting of 7,000,000 random values__________________ (sec): 0.5722800x2800 cross-product matrix (b = a' * a)_________ (sec): 0.411Linear regr. over a 3000x3000 matrix (c = a \ b')___ (sec): 0.213 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.480875883392325 II. Matrix functions --------------------FFT over 2,400,000 random values____________________ (sec): 0.372666666666666Eigenvalues of a 640x640 random matrix______________ (sec): 0.895Determinant of a 2500x2500 random matrix____________ (sec): 0.271333333333335Cholesky decomposition of a 3000x3000 matrix________ (sec): 0.243333333333333Inverse of a 1600x1600 random matrix________________ (sec): 0.328000000000001 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.321291459145567 III. Programmation ------------------3,500,000 Fibonacci numbers calculation (vector calc)(sec): 0.522333333333335Creation of a 3000x3000 Hilbert matrix (matrix calc) (sec): 0.259666666666668Grand common divisors of 400,000 pairs (recursion)__ (sec): 0.674333333333337Creation of a 500x500 Toeplitz matrix (loops)_______ (sec): 0.499333333333335Escoufier's method on a 45x45 matrix (mixed)________ (sec): 0.352999999999994 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.451548428599678Total time for all 15 tests_________________________ (sec): 6.84366666666667Overall mean (sum of I, II and III trimmed means/3)_ (sec): 0.411666478563312 --- End of test --- 可以看到total time已經從27.8秒到6.8秒左右，改善幅度已經不少，接著來compile R: 取得R與其開發包，並安裝需要的套件，在terminal use following commands: 123sudo add-apt-repository ppa:webupd8team/java &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install oracle-java8-installer &amp;&amp; sudo apt-get install oracle-java8-set-defaultapt-cache search readline xorg-dev &amp;&amp; sudo apt-get install libreadline6 libreadline6-dev texinfo texlive texlive-binaries texlive-latex-base xorg-dev tcl8.6-dev tk8.6-dev libtiff5 libtiff5-dev libjpeg-dev libpng12-dev libcairo2-dev libglu1-mesa-dev libgsl0-dev libicu-dev R-base R-base-dev libnlopt-dev libstdc++6 build-essential libcurl4-openssl-dev texlive-fonts-extra libxml2-dev aptitude# sudo apt-get install texlive-latex-extra 有一個工具要另外安裝，方式如下： 1234wget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gztar -xvzf libiconv-1.14.tar.gzcd libiconv-1.14 &amp;&amp; ./configure --prefix=/usr/local/libiconvmake &amp;&amp; sudo make install 但是我在make過程中有出錯，我google之後找到的解法是修改srclib/stdio.in.h的698列:原本的script: 1_GL_WARN_ON_USE (gets, "gets is a security hole - use fgets instead"); 修改後的scipt: 123#if defined(__GLIBC__) &amp;&amp; !defined(__UCLIBC__) &amp;&amp; !__GLIBC_PREREQ(2, 16) _GL_WARN_ON_USE (gets, "gets is a security hole - use fgets instead");#endif 之後再重新make就成功了。 取得R source code: 12wget http://cran.csie.ntu.edu.tw/src/base/R-3/R-3.2.3.tar.gztar -xvzf R-3.2.3.tar.gz 取得Intel C++ compiler and Intel MKL，你可以取得non-commercial license for this two software in intel website. 另外，64bit linux system不支援32 bits的compiler，安裝時記得取消掉IA32的安裝。 compilitation: 1234567891011121314151617181920212223sudo -ssource /opt/intel/composer_xe_2015/mkl/bin intel64source /opt/intel/composer_xe_2015/bin/compilervars.sh intel64MKL_path=/opt/intel/composer_xe_2015/mklICC_path=/opt/intel/composer_xe_2015/compilerexport LD="xild"export AR="xiar"export CC="icc"export CXX="icpc"export CFLAGS="-wd188 -ip -std=gnu99 -g -O3 -openmp -parallel -xHost -ipo -fp-model precise -fp-model source"export CXXFLAGS="-g -O3 -openmp -parallel -xHost -ipo -fp-model precise -fp-model source"export F77=ifortexport FFLAGS="-g -O3 -openmp -parallel -xHost -ipo -fp-model source"export FC=ifortexport FCFLAGS="-g -O3 -openmp -parallel -xHost -ipo -fp-model source"export ICC_LIBS=$ICC_path/lib/intel64export IFC_LIBS=$ICC_path/lib/intel64export LDFLAGS="-L$ICC_LIBS -L$IFC_LIBS -L$MKL_path/lib/intel64 -L/usr/lib -L/usr/local/lib -openmp"export SHLIB_CXXLD=icpcexport SHLIB_LDFLAGS="-shared -fPIC"export SHLIB_CXXLDFLAGS="-shared -fPIC"MKL="-L$MKL_path/lib/intel64 -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -ldl -lm"./configure --with-blas="$MKL" --with-lapack --with-x --enable-memory-profiling --with-tcl-config=/usr/lib/tcl8.6/tclConfig.sh --with-tk-config=/usr/lib/tk8.6/tkConfig.sh --enable-R-shlib --enable-BLAS-shlib --enable-prebuilt-html 如果順利會出現下方的畫面： 12345678910111213141516171819202122R is now configured for x86_64-pc-linux-gnu Source directory: . Installation directory: /usr/local C compiler: icc -wd188 -ip -std=gnu99 -g -O3 -openmp -parallel -xHost -ipo -fp-model precise -fp-model source Fortran 77 compiler: ifort -g -O3 -openmp -parallel -xHost -ipo -fp-model source C++ compiler: icpc -g -O3 -openmp -parallel -xHost -ipo -fp-model precise -fp-model source C++ 11 compiler: icpc -std=c++11 -g -O3 -openmp -parallel -xHost -ipo -fp-model precise -fp-model source Fortran 90/95 compiler: ifort -g -O3 -openmp -parallel -xHost -ipo -fp-model source Obj-C compiler: Interfaces supported: X11, tcltk External libraries: readline, BLAS(MKL), zlib, bzlib, lzma, PCRE, curl Additional capabilities: PNG, JPEG, TIFF, NLS, cairo, ICU Options enabled: shared R library, shared BLAS, R profiling, memory profiling, static HTML Capabilities skipped: Options not enabled: Recommended packages: yes 出現上方畫面就可以開始make跟install了： 123456789101112131415161718192021make &amp;&amp; make check# removing R before installationrm /usr/lib/libR.sorm -r /usr/lib/Rrm -r /usr/bin/Rscriptrm -r /usr/local/lib/Rmake docsmake installchown -R celest.celest /usr/local/lib/Rchmod -R 775 /usr/local/lib/R# to add mkl and intel c compiler into pathecho 'source /opt/intel/composer_xe_2015/mkl/bin/mklvars.sh intel64' &gt;&gt; /etc/bash.bashrcecho 'source /opt/intel/composer_xe_2015/bin/compilervars.sh intel64' &gt;&gt; /etc/bash.bashrcexit# install required packageR -e "install.packages('SuppDists', repos = 'http://cran.rstudio.com/')"# run benchmarkR -e "source('http://r.research.att.com/benchmarks/R-benchmark-25.R')"# to run rstudio-server, you have two options, first:# echo 'rsession-which-r=/usr/local/bin/R' &gt;&gt; /etc/rstudio/rserver.conf# second , please use option configure with --prefix=/usr 然後他就會幫你把R安裝於usr/local/lib/R中。 測試結果 1234567891011121314151617181920212223242526272829303132333435363738 R Benchmark 2.5 ===============Number of times each test is run__________________________: 3 I. Matrix calculation ---------------------Creation, transp., deformation of a 2500x2500 matrix (sec): 0.6832400x2400 normal distributed random matrix ^1000____ (sec): 0.259666666666667Sorting of 7,000,000 random values__________________ (sec): 0.5603333333333332800x2800 cross-product matrix (b = a' * a)_________ (sec): 0.44Linear regr. over a 3000x3000 matrix (c = a \ b')___ (sec): 0.193666666666666 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.400041560496478 II. Matrix functions --------------------FFT over 2,400,000 random values____________________ (sec): 0.393666666666667Eigenvalues of a 640x640 random matrix______________ (sec): 0.326999999999999Determinant of a 2500x2500 random matrix____________ (sec): 0.215666666666666Cholesky decomposition of a 3000x3000 matrix________ (sec): 0.183999999999999Inverse of a 1600x1600 random matrix________________ (sec): 0.182333333333332 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.234990082575285 III. Programmation ------------------3,500,000 Fibonacci numbers calculation (vector calc)(sec): 0.274333333333333Creation of a 3000x3000 Hilbert matrix (matrix calc) (sec): 0.221333333333333Grand common divisors of 400,000 pairs (recursion)__ (sec): 0.275666666666667Creation of a 500x500 Toeplitz matrix (loops)_______ (sec): 0.255Escoufier's method on a 45x45 matrix (mixed)________ (sec): 0.338999999999999 -------------------------------------------- Trimmed geom. mean (2 extremes eliminated): 0.268164327390206Total time for all 15 tests_________________________ (sec): 4.80466666666666Overall mean (sum of I, II and III trimmed means/3)_ (sec): 0.293214347493761 --- End of test --- 最後只需要用到4.8秒就可以完成了，可是complitation過程是滿麻煩的，雖然參考了多個網站，可是參數的設定都不太一樣，linux又有權限的限制，而且就算編譯成功，Rcpp這個套件不見得能夠成功，因此花了很久才終於編譯成功，並且能夠直接開啟，只是要利用到c, cpp or fortran時還是需要source compilervars.sh才能夠運行，而且我安裝了三四十個套件都沒有問題了。最後，如果沒有特別要求速度下，其實直接用OpenBLAS就可以省下很多麻煩。另外，我做了一個小小的測試於Rcpp上，速度有不少的提昇(因為用intel C++ compiler，大概增加5~10倍)，測試結果就不放上來了。以上資訊供大家參考，轉載請註明來源，謝謝。 最後附上測試環境: My environment is mint 17.3, R 3.2.3 compiled by Intel c++, fortran compiler with Intel MKL. My CPU is 3770K@4.4GHz. To use the html help page and change the default language of R to english, you can do that: 12echo 'options("help_type"="html")' &gt; ~/.Rprofileecho 'LANGUAGE="en"' &gt; ~/.Renviron 如果要讓Rstudio Server裡面成功啟動並且可以使用icpc，請在/usr/lib/rstudio-server/R/ServerOptions.R裡面加入下方： 1Sys.setenv(PATH = "/opt/intel/composer_xe_2015.1.133/bin/intel64:/opt/intel/composer_xe_2015.1.133/debugger/gdb/intel64_mic/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:$PATH") java config and install some useful packages: 123456R CMD javareconfinstall.packages(c('devtools', 'testthat'))devtools::install_github(c('klutometis/roxygen', 'hadley/assertthat', 'RcppCore/Rcpp', 'hadley/devtools', 'hadley/testthat', 'hadley/lazyeval'))devtools::install_github(c('smbache/magrittr', 'Rdatatable/data.table', 'hadley/reshape', 'hadley/plyr', 'hadley/dplyr'))devtools::install_github(c('RcppCore/RcppArmadillo', 'RcppCore/RcppEigen', 'RcppCore/RcppParallel'))devtools::install_github(c('hadley/tidyr', 'hadley/purrr', 'yihui/knitr'))]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>MKL</tag>
        <tag>Intel C++</tag>
        <tag>R</tag>
        <tag>Rcpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何利用jekyll建立你的blogger]]></title>
    <url>%2Fposts%2F201405%2F2014-05-14-how-to-build-blog-by-using-jekyll.html</url>
    <content type="text"><![CDATA[以下詳細介紹如何在windows環境下使用sublime text在github上建立屬於你自己的部落格 以下教學來自下列兩個網站 yizeng的blogger [madhur的blogger](http://www.madhur.co.in/blog/2011/09/01/runningjekyllwindows.html 安裝步驟如下:A. 下載我壓縮的工具包：Google drive解壓縮之後，裡面包含九個檔案： rubyinstaller-2.3.0-x64.exe DevKit-mingw64-64-4.7.2-20130224-1432-sfx.exe Anaconda2-4.1.0-Windows-x86_64.exe install.bat Git-1.9.5-preview20150319.exe RedmondPathzip.rar 這六個檔案分別為ruby安裝檔、ruby開發環境的檔案、python安裝檔案、python安裝bat檔、Git安裝檔案以及path修改的軟體，請依下面指示安裝。 ruby預設安裝到C:\Ruby23-x64。 點擊兩下DevKit-mingw64-64-4.7.2-20130224-1432-sfx.exe，進行解壓縮，為了方便說明，以及環境設定，請解壓縮到C:\rubydevkit 解壓縮RedmondPathzip.rar，打開資料夾中的Redmond Path.exe，在任意視窗中下方加入; C:\Ruby23-x64，(你安裝路徑有更動，請跟著更改)，如下圖下示： 點擊Anaconda2-4.1.0-Windows-x86_64.exe，安裝python，預設安裝到C:\Anaconda2，然後點擊兩下install.bat，會詢問你是否安裝，按下y，便完成python安裝。 點擊Git-1.9.5-preview20150319.exe安裝Git，中間要注意，勾選Use Git from the Windows Command Prompt B. 為了工作方便，請先按下windows鍵(在Ctr跟Alt之間)+R，開啟執行視窗，鍵入cmd，打開Windows Command Prompt，為了解釋方便，以後稱這個視窗為cmd。 打開cmd，他的預設目錄是在你的使用者下，請先輸入cd ../..，退到C:>，如圖： 然後在cmd中輸入下列指令： 123cd C:/rubydevkitruby dk.rb initnotepad config.yml 輸入完以上三行指令後，將會用記事本打開一個名為_config.yml的檔案，最後一行改成 - C:\Ruby23-x64。 回到cmd，鍵入ruby dk.rb install，如果成功會出現下面的訊息： 然後回到cmd，鍵入gem install jekyll，然後等待一下之後，他會安裝數個gems(不一定是27)，如圖： 另外，還需要安裝pygments，請鍵入gem install pygments.rb。 C. 申請git，並且clone我的庫當作基底。請到 Github申請一個帳號，假設你的使用者名稱(username)為USERNAME，在你的github中建立一個新的repository，repository的名稱請設定為USERNAME.github.com，這樣就完成github初步的設定。接下來，請先建立好你的工作目錄，例如我設定在E:\website中，那我可以利用這個指令cd /d E:\website到該目錄下，你可以自行更改工作目錄，假設clone我的庫做為基底，輸入下方指令： 12mkdir USERNAME.github.comgit clone https://github.com/ChingChuan-Chen/chingchuan-chen.github.com USERNAME.github.com 記得當中的USERNAME要改成你在github的username。例如我的username叫做imstupid，預期output如下圖： 再來就是init github的本地倉庫，以及設定你的github遠端帳號，指令如下： 1234cd USERNAME.github.comgit initgit remote set-url origin https://github.com/USERNAME/USERNAME.github.com.gitgit push origin master 過程中會要求輸入你的github的帳號(username)以及其密碼(password)，之後你就可以在你的github上看到你上傳的檔案了！最後就是一些簡單的修改，例如記事本去修改_config.yml (簡單的指令是notepad _config.yml，或是用記事本把它打開)： 檔案中，#是註解，程式不會去閱讀的部分可以寫在#後面，其他前面沒有#的部分就是你可以更改的部分，當然更進階的話，你還可以添加一些選項進去，像是更動title :後面的文字就是在更改你主題頁的名稱。修改之後存檔，在cmd中輸入git commit -am &quot;message&quot;這個目的是儲存你所有的修改，以及添加修改的相關訊息message (這個可以自己改)，例如我想記錄這次的修改是增加新的文章，我可以打git commit -am &quot;new post&quot;。 還有git的使用如你想在你的目錄下新增東西，你要讓它能夠出現在網站上就要先加入名單中，輸入git add .的指令加入你所新增的檔案，舉例來說，我增加了幾張圖，我就先打入git add .，接著commit，然後push(上傳)到我的github，操作示範如下： 如果想移除檔案，就輸入git rm filename，filename是你的檔案名稱，只是注意的是這個操作不只把從github抹除，同時也會把你硬碟的檔案刪除。其他的指令利用可能就要慢慢再去學習。 D. 其他部分，最重要的是如何預覽，在cmd中輸入jekyll serve會出現下面： 然後在你的瀏覽器(例如IE, chrome or firefox)輸入localhost:4000，就可以出現你blogger的預覽畫面： 還有PO文部分，可以先更改_posts下我的文章，它的檔案格式是yyyy-mm-dd-ANameOfPost.md，可以直接利用記事本做編輯，最前面是一些基本設定： 1234567891011---layout: postcTitle: 如何利用jekyll建立你的bloggertitle: how to build blog by using jekylldescription: ""---&#123;&#125;一些文字...123456 兩個—中是關於你post的設定，layout是設定我現在的格式是什麼，在_posts裡面就理所當然是設定post，title是設定你文章的標題(這是顯示的標題)，title是標題(提供給程式控制)，decription是關於你這篇文章的敘述，category是你文章的分類，cssdemo是檔案的格式，這部分我還不熟，請先跟我設定相同，或是你自行摸索，tags是標籤，方便你自己以及其他人找尋相關文章，最後，published是設定是否要公開於網站上，你如果還沒寫好的文章就可以先改成false，那你確定要公開就改成true，include部分是必須要引入的設定，最好不要省略，more那列是在首頁顯示部分到此，例如上面的例子，就是首頁只會顯示123，而456要等你點開文章才會看到。剩下還要更改的部分是index.html以及一些小地方，如果需要幫助，再到左下角點選我的名字就可以連到我的facebook與我聯絡。最後，溫馨提醒：文章的編寫可以對照我的post跟我的blogger顯示文章去推敲寫法，總之，從模仿開始，我也才剛學會架設blogger一周而已。 Note: github不會即時更新，需要等待幾分鐘才會更新你新的上傳。 2016-01-18增補： 沒空寫新的文章，根據這個部落格風格的原作者表示出了一個github-page的gem，用法參考這篇，這篇。 12345gem install github-pagesgem install railscd USERNAME.github.combundle installbundle exec jekyll serve --watch]]></content>
      <categories>
        <category>jekyll</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
</search>
